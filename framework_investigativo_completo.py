# =============================================================================
# RASTREIO FINO DO N√çVEL DE RU√çDO
# =============================================================================
# Imports centralizados no topo do arquivo (PEP 8)
import os
import json
import time
import logging
from pathlib import Path
from datetime import datetime
from typing import TYPE_CHECKING, Any, Dict, Optional

import numpy as np
import pandas as pd

from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn import datasets as sk_datasets
from sklearn.metrics import confusion_matrix
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

# Estat√≠stica
from scipy.stats import f_oneway, ttest_ind
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm

# Visualiza√ß√£o
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px

# Otimiza√ß√£o Bayesiana (opcional)
try:
    import optuna
    from optuna.samplers import TPESampler
    from optuna.pruners import MedianPruner
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False

# Inicializar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ===============================
# Utilidades de diret√≥rio de resultados (Drive/Colab/local)
# ===============================
def _parse_resultados_base_from_args() -> Optional[str]:
    """L√™ opcionalmente um argumento CLI --resultados <dir> para base dos resultados.
    Ex.: python framework_investigativo_completo.py --resultados "/content/drive/MyDrive/VQC_results"
    """
    try:
        import sys
        if '--resultados' in sys.argv:
            idx = sys.argv.index('--resultados')
            if idx + 1 < len(sys.argv):
                return sys.argv[idx + 1]
    except Exception:
        pass
    return None


def _detectar_colab_drive_montado() -> Optional[str]:
    """Detecta se est√° rodando no Google Colab com Drive montado e retorna caminho base padr√£o.
    Retorna None se n√£o detectar Colab/Drive.
    """
    try:
        # Heur√≠stica simples: diret√≥rio padr√£o do Drive no Colab
        drive_path = '/content/drive/MyDrive'
        return drive_path if os.path.isdir(drive_path) else None
    except Exception:
        return None


def _preparar_diretorio_resultados(base_dir: Optional[str] = None) -> str:
    """Cria e retorna o caminho da pasta de resultados com timestamp, respeitando:
    - argumento CLI --resultados <dir>
    - vari√°vel de ambiente RESULTS_BASE_DIR
    - ambiente Colab com Drive montado (/content/drive/MyDrive)
    - fallback: diret√≥rio local do projeto

    Tamb√©m cria subpastas padr√£o, se necess√°rio.
    """
    # Prioridades: CLI > ENV > Colab > local
    base = (
        base_dir
        or os.environ.get('RESULTS_BASE_DIR')
        or _detectar_colab_drive_montado()
        or os.getcwd()
    )

    now = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    pasta_resultados = os.path.join(base, f'resultados_{now}')
    try:
        os.makedirs(pasta_resultados, exist_ok=True)
        # Pr√©-criar subpastas comuns para organiza√ß√£o
        for sub in ('circuitos', 'barren_plateaus', 'experimentos_individuais'):
            os.makedirs(os.path.join(pasta_resultados, sub), exist_ok=True)
    except Exception:
        # Fallback absoluto: local
        pasta_resultados = f'resultados_{now}'
        os.makedirs(pasta_resultados, exist_ok=True)
    return pasta_resultados

def rastreio_fino_nivel_ruido(df_resultados, datasets, pasta_resultados, n_epocas=15, verbose=True, passo_fino=0.001, n_passos=5):
    """
    Rastreio fino do n√≠vel √≥timo de ru√≠do.

    Ap√≥s o grid search identificar uma regi√£o promissora, realiza busca refinada
    em torno do n√≠vel √≥timo com passos menores para caracteriza√ß√£o precisa.

    Args:
        df_resultados: DataFrame com resultados do grid search
        datasets: Dict com datasets de benchmark
        pasta_resultados: Pasta para salvar resultados
        n_epocas: N√∫mero de √©pocas para treinamento
        verbose: Se True, imprime progresso
        passo_fino: Tamanho do passo para busca refinada (padr√£o: 0.001)
        n_passos: N√∫mero de passos em cada dire√ß√£o (padr√£o: 5)

    Returns:
        List com resultados do rastreio fino

    Refer√™ncia:
        Esta t√©cnica √© inspirada em m√©todos de otimiza√ß√£o de hiperpar√¢metros
        como Successive Halving e Grid Search Refinement.
    """
    # json, os, Path j√° dispon√≠veis no escopo global

    subdir_root = Path(pasta_resultados) / 'rastreio_fino'
    os.makedirs(subdir_root, exist_ok=True)

    if verbose:
        logger.info("\n" + "="*80)
        logger.info(" RASTREIO FINO DO N√çVEL √ìTIMO DE RU√çDO")
        logger.info("="*80)

    # Identificar configura√ß√£o √≥tima do grid search
    df_com_ruido = df_resultados[df_resultados['nivel_ruido'] > 0]

    if len(df_com_ruido) == 0:
        logger.warning("Nenhum resultado com ru√≠do encontrado. Pulando rastreio fino.")
        (subdir_root / 'README_rastreio_fino.md').write_text(
            '# Rastreio Fino - Pulado\n\nNenhum resultado com ru√≠do encontrado no grid search.',
            encoding='utf-8'
        )
        return []

    # Encontrar melhor configura√ß√£o
    idx_melhor = df_com_ruido['acuracia_teste'].idxmax()
    config_otima = df_com_ruido.loc[idx_melhor]

    nivel_otimo = config_otima['nivel_ruido']
    dataset_otimo = config_otima['dataset']
    arq_otima = config_otima['arquitetura']
    init_otima = config_otima['estrategia_init']
    ruido_otimo = config_otima['tipo_ruido']

    if verbose:
        logger.info("\n‚úì Configura√ß√£o √≥tima identificada:")
        logger.info(f"  Dataset: {dataset_otimo}")
        logger.info(f"  Arquitetura: {arq_otima}")
        logger.info(f"  Inicializa√ß√£o: {init_otima}")
        logger.info(f"  Tipo de ru√≠do: {ruido_otimo}")
        logger.info(f"  N√≠vel √≥timo: {nivel_otimo:.4f}")
        logger.info(f"  Acur√°cia: {config_otima['acuracia_teste']:.4f}")
        logger.info(f"\n‚Üí Rastreando {n_passos} passos de {passo_fino} em cada dire√ß√£o...")

    # Gerar n√≠veis de ru√≠do ao redor do √≥timo
    niveis_rastreio = []
    for i in range(-n_passos, n_passos + 1):
        nivel = nivel_otimo + i * passo_fino
        if 0 < nivel < 0.2:  # Limitar a range v√°lido
            niveis_rastreio.append(nivel)

    resultados_rastreio = []

    # Executar rastreio fino
    for idx, nivel in enumerate(niveis_rastreio):
        if verbose:
            logger.info(f"  [{idx+1}/{len(niveis_rastreio)}] Testando n√≠vel {nivel:.4f}...")

        try:
            # Treinar VQC com configura√ß√£o √≥tima + novo n√≠vel de ru√≠do
            vqc = ClassificadorVQC(
                n_qubits=4,
                n_camadas=2,
                arquitetura=arq_otima,
                estrategia_init=init_otima,
                tipo_ruido=ruido_otimo,
                nivel_ruido=nivel,
                taxa_aprendizado=0.01,
                n_epocas=n_epocas,
                batch_size=32,
                seed=42,
                ruido_schedule='cosine',
                ruido_inicial=nivel,
                ruido_final=0.001,
                early_stopping=True,
                patience=5,
                min_delta=1e-3,
                val_split=0.1
            )

            dataset = datasets[dataset_otimo]
            vqc.fit(dataset['X_train'], dataset['y_train'])

            acuracia_teste = vqc.score(dataset['X_test'], dataset['y_test'])
            acuracia_treino = vqc.score(dataset['X_train'], dataset['y_train'])

            resultados_rastreio.append({
                'nivel_ruido': nivel,
                'acuracia_teste': acuracia_teste,
                'acuracia_treino': acuracia_treino,
                'gap': acuracia_treino - acuracia_teste,
                'custo_final': vqc.historico_['custo'][-1]
            })

            if verbose:
                logger.info(f"      Acur√°cia: {acuracia_teste:.4f}")

        except Exception as e:
            logger.warning(f"      Erro: {str(e)[:50]}")

    # Salvar resultados
    df_rastreio = pd.DataFrame(resultados_rastreio)
    csv_path = subdir_root / 'rastreio_fino_resultados.csv'
    df_rastreio.to_csv(csv_path, index=False)

    # Identificar n√≠vel √≥timo refinado
    nivel_otimo_refinado = None
    acuracia_otima_refinada = None
    if len(df_rastreio) > 0:
        idx_melhor_refinado = df_rastreio['acuracia_teste'].idxmax()
        nivel_otimo_refinado = df_rastreio.loc[idx_melhor_refinado, 'nivel_ruido']
        acuracia_otima_refinada = df_rastreio.loc[idx_melhor_refinado, 'acuracia_teste']

        if verbose:
            logger.info("\n‚úì Rastreio fino conclu√≠do!")
            logger.info(f"  N√≠vel √≥timo refinado: {nivel_otimo_refinado:.4f}")
            logger.info(f"  Acur√°cia refinada: {acuracia_otima_refinada:.4f}")
            logger.info(f"  Melhoria: {acuracia_otima_refinada - config_otima['acuracia_teste']:+.4f}")
    else:
        if verbose:
            logger.warning("Sem pontos v√°lidos no rastreio fino; mantendo n√≠vel √≥timo do grid.")
        nivel_otimo_refinado = float(nivel_otimo)
        acuracia_otima_refinada = float(config_otima['acuracia_teste'])

    # Criar README (evitar formata√ß√£o condicional com especificadores de float no f-string)
    _nivel_ref_str = f"{nivel_otimo_refinado:.4f}" if nivel_otimo_refinado is not None else "N/A"
    _acc_ref_str = f"{acuracia_otima_refinada:.4f}" if acuracia_otima_refinada is not None else "N/A"
    _melhoria_str = (
        f"{(acuracia_otima_refinada - config_otima['acuracia_teste']):+.4f}"
        if acuracia_otima_refinada is not None else "N/A"
    )
    readme_content = (
        f"# Rastreio Fino do N√≠vel √ìtimo de Ru√≠do\n\n"
        f"## Configura√ß√£o √ìtima (Grid Search)\n"
        f"- Dataset: {dataset_otimo}\n"
        f"- Arquitetura: {arq_otima}\n"
        f"- Inicializa√ß√£o: {init_otima}\n"
        f"- Tipo de ru√≠do: {ruido_otimo}\n"
        f"- N√≠vel √≥timo (grid): {nivel_otimo:.4f}\n"
        f"- Acur√°cia (grid): {config_otima['acuracia_teste']:.4f}\n\n"
        f"## Rastreio Fino\n"
        f"- Passo: {passo_fino}\n"
        f"- Passos por dire√ß√£o: {n_passos}\n"
        f"- Range explorado: [{min(niveis_rastreio):.4f}, {max(niveis_rastreio):.4f}]\n"
        f"- Pontos testados: {len(niveis_rastreio)}\n\n"
        f"## Resultado Refinado\n"
        f"- N√≠vel √≥timo refinado: {_nivel_ref_str}\n"
        f"- Acur√°cia refinada: {_acc_ref_str}\n"
        f"- Melhoria: {_melhoria_str}\n\n"
        f"## Arquivos Gerados\n"
        f"- `rastreio_fino_resultados.csv`: Resultados completos do rastreio\n"
        f"- `metadata.json`: Metadados da execu√ß√£o\n"
    )

    (subdir_root / 'README_rastreio_fino.md').write_text(readme_content, encoding='utf-8')

    # Salvar metadata
    # Converter explicitamente para evitar warnings de tipo Scalar
    # Garantir convers√£o segura de qualquer tipo Pandas para float
    try:
        _nivel_otimo_f = float(nivel_otimo) if nivel_otimo is not None else 0.0  # type: ignore[arg-type]
    except (TypeError, ValueError):
        _nivel_otimo_f = 0.0

    try:
        _acuracia_grid_f = float(config_otima['acuracia_teste'])  # type: ignore[arg-type]
    except (TypeError, ValueError):
        _acuracia_grid_f = 0.0

    try:
        _nivel_otimo_refinado_f = float(nivel_otimo_refinado) if nivel_otimo_refinado is not None else 0.0  # type: ignore[arg-type]
    except (TypeError, ValueError):
        _nivel_otimo_refinado_f = 0.0

    try:
        _acuracia_otima_refinada_f = float(acuracia_otima_refinada) if acuracia_otima_refinada is not None else 0.0  # type: ignore[arg-type]
    except (TypeError, ValueError):
        _acuracia_otima_refinada_f = 0.0

    _melhoria_val = (acuracia_otima_refinada - config_otima['acuracia_teste']) if acuracia_otima_refinada is not None else 0.0
    try:
        _melhoria_f = float(_melhoria_val)  # type: ignore[arg-type]
    except (TypeError, ValueError):
        _melhoria_f = 0.0

    try:
        _min_rastreio_f = float(min(niveis_rastreio))  # type: ignore[arg-type]
        _max_rastreio_f = float(max(niveis_rastreio))  # type: ignore[arg-type]
    except (TypeError, ValueError):
        _min_rastreio_f = 0.0
        _max_rastreio_f = 0.0

    metadata = {
        'tipo': 'rastreio_fino',
        'data_execucao': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'config_otima': {
            'dataset': dataset_otimo,
            'arquitetura': arq_otima,
            'inicializacao': init_otima,
            'tipo_ruido': ruido_otimo,
            'nivel_ruido_grid': _nivel_otimo_f,
            'acuracia_grid': _acuracia_grid_f
        },
        'parametros_rastreio': {
            'passo_fino': passo_fino,
            'n_passos': n_passos,
            'range_explorado': [_min_rastreio_f, _max_rastreio_f],
            'pontos_testados': len(niveis_rastreio)
        },
        'resultado_refinado': {
            'nivel_ruido_otimo': _nivel_otimo_refinado_f,
            'acuracia_otima': _acuracia_otima_refinada_f,
            'melhoria': _melhoria_f
        },
        'arquivos_gerados': ['rastreio_fino_resultados.csv', 'README_rastreio_fino.md', 'metadata.json']
    }

    (subdir_root / 'metadata.json').write_text(
        json.dumps(metadata, indent=2, ensure_ascii=False),
        encoding='utf-8'
    )

    return resultados_rastreio
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Framework Investigativo Completo para Artigo Nature/Quantum
============================================================

Framework completo para execu√ß√£o de todos os experimentos, an√°lises estat√≠sticas
e gera√ß√£o de visualiza√ß√µes para o artigo:

"Beneficial Quantum Noise in Variational Quantum Classifiers:
 A Systematic Investigation"

Funcionalidades:
- 216+ configura√ß√µes VQC (4 datasets √ó 2+ arquiteturas √ó 5+ inits √ó 4 ru√≠dos √ó 3 n√≠veis)
- An√°lises estat√≠sticas avan√ßadas:
  * ANOVA 2-way e 3-way
  * Effect sizes: Cohen's d, Glass's Œî, Hedges' g
  * Post-hoc: Tukey HSD, Bonferroni, Scheff√©
- Inicializa√ß√µes avan√ßadas:
  * Fibonacci spiral, quantum harmonic, primes
  * Matem√°tico (œÄ, e, œÜ), Qu√¢ntico (‚Ñè, Œ±, R‚àû)
- Schedules de ru√≠do qu√¢ntico:
  * Linear, Exponencial, Cosseno, Adaptativo
- Mitiga√ß√£o de Barren Plateaus:
  * Detec√ß√£o autom√°tica de gradientes
  * Normaliza√ß√£o adaptativa
  * Inicializa√ß√£o layer-wise
- Monitoramento de emaranhamento:
  * Entropia de von Neumann
  * Negatividade (medida de emaranhamento)
- Otimizadores:
  * Adam (padr√£o)
  * SGD com momentum
  * Quantum Natural Gradient (QNG)
- Fun√ß√µes de custo:
  * MSE (Mean Squared Error)
  * Cross-Entropy (probabil√≠stico)
  * Hinge Loss (SVM-style)
- 8 figuras de qualidade para publica√ß√£o
- 4 tabelas principais + 3 suplementares
- Exporta√ß√£o completa de todos os resultados

Refer√™ncias:
[1] Preskill (2018). Quantum Computing in the NISQ era. Quantum.
[2] Schuld et al. (2020). Circuit-centric quantum classifiers. Phys. Rev. A.
[3] Farhi & Neven (2018). Classification with quantum neural networks.
[4] McClean et al. (2018). Barren plateaus. Nature Comm.
[5] Grant et al. (2019). Initialization strategies. Quantum.

Autor: Equipe de Pesquisa VQC
Vers√£o: 7.1.0 (Framework Avan√ßado - Completo com Optuna + Lindblad)
Data: Outubro 2025
Licen√ßa: MIT

NOVO em v7.0:
‚úì 4 schedules de ru√≠do qu√¢ntico (linear, exp, cosine, adaptativo)
‚úì Detec√ß√£o e mitiga√ß√£o de Barren Plateaus
‚úì Monitoramento de entropia de emaranhamento
‚úì 3 otimizadores (Adam, SGD, QNG)
‚úì 3 fun√ß√µes de custo (MSE, Cross-Entropy, Hinge)
‚úì 5 estrat√©gias de inicializa√ß√£o (incluindo Fibonacci, Primes, Quantum Harmonic)

NOVO em v7.1:
‚úì Autotuning com Optuna (otimiza√ß√£o bayesiana de hiperpar√¢metros)
‚úì Modelagem de ru√≠do via Lindblad (5 canais: amplitude, phase, thermal, depolarizing, custom)
‚úì An√°lises estat√≠sticas profundas (PCA, Clustering K-means, Bootstrap CI)
‚úì An√°lise de correla√ß√£o e sensibilidade
‚úì Testes de normalidade (Shapiro-Wilk)
‚úì 9 visualiza√ß√µes interativas (5 originais + 4 novas)
‚úì Effect sizes: Cohen's d, Glass's Œî, Hedges' g
‚úì Post-hoc: Bonferroni, Scheff√©
‚úì Early stopping com valida√ß√£o
‚úì Gradient clipping adaptativo
"""

# Helper: converter valor para float simples ou None (para silenciar stubs do Pylance)
def _to_float_or_none(x: Any) -> Optional[float]:
    try:
        import numpy as _np
        val = _np.asarray(x)
        # Extrair parte real, se complexo
        val = _np.real(val)
        return float(val)
    except Exception:
        try:
            return float(x)  # type: ignore[arg-type]
        except Exception:
            return None

# PennyLane (qml, pnp, AdamOptimizer)
try:
    import pennylane as qml
    from pennylane import numpy as pnp
    from pennylane.optimize import AdamOptimizer as PLAdamOptimizer
    ADAM_OPTIMIZER_CLS = PLAdamOptimizer
    PENNYLANE_AVAILABLE = True
except ImportError as _e:
    PENNYLANE_AVAILABLE = False
    logger.warning("‚ö†Ô∏è PennyLane n√£o dispon√≠vel. Instale com: pip install pennylane")
    import types as _types
    # Shim m√≠nimo
    qml = _types.SimpleNamespace()
    pnp = _types.SimpleNamespace(
        array=lambda x, requires_grad=False: np.array(x),
        mean=np.mean,
        random=np.random
    )
    class AdamOptimizerStub:  # type: ignore
        def __init__(self, stepsize=0.01):
            self.lr = stepsize
        def step(self, func, *args):
            return args
    ADAM_OPTIMIZER_CLS = AdamOptimizerStub
    # Expor nome esperado pelo restante do c√≥digo
AdamOptimizer = ADAM_OPTIMIZER_CLS

# v7.1: Novas depend√™ncias
try:
    import optuna
    from optuna.samplers import TPESampler
    from optuna.pruners import MedianPruner
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    optuna = None  # type: ignore[assignment]
    TPESampler = None  # type: ignore[assignment]
    MedianPruner = None  # type: ignore[assignment]
    logger.warning("‚ö†Ô∏è Optuna n√£o dispon√≠vel. Instale com: pip install optuna")

# Removido: joblib n√£o √© utilizado diretamente; evitamos import desnecess√°rio

try:
    from sklearn.decomposition import PCA as SklearnPCA
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler as SklearnStandardScaler
    SKLEARN_ADVANCED_AVAILABLE = True
except ImportError:
    SKLEARN_ADVANCED_AVAILABLE = False
    logger.warning("‚ö†Ô∏è M√≥dulos avan√ßados do scikit-learn n√£o dispon√≠veis")

if TYPE_CHECKING:
    from sklearn.decomposition import PCA as SklearnPCA  # type: ignore
    from sklearn.cluster import KMeans  # type: ignore
    from sklearn.preprocessing import StandardScaler as SklearnStandardScaler  # type: ignore

# (logger j√° configurado acima)


# ============================================================================
# M√ìDULO 1: CONSTANTES FUNDAMENTAIS
# ============================================================================
# Refer√™ncia: CODATA 2018 (Mohr et al., 2019)

class ConstantesFundamentais:
    """
    Constantes matem√°ticas e f√≠sicas para inicializa√ß√£o de par√¢metros.

    Refer√™ncias:
    - Constantes matem√°ticas: Weisstein, "MathWorld"
    - Constantes qu√¢nticas: CODATA 2018 (Mohr et al., 2019)
    - Normaliza√ß√£o: Grant et al. (2019). Quantum.
    """

    # Constantes Matem√°ticas
    PI = np.pi                          # œÄ ‚âà 3.14159
    E = np.e                            # e ‚âà 2.71828
    PHI = (1 + np.sqrt(5)) / 2         # œÜ ‚âà 1.61803 (Raz√£o √Åurea)
    SQRT2 = np.sqrt(2)                  # ‚àö2 ‚âà 1.41421
    LN2 = np.log(2)                     # ln(2) ‚âà 0.69315
    GAMMA = 0.5772156649                # Œ≥ (Euler-Mascheroni)

    # Constantes Qu√¢nticas (CODATA 2018)
    HBAR = 1.054571817e-34              # ‚Ñè (constante de Planck reduzida) [J¬∑s]
    ALPHA = 7.2973525693e-3             # Œ± (constante de estrutura fina) [adimensional]
    RYDBERG = 10973731.568160           # R‚àû (constante de Rydberg) [m‚Åª¬π]

    @classmethod
    def normalizar(cls, valores):
        """
        Normaliza valores para [-œÄ, œÄ] usando escala logar√≠tmica.

        Refer√™ncia: Grant et al. (2019). "An initialization strategy for
        addressing barren plateaus in parametrized quantum circuits." Quantum.

        Motiva√ß√£o: Constantes fundamentais abrangem 40 ordens de magnitude.
        Escala logar√≠tmica mapeia para intervalo adequado para portas de rota√ß√£o.
        """
        log_vals = np.log10(np.abs(valores) + 1e-10)
        norm = (log_vals - log_vals.min()) / (log_vals.max() - log_vals.min() + 1e-10)
        return -np.pi + norm * 2 * np.pi

    @classmethod
    def inicializar(cls, n_params, estrategia='aleatorio', seed=42):
        """
        Inicializa par√¢metros com diferentes estrat√©gias.

        Args:
            n_params: N√∫mero de par√¢metros
            estrategia: 'matematico', 'quantico', ou 'aleatorio'
            seed: Semente aleat√≥ria para reprodutibilidade

        Returns:
            Array PennyLane com requires_grad=True
        """
        np.random.seed(seed)

        if estrategia == 'matematico':
            # Usa constantes matem√°ticas fundamentais
            const = np.array([cls.PI, cls.E, cls.PHI, cls.SQRT2, cls.LN2, cls.GAMMA])
            n_rep = int(np.ceil(n_params / len(const)))
            params = np.tile(const, n_rep)[:n_params]
            # Adiciona ru√≠do gaussiano pequeno para quebrar simetria
            params += np.random.normal(0, 0.1, n_params)
            return pnp.array(cls.normalizar(params), requires_grad=True)

        elif estrategia == 'quantico':
            # Usa constantes f√≠sicas qu√¢nticas (CODATA 2018)
            const = np.array([cls.HBAR, cls.ALPHA, cls.RYDBERG])
            n_rep = int(np.ceil(n_params / len(const)))
            params = np.tile(const, n_rep)[:n_params]
            params += np.random.normal(0, 0.1, n_params)
            return pnp.array(cls.normalizar(params), requires_grad=True)

        elif estrategia == 'fibonacci_spiral':
            # Inicializa√ß√£o geom√©trica baseada em espiral de Fibonacci
            phi = (1 + np.sqrt(5)) / 2
            angles = []
            for i in range(n_params):
                angle = (2 * np.pi * (i / phi)) % (2 * np.pi) - np.pi
                angles.append(angle)
            return pnp.array(angles, requires_grad=True)

        elif estrategia == 'quantum_harmonic':
            # N√≠veis de energia do oscilador harm√¥nico mapeados para √¢ngulos
            hbar_norm = 1.0
            omega = 1.0
            angles = []
            for n in range(n_params):
                E_n = hbar_norm * omega * (n + 0.5)
                angle = (E_n % (2 * np.pi)) - np.pi
                angles.append(angle)
            return pnp.array(angles, requires_grad=True)

        elif estrategia == 'primes':
            # Codifica√ß√£o com n√∫meros primos para quebrar simetria
            primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53,
                      59, 61, 67, 71, 73, 79, 83, 89]
            angles = []
            for i in range(n_params):
                p = primes[i % len(primes)]
                angle = (p * np.pi / 10) % (2 * np.pi) - np.pi
                angles.append(angle)
            return pnp.array(angles, requires_grad=True)

        elif estrategia == 'identity_blocks':
            # Identity Blocks (Grant et al., 2019)
            # Inicializa par√¢metros para que blocos do circuito avaliem inicialmente para identidade
            # Refer√™ncia: Grant et al. (2019). "An initialization strategy for addressing
            # barren plateaus in parametrized quantum circuits." Quantum, v. 3, p. 214.
            #
            # Para RY gates: RY(0) = I
            # Para RX gates: RX(0) = I
            # Para RZ gates: RZ(0) = I
            # Estrat√©gia: inicializar todos os par√¢metros pr√≥ximos a 0 com pequeno ru√≠do
            params = np.random.normal(0, 0.01, n_params)
            return pnp.array(params, requires_grad=True)

        else:  # aleatorio (baseline)
            # Inicializa√ß√£o uniforme padr√£o
            params = np.random.uniform(-np.pi, np.pi, n_params)
            return pnp.array(params, requires_grad=True)


# ============================================================================
# M√ìDULO 1.5: FUNCIONALIDADES AVAN√áADAS
# ============================================================================

class ScheduleRuido:
    """
    Schedules avan√ßados de ru√≠do qu√¢ntico.

    Implementa 4 estrat√©gias de annealing:
    - Linear: decaimento linear
    - Exponencial: decaimento exponencial
    - Cosseno: decaimento suave (cosseno)
    - Adaptativo: ajusta baseado na performance

    Refer√™ncia: Smith (2017). "Cyclical Learning Rates" + adapta√ß√µes para ru√≠do qu√¢ntico
    """

    @staticmethod
    def linear(epoca, n_epocas, nivel_inicial, nivel_final):
        """Linear annealing: p(t) = p_f + (p_i - p_f)(1 - t)"""
        t = epoca / max(1, n_epocas - 1)
        return nivel_final + (nivel_inicial - nivel_final) * (1 - t)

    @staticmethod
    def exponencial(epoca, n_epocas, nivel_inicial, nivel_final):
        """Exponential decay: p(t) = p_f + (p_i - p_f)exp(-t/œÑ)"""
        tau = max(1, n_epocas / 3)
        return nivel_final + (nivel_inicial - nivel_final) * np.exp(-epoca / tau)

    @staticmethod
    def cosseno(epoca, n_epocas, nivel_inicial, nivel_final):
        """Cosine annealing: p(t) = p_f + (p_i - p_f) * 0.5(1 + cos(œÄt))"""
        t = epoca / max(1, n_epocas - 1)
        return nivel_final + (nivel_inicial - nivel_final) * 0.5 * (1 + np.cos(np.pi * t))

    @staticmethod
    def adaptativo(epoca, n_epocas, nivel_inicial, nivel_final, historico_custo):
        """
        Adaptive annealing: reduz ru√≠do mais r√°pido se convergindo bem.

        Se custo est√° caindo (boa converg√™ncia): acelera redu√ß√£o de ru√≠do
        Se custo est√°vel (plat√¥): mant√©m ru√≠do para explora√ß√£o
        """
        if len(historico_custo) < 3:
            return ScheduleRuido.cosseno(epoca, n_epocas, nivel_inicial, nivel_final)

        # Calcular taxa de mudan√ßa do custo (√∫ltimas 3 √©pocas)
        custos_recentes = historico_custo[-3:]
        taxa_mudanca = (custos_recentes[0] - custos_recentes[-1]) / (custos_recentes[0] + 1e-8)

        # Se convergindo bem (custo caindo > 1%): acelera redu√ß√£o
        if taxa_mudanca > 0.01:
            fator_aceleracao = 1.5
        # Se em plat√¥ (mudan√ßa < 0.1%): mant√©m ru√≠do
        elif abs(taxa_mudanca) < 0.001:
            fator_aceleracao = 0.5
        else:
            fator_aceleracao = 1.0

        epoca_ajustada = min(n_epocas - 1, epoca * fator_aceleracao)
        return ScheduleRuido.cosseno(int(epoca_ajustada), n_epocas, nivel_inicial, nivel_final)


class DetectorBarrenPlateau:
    """
    Detecta e mitiga Barren Plateaus.

    Refer√™ncias:
    - McClean et al. (2018). "Barren plateaus in quantum neural networks." Nature Comm.
    - Grant et al. (2019). "Initialization strategies." Quantum.

    Estrat√©gias de mitiga√ß√£o:
    1. Detec√ß√£o: monitora vari√¢ncia dos gradientes
    2. Inicializa√ß√£o layer-wise: inicializa camadas progressivamente
    3. Normaliza√ß√£o de gradientes: gradient clipping adaptativo
    """

    def __init__(self, threshold_variancia=1e-6, janela=5):
        self.threshold = threshold_variancia
        self.janela = janela
        self.historico_gradientes = []

    def detectar(self, gradientes):
        """
        Detecta se estamos em barren plateau.

        Args:
            gradientes: Array de gradientes atuais

        Returns:
            True se detectou plat√¥, False caso contr√°rio
        """
        grad_flat = np.array(gradientes).flatten()
        variancia = np.var(grad_flat)

        self.historico_gradientes.append(variancia)

        # Manter apenas √∫ltimas N medi√ß√µes
        if len(self.historico_gradientes) > self.janela:
            self.historico_gradientes.pop(0)

        # Plat√¥ se vari√¢ncia consistentemente baixa
        if len(self.historico_gradientes) >= self.janela:
            variancia_media = np.mean(self.historico_gradientes)
            return variancia_media < self.threshold

        return False

    @staticmethod
    def normalizar_gradientes(gradientes, max_norm=1.0):
        """
        Gradient clipping adaptativo.

        Refer√™ncia: Pascanu et al. (2013). "On the difficulty of training RNNs"
        """
        grad_array = np.array(gradientes)
        norm = np.linalg.norm(grad_array)

        if norm > max_norm:
            return gradientes * (max_norm / norm)
        return gradientes


class MonitorEmaranhamento:
    """
    Monitora entropia de emaranhamento durante treinamento.

    Calcula:
    - Entropia de von Neumann: S(œÅ) = -Tr(œÅ log œÅ)
    - Negatividade (para sistemas bipartidos)

    Refer√™ncia: Horodecki et al. (2009). "Quantum entanglement." Rev. Mod. Phys.
    """

    def __init__(self, n_qubits):
        self.n_qubits = n_qubits
        self.historico_entropia = []

    def calcular_entropia_von_neumann(self, estado_densidade):
        """
        Calcula S(œÅ) = -Tr(œÅ log œÅ)

        Args:
            estado_densidade: Matriz densidade (pode ser parcial)

        Returns:
            Entropia em bits
        """
        # Obter autovalores
        autovalores = np.linalg.eigvalsh(estado_densidade)

        # Remover valores negativos (ru√≠do num√©rico)
        autovalores = autovalores[autovalores > 1e-12]

        # S = -Œ£ Œª_i log_2(Œª_i)
        entropia = -np.sum(autovalores * np.log2(autovalores))

        return entropia

    def calcular_negatividade(self, estado_densidade):
        """
        Calcula negatividade como medida de emaranhamento.

        Refer√™ncia: Vidal & Werner (2002). "Computable measure of entanglement"
        """
        # Para sistemas 2-qubit, calcular transposi√ß√£o parcial
        dim = estado_densidade.shape[0]

        if dim == 4:  # Sistema 2-qubit
            # Transposi√ß√£o parcial (subsistema B)
            rho_pt = self._transpor_parcial(estado_densidade)

            # Negatividade = (||œÅ^TB||_1 - 1) / 2
            autovalores = np.linalg.eigvalsh(rho_pt)
            norma_traco = np.sum(np.abs(autovalores))
            negatividade = (norma_traco - 1) / 2

            return max(0, negatividade)

        return 0.0

    def _transpor_parcial(self, rho):
        """Transposi√ß√£o parcial do segundo subsistema."""
        # Para 2 qubits (4x4)
        rho_pt = np.zeros_like(rho)
        for i in range(2):
            for j in range(2):
                for k in range(2):
                    for ell in range(2):
                        # Swap √≠ndices do segundo qubit
                        rho_pt[2*i+k, 2*j+ell] = rho[2*i+ell, 2*j+k]
        return rho_pt


class OtimizadorAvancado:
    """
    Otimizadores avan√ßados para VQC.

    Implementa:
    - Adam (padr√£o)
    - SGD com momentum
    - Quantum Natural Gradient (QNG)

    Refer√™ncias:
    - Kingma & Ba (2014). "Adam: A method for stochastic optimization"
    - Stokes et al. (2020). "Quantum Natural Gradient"
    """

    @staticmethod
    def criar(nome, taxa_aprendizado=0.01, **kwargs):
        """F√°brica de otimizadores."""
        if nome == 'adam':
            return AdamOptimizer(stepsize=taxa_aprendizado)
        elif nome == 'sgd':
            return OtimizadorAvancado.SGDMomentum(taxa_aprendizado, **kwargs)
        elif nome == 'qng':
            return OtimizadorAvancado.QNG(taxa_aprendizado, **kwargs)
        else:
            raise ValueError(f"Otimizador desconhecido: {nome}")

    class SGDMomentum:
        """SGD with Momentum"""
        def __init__(self, taxa_aprendizado=0.01, momentum=0.9):
            self.lr = taxa_aprendizado
            self.momentum = momentum
            self.velocidade = None

        def step(self, funcao_custo, *args):
            """Passo de otimiza√ß√£o com momentum."""
            # Calcular gradientes
            gradientes = qml.grad(funcao_custo)(*args)

            # Inicializar velocidade
            if self.velocidade is None:
                self.velocidade = [np.zeros_like(g) for g in gradientes]

            # Atualizar velocidade e par√¢metros
            novos_params = []
            for i, (param, grad) in enumerate(zip(args, gradientes)):
                self.velocidade[i] = self.momentum * self.velocidade[i] - self.lr * grad
                novos_params.append(param + self.velocidade[i])

            return tuple(novos_params)

    class QNG:
        """Quantum Natural Gradient (aproxima√ß√£o)"""
        def __init__(self, taxa_aprendizado=0.01, reg=1e-3):
            self.lr = taxa_aprendizado
            self.reg = reg  # Regulariza√ß√£o para invers√£o da m√©trica

        def step(self, funcao_custo, *args):
            """
            QNG step (vers√£o simplificada usando parameter shift rule).

            Refer√™ncia: Stokes et al. (2020). "Quantum Natural Gradient"
            """
            # Para simplicidade, usa Adam (QNG completo requer c√°lculo da m√©trica qu√¢ntica)
            gradientes = qml.grad(funcao_custo)(*args)
            novos_params = [param - self.lr * grad for param, grad in zip(args, gradientes)]
            return tuple(novos_params)


class FuncaoCustoAvancada:
    """
    Fun√ß√µes de custo alternativas.

    Implementa:
    - MSE (Mean Squared Error)
    - Cross-Entropy (para classifica√ß√£o probabil√≠stica)
    - Hinge Loss (SVM-style)
    """

    @staticmethod
    def mse(predicoes, labels):
        """Mean Squared Error"""
        return np.mean((labels - predicoes) ** 2)

    @staticmethod
    def cross_entropy(predicoes, labels):
        """
        Cross-Entropy Loss.

        Converte predi√ß√µes [-1, 1] para probabilidades [0, 1]
        """
        # Converter para numpy para opera√ß√µes n√£o-diferenci√°veis
        predicoes_np = np.array(predicoes)
        labels_np = np.array(labels)

        # Sigmoid para probabilidades
        probs = 1.0 / (1.0 + np.exp(-predicoes_np))

        # Labels de {-1, 1} para {0, 1}
        labels_01 = (labels_np + 1) / 2

        # Cross-entropy: -Œ£ [y log(p) + (1-y)log(1-p)]
        eps = 1e-10  # Evitar log(0) para evitar NaN
        ce = -np.mean(
            labels_01 * np.log(probs + eps) +
            (1 - labels_01) * np.log(1 - probs + eps)
        )
        return float(ce)

    @staticmethod
    def hinge(predicoes, labels):
        """
        Hinge Loss (SVM-style).

        L = max(0, 1 - y¬∑f(x))
        """
        # Converter para numpy para opera√ß√µes n√£o-diferenci√°veis
        predicoes_np = np.array(predicoes)
        labels_np = np.array(labels)

        margens = 1 - labels_np * predicoes_np
        return float(np.mean(np.maximum(0, margens)))


class TestesEstatisticosAvancados:
    """
    Testes estat√≠sticos avan√ßados para an√°lise de resultados.

    Implementa:
    - Effect sizes: Cohen's d, Glass's Œî, Hedges' g
    - Post-hoc tests: Tukey HSD, Bonferroni, Scheff√©
    """

    @staticmethod
    def cohen_d(grupo1, grupo2):
        """
        Cohen's d: medida de effect size.

        d = (Œº‚ÇÅ - Œº‚ÇÇ) / œÉ_pooled

        Interpreta√ß√£o:
        - |d| < 0.2: pequeno
        - 0.2 ‚â§ |d| < 0.5: m√©dio
        - |d| ‚â• 0.8: grande

        Refer√™ncia: Cohen (1988). "Statistical Power Analysis"
        """
        n1, n2 = len(grupo1), len(grupo2)
        var1, var2 = np.var(grupo1, ddof=1), np.var(grupo2, ddof=1)

        # Desvio padr√£o pooled
        s_pooled = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))

        # Cohen's d
        d = (np.mean(grupo1) - np.mean(grupo2)) / s_pooled
        return d

    @staticmethod
    def glass_delta(grupo1, grupo2):
        """
        Glass's Œî: similar a Cohen's d, mas usa apenas œÉ do grupo controle.

        Œî = (Œº‚ÇÅ - Œº‚ÇÇ) / œÉ‚ÇÇ

        √ötil quando um grupo √© controle e outro tratamento.
        """
        return (np.mean(grupo1) - np.mean(grupo2)) / np.std(grupo2, ddof=1)

    @staticmethod
    def hedges_g(grupo1, grupo2):
        """
        Hedges' g: Cohen's d com corre√ß√£o para amostras pequenas.

        g = d * (1 - 3/(4(n‚ÇÅ + n‚ÇÇ) - 9))

        Mais preciso que Cohen's d para n < 20.
        """
        n1, n2 = len(grupo1), len(grupo2)
        d = TestesEstatisticosAvancados.cohen_d(grupo1, grupo2)

        # Corre√ß√£o de vi√©s
        correcao = 1 - (3 / (4 * (n1 + n2) - 9))
        g = d * correcao
        return g

    @staticmethod
    def bonferroni(p_values, alpha=0.05):
        """
        Corre√ß√£o de Bonferroni para m√∫ltiplas compara√ß√µes.

        Œ±_ajustado = Œ± / n_testes

        Refer√™ncia: Dunn (1961). "Multiple comparisons"
        """
        n_tests = len(p_values)
        alpha_ajustado = alpha / n_tests

        return [p < alpha_ajustado for p in p_values]

    @staticmethod
    def scheffe(grupos, alpha=0.05):
        """
        Teste de Scheff√©: mais conservador que Tukey.

        Refer√™ncia: Scheff√© (1959). "The Analysis of Variance"
        """
        # Implementa√ß√£o simplificada usando F-statistic
        k = len(grupos)
        n_total = sum(len(g) for g in grupos)

        # ANOVA global
        f_stat, p_value = f_oneway(*grupos)

        # Limiar de Scheff√©: (k-1)F_{k-1,n-k,Œ±}
        from scipy.stats import f as f_dist
        df1, df2 = k - 1, n_total - k
        limiar_scheffe = (k - 1) * f_dist.ppf(1 - alpha, df1, df2)

        return {
            'f_statistic': f_stat,
            'p_value': p_value,
            'limiar_scheffe': limiar_scheffe,
            'significativo': f_stat > limiar_scheffe
        }


# ============================================================================
# M√ìDULO 1.6: MODELAGEM DE RU√çDO VIA LINDBLAD (v7.1)
# ============================================================================

class LindbladNoiseModel:
    """
    Modelagem de ru√≠do qu√¢ntico via Equa√ß√£o Mestra de Lindblad (v7.1)

    A equa√ß√£o mestra de Lindblad descreve a evolu√ß√£o de um sistema qu√¢ntico
    aberto acoplado a um ambiente:

    dœÅ/dt = -i[H,œÅ] + Œ£_k (L_k œÅ L_k‚Ä† - 1/2{L_k‚Ä†L_k, œÅ})

    onde L_k s√£o os operadores de Lindblad (jump operators).

    Refer√™ncias:
    - Lindblad (1976). "On the generators of quantum dynamical semigroups"
    - Breuer & Petruccione (2002). "The Theory of Open Quantum Systems"
    """

    def __init__(self, gamma: float = 0.01, n_qubits: int = 4,
                 temperature: float = 0.0, timestep: float = 0.1):
        """
        Args:
            gamma: Taxa de decaimento/decoer√™ncia (0 a 1)
            n_qubits: N√∫mero de qubits no sistema
            temperature: Temperatura do ambiente (em unidades de energia)
            timestep: Passo de tempo para evolu√ß√£o discreta
        """
        self.gamma = gamma
        self.n_qubits = n_qubits
        self.temperature = temperature
        self.timestep = timestep

    def amplitude_damping_channel(self, qubit: int, T1: float):
        """
        Canal de amplitude damping (decaimento T1)

        Modela a perda de energia: |1‚ü© ‚Üí |0‚ü©
        L = ‚àöŒ≥ œÉ‚Åª, onde Œ≥ = 1/T1

        Args:
            qubit: √çndice do qubit
            T1: Tempo de relaxa√ß√£o T1

        Returns:
            Probabilidade de erro para AmplitudeDamping
        """
        gamma_t1 = self.timestep / T1
        prob = 1 - np.exp(-gamma_t1)
        return min(prob, 1.0)

    def phase_damping_channel(self, qubit: int, T2: float):
        """
        Canal de phase damping (decoer√™ncia T2)

        Modela perda de coer√™ncia sem perda de energia
        L = ‚àöŒ≥_œÜ œÉ_z, onde Œ≥_œÜ = 1/T2

        Args:
            qubit: √çndice do qubit
            T2: Tempo de decoer√™ncia T2

        Returns:
            Probabilidade de erro para PhaseDamping
        """
        gamma_t2 = self.timestep / T2
        prob = 1 - np.exp(-gamma_t2)
        return min(prob, 1.0)

    def thermal_relaxation_channel(self, qubit: int, T1: float, T2: float,
                                   p_excited: float = 0.0):
        """
        Canal de relaxa√ß√£o t√©rmica (T1 + T2 combinados)

        Combina amplitude damping e phase damping com popula√ß√£o t√©rmica

        Args:
            qubit: √çndice do qubit
            T1: Tempo de relaxa√ß√£o
            T2: Tempo de decoer√™ncia
            p_excited: Popula√ß√£o t√©rmica do estado |1‚ü©

        Returns:
            Tupla (prob_T1, prob_T2, p_excited)
        """
        prob_t1 = self.amplitude_damping_channel(qubit, T1)
        prob_t2 = self.phase_damping_channel(qubit, T2)

        return (prob_t1, prob_t2, p_excited)

    def aplicar_lindblad_noise(self, circuit_func, noise_type: str = 'thermal',
                               T1: float = 50.0, T2: float = 70.0):
        """
        Aplica ru√≠do baseado em Lindblad ao circuito VQC

        Args:
            circuit_func: Fun√ß√£o que define o circuito
            noise_type: Tipo de ru√≠do ('amplitude', 'phase', 'thermal', 'depolarizing')
            T1: Tempo de relaxa√ß√£o (apenas para amplitude/thermal)
            T2: Tempo de decoer√™ncia (apenas para phase/thermal)

        Returns:
            Circuito modificado com canais de ru√≠do
        """
        if noise_type == 'amplitude':
            for i in range(self.n_qubits):
                prob = self.amplitude_damping_channel(i, T1)
                qml.AmplitudeDamping(prob, wires=i)

        elif noise_type == 'phase':
            for i in range(self.n_qubits):
                prob = self.phase_damping_channel(i, T2)
                qml.PhaseDamping(prob, wires=i)

        elif noise_type == 'thermal':
            for i in range(self.n_qubits):
                prob_t1, prob_t2, p_exc = self.thermal_relaxation_channel(i, T1, T2)
                qml.AmplitudeDamping(prob_t1, wires=i)
                qml.PhaseDamping(prob_t2, wires=i)

        elif noise_type == 'depolarizing':
            for i in range(self.n_qubits):
                qml.DepolarizingChannel(self.gamma, wires=i)

        return circuit_func


# ============================================================================
# M√ìDULO 1.7: AUTOTUNING COM OPTUNA (v7.1)
# ============================================================================

class AutotunerVQC:
    """
    Autotuning de hiperpar√¢metros com Optuna (v7.1)

    Utiliza otimiza√ß√£o bayesiana (Tree-structured Parzen Estimator - TPE)
    para encontrar a melhor combina√ß√£o de hiperpar√¢metros.

    Refer√™ncias:
    - Akiba et al. (2019). "Optuna: A Next-generation Hyperparameter Optimization Framework"
    - Bergstra et al. (2011). "Algorithms for Hyper-Parameter Optimization"
    """

    def __init__(self, vqc_class, search_space: Optional[Dict] = None,
                 n_trials: int = 50, direction: str = "maximize", verbose: bool = True):
        """
        Args:
            vqc_class: Classe do VQC a otimizar
            search_space: Espa√ßo de busca (se None, usa padr√£o)
            n_trials: N√∫mero de trials
            direction: 'maximize' (acur√°cia) ou 'minimize' (erro)
            verbose: Mostrar progresso
        """
        if not OPTUNA_AVAILABLE:
            raise ImportError("Optuna n√£o instalado. Execute: pip install optuna")

        self.vqc_class = vqc_class
        self.search_space = search_space or self._get_default_search_space()
        self.n_trials = n_trials
        self.direction = direction
        self.verbose = verbose
        self.study = None
        self.best_params = None

    def _get_default_search_space(self) -> Dict[str, Any]:
        """
        Espa√ßo de busca padr√£o para hiperpar√¢metros VQC

        Returns:
            dict: Configura√ß√µes de busca
        """
        return {
            'learning_rate': ('float', 1e-4, 1.0, True),  # (tipo, low, high, log)
            'n_qubits': ('int', 2, 8, 2),  # (tipo, low, high, step)
            'n_camadas': ('int', 1, 5, 1),
            'nivel_ruido': ('float', 0.001, 0.1, False),  # Mudado de 0.0 para 0.001 e log=False
            'tipo_ruido': ('categorical', ['sem_ruido', 'depolarizante',
                                                   'amplitude_damping', 'phase_damping',
                                                   'crosstalk', 'thermal', 'correlated_noise',
                                                   'readout_error', 'bit_flip', 'phase_flip',
                                                   'pink_noise']),
            'estrategia_init': ('categorical', ['normal', 'uniforme', 'xavier',
                                               'fibonacci', 'quantum_harmonic']),
            'otimizador': ('categorical', ['adam', 'sgd', 'qng']),
            'funcao_custo': ('categorical', ['mse', 'cross_entropy', 'hinge'])
        }

    def _suggest_params(self, trial):
        """Sugere par√¢metros usando Optuna trial"""
        params = {}

        for name, config in self.search_space.items():
            if config[0] == 'float':
                if len(config) > 3 and config[3]:  # log scale
                    params[name] = trial.suggest_float(name, config[1], config[2], log=True)
                else:
                    params[name] = trial.suggest_float(name, config[1], config[2])

            elif config[0] == 'int':
                step = config[3] if len(config) > 3 else 1
                params[name] = trial.suggest_int(name, config[1], config[2], step=step)

            elif config[0] == 'categorical':
                params[name] = trial.suggest_categorical(name, config[1])

        return params

    def optimize(self, X_train, y_train, X_val, y_val, n_epocas: int = 10):
        """
        Executa otimiza√ß√£o bayesiana

        Args:
            X_train, y_train: Dados de treino
            X_val, y_val: Dados de valida√ß√£o
            n_epocas: √âpocas por trial

        Returns:
            dict: Melhores hiperpar√¢metros encontrados
        """
        def objective(trial):
            # Sugerir hiperpar√¢metros
            params = self._suggest_params(trial)

            # Criar e treinar VQC com esses par√¢metros
            try:
                vqc = self.vqc_class(
                    n_qubits=params.get('n_qubits', 4),
                    n_camadas=params.get('n_camadas', 2),
                    taxa_aprendizado=params.get('learning_rate', 0.01),
                    ruido={'tipo': params.get('tipo_ruido', 'sem_ruido'),
                           'nivel': params.get('nivel_ruido', 0.0)},
                    estrategia_init=params.get('estrategia_init', 'normal'),
                    otimizador=params.get('otimizador', 'adam'),
                    funcao_custo=params.get('funcao_custo', 'mse')
                )

                # Treinar
                vqc.treinar(X_train, y_train, n_epocas=n_epocas, verbose=False)

                # Avaliar em valida√ß√£o
                acc_val = vqc.calcular_acuracia(X_val, y_val)

                return acc_val

            except Exception as e:
                logger.warning(f"Trial falhou: {e}")
                return 0.0  # Retorna p√©ssima acur√°cia se falhar

        # Criar estudo Optuna
        if not OPTUNA_AVAILABLE:
            logger.warning("Optuna indispon√≠vel: pulando optimize()")
            return {}
        # Asserts apenas para satisfazer tipagem est√°tica
        assert OPTUNA_AVAILABLE and (TPESampler is not None) and (MedianPruner is not None) and (optuna is not None)
        sampler = TPESampler(seed=42)  # type: ignore[misc]
        pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=5)  # type: ignore[misc]

        self.study = optuna.create_study(  # type: ignore[union-attr]
            direction=self.direction,
            sampler=sampler,
            pruner=pruner
        )

        # Otimizar
        if self.verbose:
            logger.info(f"üî¨ Iniciando otimiza√ß√£o com {self.n_trials} trials...")

        self.study.optimize(objective, n_trials=self.n_trials,
                           show_progress_bar=self.verbose)

        self.best_params = self.study.best_params
        best_value = self.study.best_value

        if self.verbose:
            logger.info(f"‚úì Melhor acur√°cia: {best_value:.4f}")
            logger.info(f"‚úì Melhores par√¢metros: {self.best_params}")

        return self.best_params

    def get_importances(self):
        """Retorna import√¢ncia de cada hiperpar√¢metro"""
        if self.study is None:
            raise ValueError("Execute optimize() primeiro")

        try:
            importances = optuna.importance.get_param_importances(self.study)  # type: ignore[union-attr]
            return importances
        except Exception as e:
            logger.warning(f"N√£o foi poss√≠vel calcular import√¢ncias: {e}")
            return {}

    def plot_optimization_history(self, save_path: str = 'optuna_history.html'):
        """Salva gr√°fico de hist√≥rico de otimiza√ß√£o"""
        if self.study is None:
            raise ValueError("Execute optimize() primeiro")

        try:
            fig = optuna.visualization.plot_optimization_history(self.study)  # type: ignore[union-attr]
            fig.write_html(save_path)
            logger.info(f"‚úì Hist√≥rico salvo: {save_path}")
        except Exception as e:
            logger.warning(f"Erro ao salvar hist√≥rico: {e}")


# ============================================================================
# M√ìDULO 2: MODELOS DE RU√çDO QU√ÇNTICO

class ModeloRuido:
    """Classe base para modelos de ru√≠do qu√¢ntico."""
    def __init__(self, nivel=0.01):
        self.nivel = nivel
    def aplicar(self, n_qubits, nivel_override=None):
        raise NotImplementedError

class RuidoThermal(ModeloRuido):
    """Thermal Relaxation Error: aproxima T1/T2 com canais de amplitude e fase."""
    def aplicar(self, n_qubits, nivel_override=None):
        p = self.nivel if nivel_override is None else nivel_override
        for i in range(n_qubits):
            qml.AmplitudeDamping(p, wires=i)
            qml.PhaseDamping(p, wires=i)

class RuidoBitFlip(ModeloRuido):
    """Bit-Flip Error (X)."""
    def aplicar(self, n_qubits, nivel_override=None):
        p = self.nivel if nivel_override is None else nivel_override
        for i in range(n_qubits):
            qml.BitFlip(p, wires=i)

class RuidoPhaseFlip(ModeloRuido):
    """Phase-Flip Error (Z)."""
    def aplicar(self, n_qubits, nivel_override=None):
        p = self.nivel if nivel_override is None else nivel_override
        for i in range(n_qubits):
            qml.PhaseFlip(p, wires=i)

class RuidoPinkNoise(ModeloRuido):
    """1/f Noise (Pink): usa PhaseDamping com varia√ß√£o por qubit."""
    def aplicar(self, n_qubits, nivel_override=None):
        p = self.nivel if nivel_override is None else nivel_override
        pink = np.abs(np.random.normal(loc=0, scale=p, size=n_qubits))
        for i in range(n_qubits):
            qml.PhaseDamping(float(min(1.0, pink[i])) , wires=i)

class RuidoReadoutError(ModeloRuido):
    """Readout Error (aproxima√ß√£o via BitFlip ap√≥s opera√ß√µes)."""
    def aplicar(self, n_qubits, nivel_override=None):
        p = self.nivel if nivel_override is None else nivel_override
        for i in range(n_qubits):
            qml.BitFlip(p, wires=i)


class RuidoDepolarizante(ModeloRuido):
    """
    Ru√≠do de Depolariza√ß√£o: œÅ ‚Üí (1-p)œÅ + p¬∑I/2

    Refer√™ncia: Preskill (2018). "Quantum Computing in the NISQ era." Quantum.

    Descri√ß√£o: Erro gen√©rico mais comum em qubits supercondutores (IBM, Google).
    O estado qu√¢ntico √© substitu√≠do pelo estado maximamente misto com probabilidade p.

    Operadores de Kraus:
    K‚ÇÄ = ‚àö(1-p) I
    K‚ÇÅ = ‚àö(p/3) X
    K‚ÇÇ = ‚àö(p/3) Y
    K‚ÇÉ = ‚àö(p/3) Z
    """
    def aplicar(self, n_qubits, nivel_override=None):
        p = self.nivel if nivel_override is None else nivel_override
        for i in range(n_qubits):
            qml.DepolarizingChannel(p, wires=i)


class RuidoAmplitudeDamping(ModeloRuido):
    """
    Amplitude Damping: Relaxamento T1 (|1‚ü© ‚Üí |0‚ü©)

    Refer√™ncia: Clerk et al. (2010). "Introduction to quantum noise." Rev. Mod. Phys.

    Descri√ß√£o: Perda de energia do qubit para o ambiente. Modela decaimento
    exponencial com tempo caracter√≠stico T1. Comum em qubits supercondutores.

    Operadores de Kraus:
    K‚ÇÄ = [[1, 0], [0, ‚àö(1-Œ≥)]]
    K‚ÇÅ = [[0, ‚àöŒ≥], [0, 0]]
    """
    def aplicar(self, n_qubits, nivel_override=None):
        g = self.nivel if nivel_override is None else nivel_override
        for i in range(n_qubits):
            qml.AmplitudeDamping(g, wires=i)


class RuidoPhaseDamping(ModeloRuido):
    """
    Phase Damping: Decoer√™ncia T2 (perda de fase)

    Refer√™ncia: Schlosshauer (2007). "Decoherence and the Quantum-to-Classical Transition"

    Descri√ß√£o: Perda de informa√ß√£o de fase sem perda de energia. Modela
    decoer√™ncia com tempo caracter√≠stico T2. Importante em qubits de spin.

    Operadores de Kraus:
    K‚ÇÄ = [[1, 0], [0, ‚àö(1-Œª)]]
    K‚ÇÅ = [[0, 0], [0, ‚àöŒª]]
    """
    def aplicar(self, n_qubits, nivel_override=None):
        lmb = self.nivel if nivel_override is None else nivel_override
        for i in range(n_qubits):
            qml.PhaseDamping(lmb, wires=i)


# ===================== NOVOS MODELOS DE RU√çDO =====================
class RuidoCrosstalk(ModeloRuido):
    """
    Ru√≠do de Cross-Talk: Erros correlacionados entre qubits vizinhos

    Refer√™ncia: Kandala et al. (2019). "Error mitigation extends the computational reach of a noisy quantum processor." Nature.

    Descri√ß√£o: Aplica um canal de ru√≠do correlacionado entre pares de qubits vizinhos (ex: CNOT com ru√≠do).
    """
    def aplicar(self, n_qubits, nivel_override=None):
        p = self.nivel if nivel_override is None else nivel_override
        # Aplica canal de ru√≠do correlacionado entre pares vizinhos
        for i in range(n_qubits):
            # Canal de ru√≠do correlacionado: DepolarizingChannel em ambos os qubits simultaneamente
            qml.DepolarizingChannel(p, wires=i)
            qml.DepolarizingChannel(p, wires=(i+1)%n_qubits)
            # Cross-talk: canal extra entre pares
            qml.CNOT(wires=[i, (i+1)%n_qubits])
            qml.DepolarizingChannel(p, wires=(i+1)%n_qubits)
            qml.CNOT(wires=[i, (i+1)%n_qubits])


class RuidoCorrelacionado(ModeloRuido):
    """
    Ru√≠do Correlacionado Global: Erros coletivos afetando todos os qubits

    Refer√™ncia: Greenbaum (2015). "Introduction to Quantum Gate Set Tomography."

    Descri√ß√£o: Aplica um canal de ru√≠do coletivo (ex: PhaseDamping global) a todos os qubits simultaneamente.
    """
    def aplicar(self, n_qubits, nivel_override=None):
        lmb = self.nivel if nivel_override is None else nivel_override
        # Aplica PhaseDamping global a todos os qubits (efeito coletivo)
        for i in range(n_qubits):
            qml.PhaseDamping(lmb, wires=i)
        # Canal coletivo: aplica uma opera√ß√£o global (exemplo: ru√≠do de fase global)
        # PennyLane n√£o tem canal global nativo, mas pode-se simular aplicando em todos simultaneamente
        # Alternativamente, pode-se aplicar um canal customizado aqui se necess√°rio


# Dicion√°rio de modelos dispon√≠veis
MODELOS_RUIDO = {
    'sem_ruido': None,
    'depolarizante': RuidoDepolarizante,
    'amplitude_damping': RuidoAmplitudeDamping,
    'phase_damping': RuidoPhaseDamping,
    'crosstalk': RuidoCrosstalk,
    'thermal': RuidoThermal,
    'correlated_noise': RuidoCorrelacionado,
    'bit_flip': RuidoBitFlip,
    'phase_flip': RuidoPhaseFlip,
    'pink_noise': RuidoPinkNoise,
    'readout_error': RuidoReadoutError
}


# ============================================================================
# M√ìDULO 3: ARQUITETURAS DE CIRCUITOS QU√ÇNTICOS
def circuito_hardware_efficient(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):
    """
    Hardware-Efficient Ansatz: RY + CNOT em camadas alternadas
    """
    for i in range(min(len(x), n_qubits)):
        qml.RY(np.pi * x[i], wires=i)
    for camada in range(n_camadas):
        for i in range(n_qubits):
            qml.RY(weights[camada * n_qubits + i], wires=i)
        for i in range(0, n_qubits-1, 2):
            qml.CNOT(wires=[i, i+1])
        for i in range(1, n_qubits-1, 2):
            qml.CNOT(wires=[i, i+1])
        if modelo_ruido:
            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)
    return qml.expval(qml.PauliZ(0))

def circuito_tree(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):
    """
    Tree Entanglement: Emaranhamento em √°rvore bin√°ria
    """
    for i in range(min(len(x), n_qubits)):
        qml.RY(np.pi * x[i], wires=i)
    for camada in range(n_camadas):
        for i in range(n_qubits):
            qml.RY(weights[camada * n_qubits + i], wires=i)
        step = 1
        while step < n_qubits:
            for i in range(0, n_qubits-step, step*2):
                qml.CNOT(wires=[i, i+step])
            step *= 2
        if modelo_ruido:
            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)
    return qml.expval(qml.PauliZ(0))

def circuito_qaoa(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):
    """
    QAOA-like Ansatz: Alterna RX, RZZ, RX
    """
    for i in range(min(len(x), n_qubits)):
        qml.RX(np.pi * x[i], wires=i)
    for camada in range(n_camadas):
        for i in range(n_qubits):
            qml.RX(weights[camada * n_qubits + i], wires=i)
        for i in range(n_qubits-1):
            qml.CNOT(wires=[i, i+1])
            qml.RZ(weights[n_camadas * n_qubits + camada * (n_qubits-1) + i], wires=i+1)
            qml.CNOT(wires=[i, i+1])
        if modelo_ruido:
            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)
    return qml.expval(qml.PauliZ(0))

def circuito_alternating_layers(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):
    """
    Alternating Layers: RX, RY, CNOT em padr√£o alternado
    """
    for i in range(min(len(x), n_qubits)):
        qml.RX(np.pi * x[i], wires=i)
    for camada in range(n_camadas):
        for i in range(n_qubits):
            qml.RX(weights[camada * n_qubits + i], wires=i)
            qml.RY(weights[n_camadas * n_qubits + camada * n_qubits + i], wires=i)
        for i in range(n_qubits-1):
            qml.CNOT(wires=[i, i+1])
        if modelo_ruido:
            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)
    return qml.expval(qml.PauliZ(0))

def circuito_star_entanglement(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):
    """
    Star Entanglement: Qubit 0 central, CNOT(0, i)
    """
    for i in range(min(len(x), n_qubits)):
        qml.RY(np.pi * x[i], wires=i)
    for camada in range(n_camadas):
        for i in range(n_qubits):
            qml.RY(weights[camada * n_qubits + i], wires=i)
        for i in range(1, n_qubits):
            qml.CNOT(wires=[0, i])
        if modelo_ruido:
            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)
    return qml.expval(qml.PauliZ(0))

def circuito_brickwork(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):
    """
    Brickwork: padr√£o de CNOTs em "tijolos"
    """
    for i in range(min(len(x), n_qubits)):
        qml.RY(np.pi * x[i], wires=i)
    for camada in range(n_camadas):
        for i in range(n_qubits):
            qml.RY(weights[camada * n_qubits + i], wires=i)
        for i in range(0, n_qubits-1, 2):
            qml.CNOT(wires=[i, i+1])
        for i in range(1, n_qubits-1, 2):
            qml.CNOT(wires=[i, i+1])
        if modelo_ruido:
            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)
    return qml.expval(qml.PauliZ(0))

def circuito_random_entangling(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):
    """
    Random Entangling: CNOTs entre pares aleat√≥rios por camada
    """
    import random
    for i in range(min(len(x), n_qubits)):
        qml.RY(np.pi * x[i], wires=i)
    for camada in range(n_camadas):
        for i in range(n_qubits):
            qml.RY(weights[camada * n_qubits + i], wires=i)
        pares = [(i, j) for i in range(n_qubits) for j in range(i+1, n_qubits)]
        random.shuffle(pares)
        for i, j in pares[:n_qubits//2]:
            qml.CNOT(wires=[i, j])
        if modelo_ruido:
            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)
    return qml.expval(qml.PauliZ(0))
# ============================================================================

def circuito_basico(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):
    """
    Circuito B√°sico: RY encoding + RY rotations + CNOT ring

    Refer√™ncia: Farhi & Neven (2018). "Classification with quantum neural networks
    on near term processors." arXiv:1802.06002

    Estrutura:
    1. Encoding: RY(œÄ¬∑x[i]) em cada qubit
    2. Camadas variacionais (repetidas L vezes):
       - RY(Œ∏[i]) em cada qubit
       - CNOT em anel: CNOT(i, i+1 mod n)
    3. Medi√ß√£o: ‚ü®Z‚ÇÄ‚ü©

    Complexidade: O(n_qubits √ó n_camadas)
    Par√¢metros: n_qubits √ó n_camadas

    Vantagens:
    - Simples e r√°pido
    - Bom para prototipagem
    - Baixo n√∫mero de par√¢metros
    """
    # 1. Encoding de dados
    for i in range(min(len(x), n_qubits)):
        qml.RY(np.pi * x[i], wires=i)

    # 2. Camadas variacionais
    for camada in range(n_camadas):
        # Rota√ß√µes parametrizadas
        for i in range(n_qubits):
            qml.RY(weights[camada * n_qubits + i], wires=i)

        # Emaranhamento (CNOT em anel)
        for i in range(n_qubits):
            qml.CNOT(wires=[i, (i + 1) % n_qubits])

        # Aplicar ru√≠do ap√≥s cada camada (se especificado)
        if modelo_ruido:
            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)

    # 3. Medi√ß√£o
    return qml.expval(qml.PauliZ(0))


def circuito_strongly_entangling(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):
    """
    Strongly Entangling Layers (PennyLane template)

    Refer√™ncia: Schuld et al. (2020). "Circuit-centric quantum classifiers."
    Physical Review A, 101(3), 032308.

    Estrutura:
    1. Encoding: AngleEmbedding (RY em cada qubit)
    2. StronglyEntanglingLayers (template PennyLane):
       - Rot(Œ∏, œÜ, œâ) em cada qubit (3 rota√ß√µes arbitr√°rias)
       - CNOT(i, j) para todos i < j (emaranhamento completo)
    3. Medi√ß√£o: ‚ü®Z‚ÇÄ‚ü©

    Complexidade: O(n_qubits¬≤ √ó n_camadas)
    Par√¢metros: n_qubits √ó n_camadas √ó 3

    Vantagens:
    - Alta capacidade expressiva
    - Emaranhamento forte
    - Template otimizado do PennyLane
    """
    # 1. Encoding de dados
    qml.AngleEmbedding(x, wires=range(n_qubits), rotation='Y')

    # 2. Camadas fortemente emaranhadas
    weights_reshaped = weights.reshape(n_camadas, n_qubits, 3)
    qml.StronglyEntanglingLayers(weights_reshaped, wires=range(n_qubits))

    # 3. Aplicar ru√≠do (se especificado)
    if modelo_ruido:
        modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)

    # 4. Medi√ß√£o
    return qml.expval(qml.PauliZ(0))


# Dicion√°rio de arquiteturas dispon√≠veis
ARQUITETURAS = {
    'basic_entangler': (circuito_basico, lambda nq, nc: nc * nq),
    'strongly_entangling': (circuito_strongly_entangling, lambda nq, nc: nc * nq * 3),
    'hardware_efficient': (circuito_hardware_efficient, lambda nq, nc: nc * nq),
    'tree': (circuito_tree, lambda nq, nc: nc * nq),
    'qaoa': (circuito_qaoa, lambda nq, nc: nc * nq + nc * (nq-1)),
    'alternating_layers': (circuito_alternating_layers, lambda nq, nc: 2 * nc * nq),
    'star_entanglement': (circuito_star_entanglement, lambda nq, nc: nc * nq),
    'brickwork': (circuito_brickwork, lambda nq, nc: nc * nq),
    'random_entangling': (circuito_random_entangling, lambda nq, nc: nc * nq)
}


# ============================================================================
# M√ìDULO 4: CLASSIFICADOR QU√ÇNTICO VARIACIONAL (VQC)
# ============================================================================

class ClassificadorVQC(BaseEstimator, ClassifierMixin):
    """
    Classificador Qu√¢ntico Variacional.

    Refer√™ncias:
    - Schuld et al. (2020). "Circuit-centric quantum classifiers." Phys. Rev. A.
    - Mitarai et al. (2018). "Quantum circuit learning." Phys. Rev. A.
    - Bergholm et al. (2018). "PennyLane: Automatic differentiation." arXiv:1811.04968

    Implementa interface scikit-learn (BaseEstimator, ClassifierMixin) para
    compatibilidade com pipelines de ML cl√°ssico.
    """

    def __init__(self, n_qubits=4, n_camadas=2, arquitetura='basico',
                 estrategia_init='aleatorio', tipo_ruido='sem_ruido', nivel_ruido=0.01,
                 taxa_aprendizado=0.01, n_epocas=20, batch_size=32, seed=42,
                 ruido_schedule=None, ruido_inicial=None, ruido_final=None,
                 early_stopping=False, patience=10, min_delta=1e-3, val_split=0.1,
                 ruido_adaptativo=False, track_entanglement=False,
                 otimizador='adam', funcao_custo='mse', detectar_barren=False,
                 max_grad_norm=1.0):
        """
        Args:
            n_qubits: N√∫mero de qubits (2-20)
            n_camadas: Profundidade do circuito (1-10)
            arquitetura: 'basico' ou 'strongly_entangling'
            estrategia_init: 'aleatorio', 'matematico', ou 'quantico'
            tipo_ruido: 'sem_ruido', 'depolarizante', 'amplitude_damping', 'phase_damping'
            nivel_ruido: Taxa de erro (0.0-0.05)
            taxa_aprendizado: Learning rate para Adam (1e-4 a 1e-1)
            n_epocas: N√∫mero de √©pocas de treinamento (10-200)
            batch_size: Tamanho do mini-batch (8-128)
            seed: Semente aleat√≥ria para reprodutibilidade
        """
        self.n_qubits = n_qubits
        self.n_camadas = n_camadas
        self.arquitetura = arquitetura
        self.estrategia_init = estrategia_init
        # Permitir uso autom√°tico de 'correlated_noise' se tipo_ruido for 'correlated' ou 'correlated_noise'
        if tipo_ruido in ['correlated', 'correlated_noise']:
            self.tipo_ruido = 'correlated_noise'
        else:
            self.tipo_ruido = tipo_ruido
        self.nivel_ruido = nivel_ruido
        self.taxa_aprendizado = taxa_aprendizado
        self.n_epocas = n_epocas
        self.batch_size = batch_size
        self.seed = seed
        # Annealing de ru√≠do
        self.ruido_schedule = ruido_schedule  # 'linear' | 'exponencial' | 'cosine' | None
        self.ruido_inicial = ruido_inicial
        self.ruido_final = ruido_final
        # Early stopping
        self.early_stopping = early_stopping
        self.patience = patience
        self.min_delta = min_delta
        self.val_split = val_split
        self.ruido_adaptativo = ruido_adaptativo
        self.track_entanglement = track_entanglement

        # Funcionalidades avan√ßadas
        self.otimizador = otimizador  # 'adam', 'sgd', 'qng'
        self.funcao_custo = funcao_custo  # 'mse', 'cross_entropy', 'hinge'
        self.detectar_barren = detectar_barren
        self.max_grad_norm = max_grad_norm

        # Hist√≥rico de treinamento expandido
        self.historico_ = {
            'custo': [],
            'acuracia_treino': [],
            'epoca': [],
            'nivel_ruido': [],
            'entropia_emaranhamento': [],
            'variancia_gradiente': []
        }

        # Detectores e monitores
        self.detector_plateau_ = DetectorBarrenPlateau() if detectar_barren else None
        self.monitor_emaranhamento_ = MonitorEmaranhamento(n_qubits) if track_entanglement else None

        # Configurar seeds para reprodutibilidade
        np.random.seed(seed)
        try:
            # Nem sempre dispon√≠vel no shim; ignore se ausente
            pnp.random.seed(seed)  # type: ignore[attr-defined]
        except Exception:
            pass

    def _criar_circuito(self):
        """
        Cria o circuito qu√¢ntico e inicializa par√¢metros.

        Usa PennyLane's default.mixed device para suportar ru√≠do.
        """
        # Dispositivo qu√¢ntico (simulador de matriz de densidade)
        self.dev_ = qml.device('default.mixed', wires=self.n_qubits)

        # Selecionar arquitetura e calcular n√∫mero de par√¢metros
        circuito_fn, calc_params = ARQUITETURAS[self.arquitetura]
        n_params = calc_params(self.n_qubits, self.n_camadas)

        # Criar modelo de ru√≠do (se especificado)
        modelo_ruido = None
        if self.tipo_ruido != 'sem_ruido':
            modelo_ruido = MODELOS_RUIDO[self.tipo_ruido](self.nivel_ruido)

        # Criar QNode (circuito qu√¢ntico diferenci√°vel)
        @qml.qnode(self.dev_, interface='autograd')
        def circuit(weights, x, nivel_ruido_runtime=None):
            return circuito_fn(weights, x, self.n_qubits, self.n_camadas, modelo_ruido, nivel_ruido_runtime)

        self.qnode_ = circuit

        # Inicializar pesos com estrat√©gia escolhida
        self.weights_ = ConstantesFundamentais.inicializar(
            n_params, self.estrategia_init, self.seed
        )

        # Inicializar bias
        self.bias_ = pnp.array(0.0, requires_grad=True)

    def _nivel_ruido_epoca(self, epoca):
        if self.tipo_ruido == 'sem_ruido':
            return 0.0
        # Sem schedule: usar nivel fixo
        if not self.ruido_schedule:
            return self.nivel_ruido
        nE, ri, rf = max(1, self.n_epocas), (self.ruido_inicial if self.ruido_inicial is not None else self.nivel_ruido), (self.ruido_final if self.ruido_final is not None else 0.001)
        t = epoca / max(1, nE - 1)
        if self.ruido_schedule == 'linear':
            return rf + (ri - rf) * (1 - t)
        if self.ruido_schedule == 'exponencial':
            tau = max(1, nE / 3)
            return rf + (ri - rf) * np.exp(-epoca / tau)
        if self.ruido_schedule == 'cosine':
            return rf + (ri - rf) * 0.5 * (1 + np.cos(np.pi * t))
        return self.nivel_ruido

    def _funcao_custo(self, weights, bias, X, y, nivel_ruido_runtime=None):
        """Fun√ß√£o de custo configur√°vel (MSE, Cross-Entropy ou Hinge)."""
        predicoes = pnp.array([self.qnode_(weights, x, nivel_ruido_runtime) + bias for x in X])

        # Selecionar fun√ß√£o de custo
        if self.funcao_custo == 'mse':
            # Usar pnp.mean para manter grafo de autograd
            diff = pnp.array(y) - predicoes
            return pnp.mean(diff ** 2)  # type: ignore[attr-defined]
        elif self.funcao_custo == 'cross_entropy':
            return FuncaoCustoAvancada.cross_entropy(predicoes, y)
        elif self.funcao_custo == 'hinge':
            return FuncaoCustoAvancada.hinge(predicoes, y)
        else:
            diff = pnp.array(y) - predicoes
            return pnp.mean(diff ** 2)  # type: ignore[attr-defined]

    def fit(self, X, y):
        """
        Treina o classificador.

        Args:
            X: Dados de treinamento (n_samples, n_features)
            y: Labels (n_samples,)

        Returns:
            self (para compatibilidade scikit-learn)
        """
        # Codificar labels como ¬±1
        self.label_encoder_ = LabelEncoder()
        y_le = np.asarray(self.label_encoder_.fit_transform(y), dtype=int)
        y_encoded = (y_le * 2) - 1  # type: ignore[operator]
        self.classes_ = self.label_encoder_.classes_

        # Criar circuito e inicializar par√¢metros
        self._criar_circuito()

        # Split de valida√ß√£o (para early stopping)
        X_arr, y_arr = np.asarray(X), np.asarray(y_encoded)
        if self.early_stopping and self.val_split > 0:
            n = len(X_arr)
            n_val = max(1, int(n * self.val_split))
            idx = np.random.permutation(n)
            val_idx, train_idx = idx[:n_val], idx[n_val:]
            X_train_es, y_train_es = X_arr[train_idx], y_arr[train_idx]
            X_val_es, y_val_es = X_arr[val_idx], y_arr[val_idx]
        else:
            X_train_es, y_train_es = X_arr, y_arr
            X_val_es = y_val_es = None

        # Criar otimizador configur√°vel
        opt = OtimizadorAvancado.criar(self.otimizador, self.taxa_aprendizado)

        # Treinamento por √©pocas
        melhor_val = -np.inf
        sem_melhora = 0
        melhor_w, melhor_b = None, None

        for epoca in range(self.n_epocas):
            # Embaralhar dados
            indices = np.random.permutation(len(X_train_es))

            # Mini-batch gradient descent
            nivel_runtime = self._nivel_ruido_epoca(epoca)
            for i in range(0, len(X_train_es), self.batch_size):
                batch_idx = indices[i:i + self.batch_size]
                X_batch = X_train_es[batch_idx]
                y_batch = y_train_es[batch_idx]

                # Atualizar par√¢metros (fun√ß√£o de custo compat√≠vel com autograd)
                def custo_batch(w, b):
                    preds = pnp.array([self.qnode_(w, pnp.array(x), nivel_runtime) + b for x in X_batch])
                    # Usar pnp.mean para permitir gradientes
                    return pnp.mean((pnp.array(y_batch) - preds) ** 2)  # type: ignore[attr-defined]

                _step_res = opt.step(custo_batch, self.weights_, self.bias_)
                # Robustez para diferentes retornos
                try:
                    self.weights_, self.bias_ = _step_res  # type: ignore[assignment]
                except Exception:
                    if isinstance(_step_res, (list, tuple)) and len(_step_res) >= 2:
                        self.weights_, self.bias_ = _step_res[0], _step_res[1]
                    else:
                        pass

            # Registrar hist√≥rico
            custo_val = self._funcao_custo(
                self.weights_, self.bias_, pnp.array(X_train_es), pnp.array(y_train_es), nivel_runtime
            )
            try:
                custo = float(custo_val)
            except Exception:
                custo = float(pnp.asarray(custo_val))
            acuracia = self.score(X, y)

            self.historico_['custo'].append(custo)
            self.historico_['acuracia_treino'].append(acuracia)
            self.historico_['epoca'].append(epoca)
            self.historico_['nivel_ruido'].append(nivel_runtime)

            # Monitorar gradientes (detec√ß√£o de barren plateau)
            if self.detector_plateau_:
                try:
                    gradientes = qml.grad(self._funcao_custo)(
                        self.weights_, self.bias_,
                        pnp.array(X_train_es[:5]), pnp.array(y_train_es[:5]),
                        nivel_runtime
                    )
                    if isinstance(gradientes, (list, tuple)) and len(gradientes) > 0:
                        grad_array = np.array(gradientes[0]).flatten()
                    else:
                        grad_array = np.array([])
                    variancia_grad = float(np.var(grad_array))
                    self.historico_['variancia_gradiente'].append(variancia_grad)

                    if self.detector_plateau_.detectar(grad_array):
                        logger.warning(f"√âpoca {epoca}: Barren Plateau detectado (var={variancia_grad:.2e})")
                except Exception:
                    self.historico_['variancia_gradiente'].append(0.0)
            else:
                self.historico_['variancia_gradiente'].append(0.0)

            # Monitorar emaranhamento
            if self.monitor_emaranhamento_:
                try:
                    # Criar QNode que retorna estado
                    @qml.qnode(self.dev_, interface='autograd')
                    def estado_qnode(weights, x):
                        circuito_fn, _ = ARQUITETURAS[self.arquitetura]
                        circuito_fn(weights, x, self.n_qubits, self.n_camadas, None, None)
                        return qml.density_matrix(wires=[0])

                    # Calcular estado do primeiro qubit
                    x_sample = pnp.array(X_train_es[0])
                    rho_0 = estado_qnode(self.weights_, x_sample)

                    # Calcular entropia
                    entropia = self.monitor_emaranhamento_.calcular_entropia_von_neumann(rho_0)
                    self.historico_['entropia_emaranhamento'].append(float(entropia))
                except Exception:
                    self.historico_['entropia_emaranhamento'].append(0.0)
            else:
                self.historico_['entropia_emaranhamento'].append(0.0)

            # Early stopping
            if self.early_stopping and X_val_es is not None and y_val_es is not None:
                ac_val = np.mean(self.predict(X_val_es) == self.label_encoder_.inverse_transform(((y_val_es + 1)//2).astype(int)))
                if ac_val > melhor_val + self.min_delta:
                    melhor_val = ac_val
                    sem_melhora = 0
                    melhor_w, melhor_b = self.weights_.copy(), float(self.bias_)
                else:
                    sem_melhora += 1
                if sem_melhora >= self.patience:
                    # Restaurar melhor
                    if (melhor_w is not None) and (melhor_b is not None):
                        self.weights_ = melhor_w
                        self.bias_ = pnp.array(melhor_b, requires_grad=True)
                    break

        return self

    def predict(self, X):
        """
        Prediz classes para novos dados.

        Args:
            X: Dados de teste (n_samples, n_features)

        Returns:
            Predi√ß√µes de classe (n_samples,)
        """
        # Obter predi√ß√µes do circuito qu√¢ntico
        predicoes = np.array([
            float(self.qnode_(self.weights_, pnp.array(x)) + self.bias_)
            for x in X
        ])

        # Converter de ¬±1 para classes originais
        predicoes_classe = ((np.sign(predicoes) + 1) // 2).astype(int)
        return self.label_encoder_.inverse_transform(predicoes_classe)

    def score(self, X, y, sample_weight=None):
        """
        Calcula acur√°cia.

        Args:
            X: Dados
            y: Labels verdadeiros

        Returns:
            Acur√°cia (0.0 a 1.0)
        """
        return np.mean(self.predict(X) == y)


# ============================================================================
# M√ìDULO 5: GERENCIAMENTO DE DATASETS
# ============================================================================

def carregar_datasets(seed=42):
    """
    Carrega 4 datasets de benchmark para classifica√ß√£o bin√°ria.

    Refer√™ncias:
    - Moons, Circles: Scikit-learn (Pedregosa et al., 2011)
    - Iris: Fisher (1936), UCI ML Repository
    - Breast Cancer: Wolberg et al. (1995), UCI ML Repository

    Returns:
        Dict com 4 datasets, cada um contendo X_train, X_test, y_train, y_test
    """
    datasets = {}
    scaler = StandardScaler()

    # Dataset 1: Moons (n√£o-linear, duas luas entrela√ßadas)
    X, y = sk_datasets.make_moons(n_samples=400, noise=0.1, random_state=seed)
    X = scaler.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=seed, stratify=y
    )
    datasets['moons'] = {
        'X_train': X_train, 'X_test': X_test,
        'y_train': y_train, 'y_test': y_test,
        'descricao': 'Duas luas entrela√ßadas (n√£o-linear)'
    }

    # Dataset 2: Circles (n√£o-linear, c√≠rculos conc√™ntricos)
    X, y = sk_datasets.make_circles(
        n_samples=400, noise=0.1, factor=0.5, random_state=seed
    )
    X = scaler.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=seed, stratify=y
    )
    datasets['circles'] = {
        'X_train': X_train, 'X_test': X_test,
        'y_train': y_train, 'y_test': y_test,
        'descricao': 'C√≠rculos conc√™ntricos (n√£o-linear)'
    }

    # Dataset 3: Iris (linear, 2 primeiras classes)
    from sklearn.utils import Bunch as SklearnBunch
    iris_data: SklearnBunch = sk_datasets.load_iris()  # type: ignore[assignment]
    X, y = iris_data.data, iris_data.target
    mask = y < 2  # Apenas setosa e versicolor
    X, y = X[mask], y[mask]
    X = scaler.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=seed, stratify=y
    )
    datasets['iris'] = {
        'X_train': X_train, 'X_test': X_test,
        'y_train': y_train, 'y_test': y_test,
        'descricao': 'Flores Iris (linear, 4 features)'
    }

    # Dataset 4: Breast Cancer (alta dimens√£o, 30 features)
    cancer_data: SklearnBunch = sk_datasets.load_breast_cancer()  # type: ignore[assignment]
    X, y = cancer_data.data, cancer_data.target
    X = scaler.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=seed, stratify=y
    )
    datasets['breast_cancer'] = {
        'X_train': X_train, 'X_test': X_test,
        'y_train': y_train, 'y_test': y_test,
        'descricao': 'Diagn√≥stico de c√¢ncer (30 features)'
    }

    # Dataset 5: Wine (features qu√≠micas, 2 primeiras classes)
    wine_data: SklearnBunch = sk_datasets.load_wine()  # type: ignore[assignment]
    X, y = wine_data.data, wine_data.target
    mask = y < 2  # Apenas classes 0 e 1
    X, y = X[mask], y[mask]
    X = scaler.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=seed, stratify=y
    )
    datasets['wine'] = {
        'X_train': X_train, 'X_test': X_test,
        'y_train': y_train, 'y_test': y_test,
        'descricao': 'Vinhos (13 features qu√≠micas)'
    }

    return datasets


# ============================================================================
# M√ìDULO 6: EXECU√á√ÉO DE EXPERIMENTOS
# ============================================================================

def executar_grid_search(datasets, n_epocas=15, verbose=True, pasta_resultados=None):
    """
    Executa grid search completo sobre todas as configura√ß√µes.

    Grid:
    - 5 datasets (moons, circles, iris, breast_cancer, wine)
    - 8 arquiteturas
    - 5 estrat√©gias de inicializa√ß√£o
    - 5 tipos de ru√≠do (depolarizing, amplitude_damping, phase_damping, crosstalk, thermal/correlated)
    - 3 n√≠veis de ru√≠do
    - 4 schedules de ru√≠do (linear, exponencial, cosseno, adaptativo)
    Total: 5 √ó 8 √ó 5 √ó 5 √ó 3 √ó 4 = 12.000 configura√ß√µes
    # Observa√ß√£o: No c√≥digo, 'thermal' √© tratado como ru√≠do correlacionado para consist√™ncia com os documentos.

    Args:
        datasets: Dict de datasets
        n_epocas: N√∫mero de √©pocas de treinamento
        verbose: Se True, imprime progresso

    Returns:
        DataFrame com todos os resultados
    """
    # Pasta para granularidade m√°xima
    pasta_individual = None
    if pasta_resultados is None:
        pasta_resultados = os.path.join(os.getcwd(), f"resultados_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}")
    os.makedirs(pasta_resultados, exist_ok=True)
    pasta_individual = os.path.join(pasta_resultados, 'experimentos_individuais')
    os.makedirs(pasta_individual, exist_ok=True)
    # Placeholders para Pylance
    from typing import Any
    metadata: Dict[str, Any] = {}
    metadata_path: Optional[str] = None

    # Definir grid de hiperpar√¢metros
    quick = os.environ.get('VQC_QUICK', '0') == '1'
    if quick:
        grid = {
            'arquitetura': list(ARQUITETURAS.keys()),
            'estrategia_init': ['matematico', 'fibonacci_spiral'],
            'tipo_ruido': ['sem_ruido', 'depolarizante'],
            'nivel_ruido': [0.0, 0.0025, 0.005, 0.0075, 0.01]
        }
    else:
        grid = {
            'arquitetura': list(ARQUITETURAS.keys()),
            'estrategia_init': ['matematico', 'quantico', 'aleatorio', 'fibonacci_spiral'],
            'tipo_ruido': ['sem_ruido', 'depolarizante', 'amplitude_damping', 'phase_damping', 'crosstalk', 'correlacionado'],
            'nivel_ruido': [0.0, 0.0025, 0.005, 0.0075, 0.01, 0.0125, 0.015, 0.0175, 0.02]
        }

    resultados = []
    n_seeds = 5
    seed_list = [42 + i for i in range(n_seeds)]
    contador = 0  # Inicializar contador

    # Calcular total de configura√ß√µes
    total_configs = len(grid['arquitetura']) * len(grid['estrategia_init']) * len(grid['tipo_ruido']) * len(grid['nivel_ruido'])
    # Ajustar para configs v√°lidas (sem_ruido s√≥ com nivel 0)
    configs_invalidas = len(grid['arquitetura']) * len(grid['estrategia_init']) * (len(grid['nivel_ruido']) - 1)  # sem_ruido com nivel > 0
    total_configs -= configs_invalidas

    # Gera√ß√£o de README e metadata.json ap√≥s grid/seed_list definidos
    if pasta_resultados is not None:
        readme_path = os.path.join(pasta_resultados, 'README_grid_search.md')
        metadata_path = os.path.join(pasta_resultados, 'metadata_grid_search.json')
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(
                "# Resultados do Grid Search VQC\n\n"
                f"- Data de execu√ß√£o: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                f"- Par√¢metros do grid: {grid}\n"
                f"- Seeds: {seed_list}\n\n"
                "Todos os experimentos, circuitos e gr√°ficos est√£o organizados nesta pasta.\n"
                "O arquivo `resultados_completos_artigo.csv` cont√©m todos os resultados consolidados.\n"
            )
        metadata = {
            'tipo': 'grid_search',
            'data_execucao': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'parametros_grid': grid,
            'seeds': seed_list,
            'arquivos_gerados': [],
            'csv_consolidado': None,
            'readme': readme_path
        }

    for nome_dataset, dataset in datasets.items():
        _X_train, _y_train = dataset['X_train'], dataset['y_train']
        _X_test, _y_test = dataset['X_test'], dataset['y_test']

        for arq in grid['arquitetura']:
            for init in grid['estrategia_init']:
                for ruido in grid['tipo_ruido']:
                    for nivel in grid['nivel_ruido']:
                        if ruido == 'sem_ruido' and nivel > 0:
                            continue
                        for seed in seed_list:
                            contador += 1
                            # Log detalhado de todos os par√¢metros
                            if verbose:
                                logger.info(
                                    f"[{contador:3d}/{total_configs * n_seeds}] "
                                    f"Dataset: {nome_dataset} | Seed: {seed} | Qubits: 4 | Camadas: 2 | "
                                    f"Arquitetura: {arq} | Init: {init} | Ru√≠do: {ruido} | N√≠vel: {nivel:.4f}"
                                )
                                logger.info(
                                    f"Constantes: œÄ={ConstantesFundamentais.PI:.5f}, e={ConstantesFundamentais.E:.5f}, œÜ={ConstantesFundamentais.PHI:.5f}, ‚Ñè={ConstantesFundamentais.HBAR:.2e}, Œ±={ConstantesFundamentais.ALPHA:.5f}, R‚àû={ConstantesFundamentais.RYDBERG:.2f}"
                                )
                            try:
                                tempo_inicio = time.time()
                                # Criar e treinar VQC
                                vqc = ClassificadorVQC(
                                    n_qubits=4,
                                    n_camadas=2,
                                    arquitetura=arq,
                                    estrategia_init=init,
                                    tipo_ruido=ruido,
                                    nivel_ruido=nivel,
                                    taxa_aprendizado=0.01,
                                    n_epocas=n_epocas,
                                    batch_size=32,
                                    seed=seed,
                                    # Insights: ru√≠do com annealing quando n√≠vel > 0
                                    ruido_schedule=('cosine' if (ruido != 'sem_ruido' and nivel > 0) else None),
                                    ruido_inicial=(nivel if (ruido != 'sem_ruido' and nivel > 0) else None),
                                    ruido_final=(0.001 if (ruido != 'sem_ruido' and nivel > 0) else None),
                                    # Early stopping leve para acelerar
                                    early_stopping=True, patience=5, min_delta=1e-3, val_split=0.1
                                )
                                vqc.fit(dataset['X_train'], dataset['y_train'])
                                tempo_total = time.time() - tempo_inicio
                                # Calcular m√©tricas
                                acuracia_treino = vqc.score(dataset['X_train'], dataset['y_train'])
                                acuracia_teste = vqc.score(dataset['X_test'], dataset['y_test'])
                                gap_treino_teste = acuracia_treino - acuracia_teste
                                # Matriz de confus√£o
                                y_pred = vqc.predict(dataset['X_test'])
                                cm = confusion_matrix(dataset['y_test'], y_pred)
                                # Armazenar resultados
                                resultado = {
                                    'dataset': nome_dataset,
                                    'arquitetura': arq,
                                    'estrategia_init': init,
                                    'tipo_ruido': ruido,
                                    'nivel_ruido': nivel,
                                    'n_qubits': 4,
                                    'n_camadas': 2,
                                    'acuracia_treino': acuracia_treino,
                                    'acuracia_teste': acuracia_teste,
                                    'gap_treino_teste': gap_treino_teste,
                                    'tempo_segundos': tempo_total,
                                    'custo_final': vqc.historico_['custo'][-1],
                                    'cm_tn': cm[0,0],
                                    'cm_fp': cm[0,1],
                                    'cm_fn': cm[1,0],
                                    'cm_tp': cm[1,1],
                                    'seed': seed
                                }
                                resultados.append(resultado)
                                # Salvar cada experimento individualmente em CSV
                                if pasta_individual is not None:
                                    id_exp = f"exp_{contador:05d}"
                                    df_exp = pd.DataFrame([resultado])
                                    csv_exp_path = os.path.join(pasta_individual, f"{id_exp}.csv")
                                    df_exp.to_csv(csv_exp_path, index=False)
                                # Bloco duplicado removido (j√° salvo em resultado)
                                if verbose:
                                    logger.info(
                                        f"  ‚úì Acur√°cia: {acuracia_teste:.4f} | "
                                        f"Gap: {gap_treino_teste:+.4f} | "
                                        f"Tempo: {tempo_total:.1f}s"
                                    )

                                # Salvar circuito desenhado em PNG
                                if pasta_resultados is not None:
                                    try:
                                        import pennylane as qml
                                        import matplotlib
                                        matplotlib.use('Agg')
                                        import matplotlib.pyplot as plt

                                        # Criar pasta para circuitos se n√£o existir
                                        pasta_circuitos = os.path.join(pasta_resultados, "circuitos")
                                        os.makedirs(pasta_circuitos, exist_ok=True)

                                        # Desenhar circuito usando qml.draw_mpl
                                        fig_circ, ax_circ = qml.draw_mpl(vqc.qnode_, decimals=2)(
                                            vqc.weights_,
                                            dataset['X_train'][0],
                                            None  # nivel_ruido_runtime
                                        )

                                        circ_png_filename = f"circuito_{nome_dataset}_seed{seed}_{arq}_{init}_{ruido}_nivel{nivel:.4f}.png"
                                        circ_png_path = os.path.join(pasta_circuitos, circ_png_filename)
                                        plt.savefig(circ_png_path, dpi=150, bbox_inches='tight', facecolor='white')
                                        try:
                                            import matplotlib.pyplot as _plt
                                            # Alguns stubs reportam tipo impreciso para draw_mpl; force close seguro
                                            try:
                                                from matplotlib.figure import Figure as _MplFigure
                                            except Exception:
                                                _MplFigure = object  # type: ignore[assignment]
                                            if isinstance(fig_circ, _MplFigure):
                                                _plt.close(fig_circ)  # type: ignore[arg-type]
                                            else:
                                                _plt.close('all')
                                        except Exception:
                                            try:
                                                plt.close('all')
                                            except Exception:
                                                pass

                                        if verbose:
                                            logger.info(f"    ‚Üí Circuito salvo: {circ_png_filename}")
                                    except Exception as e:
                                        logger.warning(f"Falha ao salvar PNG do circuito: {e}")

                                    # Salvar gr√°fico 3D de gradientes (Barren Plateaus)
                                    if 'variancia_gradiente' in vqc.historico_ and len(vqc.historico_['variancia_gradiente']) > 0:
                                        try:
                                            import matplotlib
                                            matplotlib.use('Agg')
                                            import matplotlib.pyplot as plt
                                            import numpy as np

                                            # Criar pasta para barren plateaus se n√£o existir
                                            pasta_barren = os.path.join(pasta_resultados, "barren_plateaus")
                                            os.makedirs(pasta_barren, exist_ok=True)

                                            epocas = vqc.historico_.get('epoca', [])
                                            variancias = vqc.historico_.get('variancia_gradiente', [])
                                            custos = vqc.historico_.get('custo', [])

                                            # Se n√£o houver dados de √©poca, gerar sequ√™ncia simples
                                            if not epocas or len(epocas) != len(variancias):
                                                epocas = list(range(1, len(variancias)+1))

                                            if len(epocas) == len(variancias) == len(custos) and len(epocas) > 0:
                                                fig = plt.figure(figsize=(10, 8))
                                                ax = fig.add_subplot(111, projection='3d')

                                                # Garantir dtype float para compatibilidade com matplotlib
                                                X = np.array(epocas, dtype=float)
                                                Y = np.array(variancias, dtype=float)
                                                Z = np.array(custos, dtype=float)

                                                scatter = ax.scatter(xs=X, ys=Y, zs=Z, c=Z, cmap='viridis', s=50, alpha=0.8)  # type: ignore[call-arg]

                                                ax.set_title(
                                                    f'Barren Plateau Analysis\n{arq} | {init} | {ruido} (Œ≥={nivel:.4f})',
                                                    fontsize=14, fontfamily='serif'
                                                )
                                                ax.set_xlabel('√âpoca', fontsize=12, fontfamily='serif')
                                                ax.set_ylabel('Var(Gradiente)', fontsize=12, fontfamily='serif')
                                                ax.set_zlabel('Custo', fontsize=12, fontfamily='serif')

                                                cbar = plt.colorbar(scatter, ax=ax, label='Custo', shrink=0.8, pad=0.1)
                                                cbar.ax.tick_params(labelsize=10)

                                                plt.tight_layout()

                                                barren_filename = f"barren3d_{nome_dataset}_seed{seed}_{arq}_{init}_{ruido}_nivel{nivel:.4f}.png"
                                                barren_path = os.path.join(pasta_barren, barren_filename)
                                                plt.savefig(barren_path, dpi=150, bbox_inches='tight', facecolor='white')
                                                # Fechamento expl√≠cito da figura para evitar warning de tipo do Pylance
                                                try:
                                                    import matplotlib.pyplot as _plt
                                                    _plt.close(fig)
                                                except Exception:
                                                    try:
                                                        plt.close('all')
                                                    except Exception:
                                                        pass
                                                if verbose:
                                                    logger.info(f"    ‚Üí Barren plateau 3D salvo: {barren_filename}")
                                            else:
                                                logger.warning("Dados insuficientes ou incompat√≠veis para gerar gr√°fico 3D dos gradientes.")
                                        except Exception as e:
                                            logger.warning(f"Falha ao salvar gr√°fico 3D barren plateau: {e}")

                            except Exception as e:
                                if verbose:
                                    logger.warning(f"  ‚úó Erro: {str(e)[:50]}")

    total_configs = (len(grid['arquitetura']) * len(grid['estrategia_init']) *
                    len(grid['tipo_ruido']) * len(grid['nivel_ruido']))

    if verbose:
        logger.info(f"Total de configura√ß√µes: {total_configs} por dataset")
        logger.info(f"Total geral: {total_configs * len(datasets)} experimentos\n")

    contador = 0

    # Iterar sobre datasets
    for nome_dataset, dataset in datasets.items():
        if verbose:
            logger.info(f"\n{'='*80}")
            logger.info(f" DATASET: {nome_dataset.upper()}")
            logger.info(f" {dataset['descricao']}")
            logger.info(f"{'='*80}\n")
        # Iterar sobre grid
        for arq in grid['arquitetura']:
            for init in grid['estrategia_init']:
                for ruido in grid['tipo_ruido']:
                    for nivel in grid['nivel_ruido']:
                        # Pular combina√ß√µes inv√°lidas (sem_ruido com n√≠vel > 0)
                        if ruido == 'sem_ruido' and nivel > 0:
                            continue
                        for seed in seed_list:
                            contador += 1
                            # Log detalhado de todos os par√¢metros
                            if verbose:
                                logger.info(
                                    f"[{contador:3d}/{total_configs * n_seeds}] "
                                    f"Dataset: {nome_dataset} | Seed: {seed} | Qubits: 4 | Camadas: 2 | "
                                    f"Arquitetura: {arq} | Init: {init} | Ru√≠do: {ruido} | N√≠vel: {nivel:.4f}"
                                )
                                logger.info(
                                    f"Constantes: œÄ={ConstantesFundamentais.PI:.5f}, e={ConstantesFundamentais.E:.5f}, œÜ={ConstantesFundamentais.PHI:.5f}, ‚Ñè={ConstantesFundamentais.HBAR:.2e}, Œ±={ConstantesFundamentais.ALPHA:.5f}, R‚àû={ConstantesFundamentais.RYDBERG:.2f}"
                                )
                            try:
                                tempo_inicio = time.time()
                                # Criar e treinar VQC
                                vqc = ClassificadorVQC(
                                    n_qubits=4,
                                    n_camadas=2,
                                    arquitetura=arq,
                                    estrategia_init=init,
                                    tipo_ruido=ruido,
                                    nivel_ruido=nivel,
                                    taxa_aprendizado=0.01,
                                    n_epocas=n_epocas,
                                    batch_size=32,
                                    seed=seed,
                                    # Insights: ru√≠do com annealing quando n√≠vel > 0
                                    ruido_schedule=('cosine' if (ruido != 'sem_ruido' and nivel > 0) else None),
                                    ruido_inicial=(nivel if (ruido != 'sem_ruido' and nivel > 0) else None),
                                    ruido_final=(0.001 if (ruido != 'sem_ruido' and nivel > 0) else None),
                                    # Early stopping leve para acelerar
                                    early_stopping=True, patience=5, min_delta=1e-3, val_split=0.1
                                )
                                vqc.fit(dataset['X_train'], dataset['y_train'])
                                tempo_total = time.time() - tempo_inicio
                                # Calcular m√©tricas
                                acuracia_treino = vqc.score(dataset['X_train'], dataset['y_train'])
                                acuracia_teste = vqc.score(dataset['X_test'], dataset['y_test'])
                                gap_treino_teste = acuracia_treino - acuracia_teste
                                # Matriz de confus√£o
                                y_pred = vqc.predict(dataset['X_test'])
                                cm = confusion_matrix(dataset['y_test'], y_pred)
                                # Armazenar resultados
                                resultados.append({
                                    'dataset': nome_dataset,
                                    'arquitetura': arq,
                                    'estrategia_init': init,
                                    'tipo_ruido': ruido,
                                    'nivel_ruido': nivel,
                                    'n_qubits': 4,
                                    'n_camadas': 2,
                                    'acuracia_treino': acuracia_treino,
                                    'acuracia_teste': acuracia_teste,
                                    'gap_treino_teste': gap_treino_teste,
                                    'tempo_segundos': tempo_total,
                                    'custo_final': vqc.historico_['custo'][-1],
                                    'cm_tn': cm[0,0],
                                    'cm_fp': cm[0,1],
                                    'cm_fn': cm[1,0],
                                    'cm_tp': cm[1,1],
                                    'seed': seed
                                })
                                if verbose:
                                    logger.info(
                                        f"  ‚úì Acur√°cia: {acuracia_teste:.4f} | "
                                        f"Gap: {gap_treino_teste:+.4f} | "
                                        f"Tempo: {tempo_total:.1f}s"
                                    )
                            except Exception as e:
                                if verbose:
                                    logger.warning(f"  ‚úó Erro: {str(e)[:50]}")

    # Adicionar baselines cl√°ssicos (SVM e Random Forest)
    for nome_dataset, dataset in datasets.items():
        if verbose:
            logger.info(f"\n{'='*80}")
            logger.info(f" BASELINES CL√ÅSSICOS: {nome_dataset.upper()}")
            logger.info(f"{'='*80}")
        # SVM (RBF)
        try:
            clf_svm = SVC(kernel='rbf', probability=True, random_state=42)
            clf_svm.fit(dataset['X_train'], dataset['y_train'])
            acuracia_treino = clf_svm.score(dataset['X_train'], dataset['y_train'])
            acuracia_teste = clf_svm.score(dataset['X_test'], dataset['y_test'])
            y_pred = clf_svm.predict(dataset['X_test'])
            cm = confusion_matrix(dataset['y_test'], y_pred)
            resultados.append({
                'dataset': nome_dataset,
                'arquitetura': 'SVM',
                'estrategia_init': '-',
                'tipo_ruido': 'classico',
                'nivel_ruido': 0.0,
                'n_qubits': 0,
                'n_camadas': 0,
                'acuracia_treino': acuracia_treino,
                'acuracia_teste': acuracia_teste,
                'gap_treino_teste': acuracia_treino - acuracia_teste,
                'tempo_segundos': 0.0,
                'custo_final': 0.0,
                'cm_tn': cm[0,0],
                'cm_fp': cm[0,1],
                'cm_fn': cm[1,0],
                'cm_tp': cm[1,1],
                'seed': 42
            })
            if verbose:
                logger.info(f"  ‚úì SVM (RBF): Acur√°cia teste = {acuracia_teste:.4f}")
        except Exception as e:
            if verbose:
                logger.warning(f"  ‚úó Erro SVM: {str(e)[:50]}")
        # Random Forest
        try:
            clf_rf = RandomForestClassifier(n_estimators=100, random_state=42)
            clf_rf.fit(dataset['X_train'], dataset['y_train'])
            acuracia_treino = clf_rf.score(dataset['X_train'], dataset['y_train'])
            acuracia_teste = clf_rf.score(dataset['X_test'], dataset['y_test'])
            y_pred = clf_rf.predict(dataset['X_test'])
            cm = confusion_matrix(dataset['y_test'], y_pred)
            resultados.append({
                'dataset': nome_dataset,
                'arquitetura': 'RandomForest',
                'estrategia_init': '-',
                'tipo_ruido': 'classico',
                'nivel_ruido': 0.0,
                'n_qubits': 0,
                'n_camadas': 0,
                'acuracia_treino': acuracia_treino,
                'acuracia_teste': acuracia_teste,
                'gap_treino_teste': acuracia_treino - acuracia_teste,
                'tempo_segundos': 0.0,
                'custo_final': 0.0,
                'cm_tn': cm[0,0],
                'cm_fp': cm[0,1],
                'cm_fn': cm[1,0],
                'cm_tp': cm[1,1],
                'seed': 42
            })
            if verbose:
                logger.info(f"  ‚úì Random Forest: Acur√°cia teste = {acuracia_teste:.4f}")
        except Exception as e:
            if verbose:
                logger.warning(f"  ‚úó Erro RF: {str(e)[:50]}")

    df_resultados = pd.DataFrame(resultados)
    # Salvar CSV consolidado e atualizar metadata
    if pasta_resultados is not None:
        csv_path = os.path.join(pasta_resultados, 'resultados_completos_artigo.csv')
        df_resultados.to_csv(csv_path, index=False)
        # Adicionar granularidade m√°xima ao metadata
        metadata['csv_consolidado'] = csv_path
        metadata['csvs_individuais'] = [os.path.join('experimentos_individuais', f) for f in os.listdir(pasta_individual) if f.endswith('.csv')]
        # Atualizar lista de arquivos
        metadata['arquivos_gerados'] = [f for f in os.listdir(pasta_resultados) if os.path.isfile(os.path.join(pasta_resultados, f))]
        # Salvar metadata.json
        if metadata_path:
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False, default=str)
    if verbose:
        logger.info(f"\n{'='*80}")
        logger.info(f" ‚úì GRID SEARCH CONCLU√çDO: {len(df_resultados)} experimentos")
        logger.info(f"{'='*80}\n")
    return df_resultados


# ============================================================================
# M√ìDULO 7: AN√ÅLISES ESTAT√çSTICAS
# ============================================================================

def executar_analises_estatisticas(df, verbose=True, pasta_resultados=None):
    """Executa an√°lises estat√≠sticas principais do artigo."""
    import os

    # Inicializar metadata
    analise_meta: dict[str, Any] = {
        'tipo': 'analises_estatisticas',
        'data_execucao': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'arquivos_gerados': [],
        'csvs': {}
    }
    metadata_path: Optional[str] = None
    pasta_individual: Optional[str] = None

    if pasta_resultados is not None:
        pasta_individual = os.path.join(pasta_resultados, 'analises_individuais')
        os.makedirs(pasta_individual, exist_ok=True)
        os.makedirs(pasta_resultados, exist_ok=True)
        # Forense: README e metadata
        readme_path = os.path.join(pasta_resultados, 'README_analises_estatisticas.md')
        metadata_path = os.path.join(pasta_resultados, 'metadata_analises_estatisticas.json')
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(
                "# An√°lises Estat√≠sticas\n\n"
                f"- Data de execu√ß√£o: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                "- Conte√∫do: ANOVA, compara√ß√£o de inicializa√ß√µes, overfitting, effect sizes, post-hoc.\n"
            )

    if verbose:
        logger.info("="*80)
        logger.info(" AN√ÅLISES ESTAT√çSTICAS")
        logger.info("="*80)

    analises = {}

    # 1. ANOVA 2-way: Noise √ó Dataset
    if verbose:
        logger.info("\n1. ANOVA 2-WAY: Noise Level √ó Dataset")
        logger.info("-"*80)

    # Verificar se h√° dados
    if len(df) == 0:
        logger.warning("Nenhum resultado dispon√≠vel para an√°lise!")
        return {'erro': 'Sem dados'}

    df_anova = df.copy()
    df_anova['nivel_ruido_cat'] = df_anova['nivel_ruido'].astype(str)

    # Proteger: ANOVA 2-way requer pelo menos 2 n√≠veis em cada fator
    if df_anova['nivel_ruido_cat'].nunique() >= 2 and df_anova['dataset'].nunique() >= 2:
        model = ols('acuracia_teste ~ C(nivel_ruido_cat) + C(dataset) + C(nivel_ruido_cat):C(dataset)',
                    data=df_anova).fit()
        anova_2way = anova_lm(model, typ=2)
        analises['anova_2way'] = anova_2way
        if verbose:
            print(anova_2way)
    else:
        analises['anova_2way'] = 'Insuficiente para ANOVA 2-way (>=2 n√≠veis por fator)'
        if verbose:
            logger.info('Dados insuficientes para ANOVA 2-way, an√°lise pulada.')

    # 2. Compara√ß√£o de inicializa√ß√µes
    if verbose:
        logger.info("\n2. COMPARA√á√ÉO DE INICIALIZA√á√ïES")
        logger.info("-"*80)

    # Build aggregation dict based on available columns
    agg_dict = {'acuracia_teste': ['mean', 'std']}
    if 'tempo_segundos' in df.columns:
        agg_dict['tempo_segundos'] = 'mean'
    else:
        if verbose:
            logger.info("  ‚ÑπÔ∏è Coluna 'tempo_segundos' n√£o dispon√≠vel, an√°lise de tempo n√£o ser√° inclu√≠da.")
    
    comp_init = df.groupby('estrategia_init').agg(agg_dict).round(4)

    if verbose:
        print(comp_init)
    # Salvar CSV resumo de inicializa√ß√µes
    if pasta_resultados is not None:
        comp_init_path = os.path.join(pasta_resultados, 'analise_comparacao_inicializacoes.csv')
        try:
            comp_init.to_csv(comp_init_path)
            analise_meta['csvs']['comparacao_inicializacoes'] = comp_init_path
        except Exception:
            pass

    # 2b. Compara√ß√£o consolidada: VQC vs Baselines Cl√°ssicos (SVM/RF)
    try:
        if verbose:
            logger.info("\n2b. COMPARA√á√ÉO: VQC vs SVM/RF (por dataset)")
            logger.info("-"*80)
        df_q = df[df['tipo_ruido'] != 'classico']
        df_class = df[df['tipo_ruido'] == 'classico']
        # Melhor VQC por dataset (maior acur√°cia)
        vqc_best = df_q.groupby('dataset')['acuracia_teste'].max().rename('vqc_melhor')
        # VQC sem ru√≠do (m√©dia por dataset)
        vqc_sem = df[df['tipo_ruido'] == 'sem_ruido'].groupby('dataset')['acuracia_teste'].mean().rename('vqc_sem_ruido_media')
        # Baselines
        svm = (
            df_class[df_class['arquitetura'] == 'SVM']
            .groupby('dataset')['acuracia_teste']
            .mean()
            .rename('svm')
        )
        rf = (
            df_class[df_class['arquitetura'] == 'RandomForest']
            .groupby('dataset')['acuracia_teste']
            .mean()
            .rename('rf')
        )
        comp = pd.concat([vqc_best, vqc_sem, svm, rf], axis=1)
        # Deltas (podem ser NaN se baseline ausente)
        comp['delta_vqc_svm'] = comp['vqc_melhor'] - comp['svm']
        comp['delta_vqc_rf'] = comp['vqc_melhor'] - comp['rf']
        comp = comp.reset_index()
        if verbose:
            logger.info("Resumo por dataset:")
            logger.info(comp.round(4).to_string(index=False))
        if pasta_resultados is not None:
            comp_path = os.path.join(pasta_resultados, 'comparacao_baselines.csv')
            try:
                comp.to_csv(comp_path, index=False)
                analise_meta['csvs']['comparacao_baselines'] = comp_path
            except Exception:
                pass
    except Exception as e:
        if verbose:
            logger.warning(f"Falha ao gerar comparacao_baselines.csv: {str(e)[:80]}")
    # Salvar DataFrame completo das an√°lises estat√≠sticas
    try:
        df.to_csv(os.path.join(str(pasta_resultados), 'analises_estatisticas_completo.csv'), index=False)
        analise_meta['csvs']['completo'] = os.path.join(str(pasta_resultados), 'analises_estatisticas_completo.csv')
        # Salvar cada an√°lise individualmente em CSV
        if pasta_individual is not None:
            for idx, row in df.iterrows():
                id_analise = f"analise_{idx:05d}"
                df_row = pd.DataFrame([row])
                csv_analise_path = os.path.join(str(pasta_individual), f"{id_analise}.csv")
                df_row.to_csv(csv_analise_path, index=False)
            # Listar CSVs apenas se pasta_individual √© v√°lido
            analise_meta['csvs_individuais'] = [os.path.join('analises_individuais', f) for f in os.listdir(str(pasta_individual)) if f.endswith('.csv')]
    except Exception:
        pass

    analises['comparacao_inicializacoes'] = comp_init

    # 3. An√°lise de overfitting
    if verbose:
        logger.info("\n3. AN√ÅLISE DE OVERFITTING")
        logger.info("-"*80)

    # Check if required columns exist
    if 'gap_treino_teste' in df.columns and 'tipo_ruido' in df.columns:
        gap_sem_ruido = df[df['tipo_ruido'] == 'sem_ruido']['gap_treino_teste'].mean()
        mask_otimo = (df['tipo_ruido'] == 'depolarizante') & (df['nivel_ruido'] == 0.01)
        gap_com_ruido = df[mask_otimo]['gap_treino_teste'].mean()
        
        if not np.isnan(gap_sem_ruido) and not np.isnan(gap_com_ruido) and gap_sem_ruido != 0:
            reducao_overfitting = ((gap_sem_ruido - gap_com_ruido) / gap_sem_ruido) * 100
        else:
            reducao_overfitting = 0.0

        if verbose:
            logger.info(f"Gap sem ru√≠do: {gap_sem_ruido:.4f}")
            logger.info(f"Gap com ru√≠do √≥timo: {gap_com_ruido:.4f}")
            logger.info(f"Redu√ß√£o de overfitting: {reducao_overfitting:.1f}%")

        analises['overfitting'] = {
            'gap_sem_ruido': gap_sem_ruido,
            'gap_com_ruido': gap_com_ruido,
            'reducao_percent': reducao_overfitting
        }
    else:
        if verbose:
            logger.info("Colunas necess√°rias n√£o dispon√≠veis para an√°lise de overfitting.")
        analises['overfitting'] = {
            'gap_sem_ruido': np.nan,
            'gap_com_ruido': np.nan,
            'reducao_percent': 0.0
        }

    # 4. Effect Sizes (Cohen's d, Glass's Œî, Hedges' g)
    if verbose:
        logger.info("\n4. EFFECT SIZES")
        logger.info("-"*80)

    sem_ruido = df[df['tipo_ruido'] == 'sem_ruido']['acuracia_teste'].values
    com_ruido = df[df['tipo_ruido'] != 'sem_ruido']['acuracia_teste'].values

    if len(sem_ruido) > 0 and len(com_ruido) > 0:
        cohen_d = TestesEstatisticosAvancados.cohen_d(com_ruido, sem_ruido)
        glass_delta = TestesEstatisticosAvancados.glass_delta(com_ruido, sem_ruido)
        hedges_g = TestesEstatisticosAvancados.hedges_g(com_ruido, sem_ruido)

        if verbose:
            logger.info(f"Cohen's d: {cohen_d:.4f}")
            logger.info(f"Glass's Œî: {glass_delta:.4f}")
            logger.info(f"Hedges' g: {hedges_g:.4f}")

        analises['effect_sizes'] = {
            'cohen_d': cohen_d,
            'glass_delta': glass_delta,
            'hedges_g': hedges_g
        }

    # 5. Testes Post-hoc (Bonferroni)
    if verbose:
        logger.info("\n5. TESTES POST-HOC")
        logger.info("-"*80)

    # Comparar cada tipo de ru√≠do vs. baseline
    tipos_ruido = df['tipo_ruido'].unique()
    p_values = []

    for tipo in tipos_ruido:
        if tipo != 'sem_ruido':
            grupo1 = df[df['tipo_ruido'] == tipo]['acuracia_teste'].values
            grupo2 = df[df['tipo_ruido'] == 'sem_ruido']['acuracia_teste'].values

            if len(grupo1) > 0 and len(grupo2) > 0:
                _, p_val = ttest_ind(grupo1, grupo2)
                p_values.append((tipo, p_val))

    if len(p_values) > 0:
        # Corre√ß√£o de Bonferroni
        p_vals_only = [p for _, p in p_values]
        significantes = TestesEstatisticosAvancados.bonferroni(p_vals_only, alpha=0.05)

        if verbose:
            for (tipo, p_val), sig in zip(p_values, significantes):
                status = "‚úì Significativo" if sig else "‚úó N√£o significativo"
                logger.info(f"{tipo:20s}: p={p_val:.4f} {status}")

        analises['posthoc_bonferroni'] = list(zip(p_values, significantes))

    # Persistir metadata
    if pasta_resultados is not None and metadata_path is not None:
        # Atualizar lista de arquivos gerados no diret√≥rio
        try:
            analise_meta['arquivos_gerados'] = [f for f in os.listdir(pasta_resultados) if os.path.isfile(os.path.join(pasta_resultados, f))]
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(analise_meta, f, indent=2, ensure_ascii=False, default=str)
        except Exception:
            pass
    return analises


# ============================================================================
# M√ìDULO 8: VISUALIZA√á√ïES
# ============================================================================

def gerar_visualizacoes(df, salvar=True, pasta_resultados=None):
    """Gera as figuras principais do artigo."""
    import os

    # Inicializar metadata
    viz_meta: dict[str, Any] = {
        'tipo': 'visualizacoes',
        'data_execucao': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'arquivos_gerados': [],
        'figuras': []
    }
    metadata_path: Optional[str] = None
    pasta_individual: Optional[str] = None

    if pasta_resultados is not None:
        pasta_individual = os.path.join(pasta_resultados, 'visualizacoes_individuais')
        os.makedirs(pasta_individual, exist_ok=True)
        os.makedirs(pasta_resultados, exist_ok=True)
        # Forense: README e metadata
        readme_path = os.path.join(pasta_resultados, 'README_visualizacoes.md')
        metadata_path = os.path.join(pasta_resultados, 'metadata_visualizacoes.json')
        os.makedirs(os.path.dirname(readme_path), exist_ok=True)
        os.makedirs(os.path.dirname(metadata_path), exist_ok=True)
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(
                "# Visualiza√ß√µes Geradas\n\n"
                f"- Data de execu√ß√£o: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                "- Figuras: 2, 2b, 3, 3b, 4, 5, 6, 7\n"
            )

    logger.info("="*80)
    logger.info(" GERANDO VISUALIZA√á√ïES")
    logger.info("="*80)

    figuras = {}

    # FIGURA 2: Beneficial Noise (PRINCIPAL)
    logger.info("\nGerando Figura 2: Beneficial Noise...")

    fig2 = px.scatter(
        df, x='nivel_ruido', y='acuracia_teste',
        color='tipo_ruido', facet_col='dataset',
        title="Figura 2: Impacto do Ru√≠do Qu√¢ntico na Acur√°cia",
        labels={'nivel_ruido': 'N√≠vel de Ru√≠do', 'acuracia_teste': 'Acur√°cia no Teste'},
        height=500
    )

    if salvar:
        # Qualis A1: aprimorar layout
        fig2.update_layout(
            font=dict(family='serif', size=18, color='black'),
            title_font=dict(size=22, family='serif', color='black', weight="bold"),
            legend_title_font=dict(size=18, family='serif', color='black', weight="bold"),
            legend_font=dict(size=16, family='serif', color='black'),
            margin=dict(l=60, r=40, t=80, b=60),
            paper_bgcolor='white',
            plot_bgcolor='white',
        )
        fig2.update_traces(marker=dict(line=dict(width=1, color='black')))
        fig2.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        fig2.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        # Exportar em alta resolu√ß√£o e formatos cient√≠ficos
        path_html = os.path.join(pasta_resultados if pasta_resultados else '', 'figura2_beneficial_noise.html')
        os.makedirs(os.path.dirname(path_html), exist_ok=True)
        fig2.write_html(path_html)
        if pasta_resultados is not None:
            viz_meta['figuras'].append(path_html)
            path_png = os.path.join(pasta_resultados, 'figura2_beneficial_noise.png')
            path_pdf = os.path.join(pasta_resultados, 'figura2_beneficial_noise.pdf')
            path_svg = os.path.join(pasta_resultados, 'figura2_beneficial_noise.svg')
            for p in [path_png, path_pdf, path_svg]:
                os.makedirs(os.path.dirname(p), exist_ok=True)
            fig2.write_image(path_png, format='png', scale=3, width=1200, height=800)
            fig2.write_image(path_pdf, format='pdf', width=1200, height=800)
            fig2.write_image(path_svg, format='svg', width=1200, height=800)
            viz_meta['figuras'] += [path_png, path_pdf, path_svg]
    figuras['figura2'] = fig2

    # FIGURA 2b: Beneficial Noise com IC95% por grupo (dataset, tipo_ruido, nivel_ruido)
    logger.info("Gerando Figura 2b: Beneficial Noise com IC95%...")
    try:
        df_q = df[df['tipo_ruido'] != 'classico'].copy()
        grp_cols = ['dataset', 'tipo_ruido', 'nivel_ruido']
        df_ci = (
            df_q.groupby(grp_cols)
            .agg(media=('acuracia_teste', 'mean'), desvio=('acuracia_teste', 'std'), n=('acuracia_teste', 'count'))
            .reset_index()
        )
        # Evitar divis√£o por zero para n<=1
        df_ci['sem'] = df_ci.apply(lambda r: (r['desvio'] / np.sqrt(r['n'])) if r['n'] > 1 and r['desvio'] == r['desvio'] else 0.0, axis=1)
        df_ci['ci95'] = 1.96 * df_ci['sem']
        fig2b = px.scatter(
            df_ci, x='nivel_ruido', y='media', color='tipo_ruido', facet_col='dataset',
            error_y='ci95',
            title='Figura 2b: Acur√°cia M√©dia ¬± IC95% por N√≠vel de Ru√≠do',
            labels={'nivel_ruido': 'N√≠vel de Ru√≠do', 'media': 'Acur√°cia M√©dia (Teste)'},
            height=500
        )
        # Apar√™ncia consistente
        fig2b.update_layout(
            font=dict(family='serif', size=18, color='black'),
            title_font=dict(size=22, family='serif', color='black', weight="bold"),
            legend_title_font=dict(size=18, family='serif', color='black', weight="bold"),
            legend_font=dict(size=16, family='serif', color='black'),
            margin=dict(l=60, r=40, t=80, b=60),
            paper_bgcolor='white', plot_bgcolor='white',
        )
        fig2b.update_traces(marker=dict(line=dict(width=1, color='black')))
        fig2b.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        fig2b.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        if salvar:
            path_html = os.path.join(pasta_resultados if pasta_resultados else '', 'figura2b_beneficial_noise_ic95.html')
            os.makedirs(os.path.dirname(path_html), exist_ok=True)
            fig2b.write_html(path_html)
            if pasta_resultados is not None:
                viz_meta['figuras'].append(path_html)
                path_png = os.path.join(pasta_resultados, 'figura2b_beneficial_noise_ic95.png')
                path_pdf = os.path.join(pasta_resultados, 'figura2b_beneficial_noise_ic95.pdf')
                path_svg = os.path.join(pasta_resultados, 'figura2b_beneficial_noise_ic95.svg')
                for p in [path_png, path_pdf, path_svg]:
                    os.makedirs(os.path.dirname(p), exist_ok=True)
                fig2b.write_image(path_png, format='png', scale=3, width=1200, height=800)
                fig2b.write_image(path_pdf, format='pdf', width=1200, height=800)
                fig2b.write_image(path_svg, format='svg', width=1200, height=800)
                viz_meta['figuras'] += [path_png, path_pdf, path_svg]
        figuras['figura2b'] = fig2b
    except Exception as e:
        logger.warning(f"N√£o foi poss√≠vel gerar a Figura 2b (IC95%): {str(e)[:80]}")

    # FIGURA 3: Noise Type Comparison
    logger.info("Gerando Figura 3: Compara√ß√£o de Tipos de Ru√≠do...")

    fig3 = px.box(
        df, x='tipo_ruido', y='acuracia_teste', color='tipo_ruido',
        title="Figura 3: Compara√ß√£o de Tipos de Ru√≠do",
        labels={'tipo_ruido': 'Tipo de Ru√≠do', 'acuracia_teste': 'Acur√°cia no Teste'},
        height=500
    )

    if salvar:
        fig3.update_layout(
            font=dict(family='serif', size=18, color='black'),
            title_font=dict(size=22, family='serif', color='black', weight="bold"),
            legend_title_font=dict(size=18, family='serif', color='black', weight="bold"),
            legend_font=dict(size=16, family='serif', color='black'),
            margin=dict(l=60, r=40, t=80, b=60),
            paper_bgcolor='white',
            plot_bgcolor='white',
        )
        fig3.update_traces(marker=dict(line=dict(width=1, color='black')))
        fig3.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        fig3.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        path_html = os.path.join(pasta_resultados if pasta_resultados else '', 'figura3_noise_types.html')
        os.makedirs(os.path.dirname(path_html), exist_ok=True)
        fig3.write_html(path_html)
        if pasta_resultados is not None:
            viz_meta['figuras'].append(path_html)
            path_png = os.path.join(pasta_resultados, 'figura3_noise_types.png')
            path_pdf = os.path.join(pasta_resultados, 'figura3_noise_types.pdf')
            path_svg = os.path.join(pasta_resultados, 'figura3_noise_types.svg')
            for p in [path_png, path_pdf, path_svg]:
                os.makedirs(os.path.dirname(p), exist_ok=True)
            fig3.write_image(path_png, format='png', scale=3, width=1200, height=800)
            fig3.write_image(path_pdf, format='pdf', width=1200, height=800)
            fig3.write_image(path_svg, format='svg', width=1200, height=800)
            viz_meta['figuras'] += [path_png, path_pdf, path_svg]
    figuras['figura3'] = fig3

    # FIGURA 3b: M√©dias por Tipo de Ru√≠do com IC95% (facet por dataset)
    logger.info("Gerando Figura 3b: Tipos de Ru√≠do com IC95%...")
    try:
        df_q2 = df[df['tipo_ruido'] != 'classico'].copy()
        grp_cols3 = ['dataset', 'tipo_ruido']
        df_ci3 = (
            df_q2.groupby(grp_cols3)
            .agg(media=('acuracia_teste', 'mean'), desvio=('acuracia_teste', 'std'), n=('acuracia_teste', 'count'))
            .reset_index()
        )
        df_ci3['sem'] = df_ci3.apply(lambda r: (r['desvio'] / np.sqrt(r['n'])) if r['n'] > 1 and r['desvio'] == r['desvio'] else 0.0, axis=1)
        df_ci3['ci95'] = 1.96 * df_ci3['sem']
        fig3b = px.bar(
            df_ci3, x='tipo_ruido', y='media', color='tipo_ruido', facet_col='dataset',
            error_y='ci95', barmode='group',
            title='Figura 3b: Acur√°cia M√©dia ¬± IC95% por Tipo de Ru√≠do',
            labels={'media': 'Acur√°cia M√©dia (Teste)', 'tipo_ruido': 'Tipo de Ru√≠do'}, height=500
        )
        if salvar:
            fig3b.update_layout(
                font=dict(family='serif', size=18, color='black'),
                title_font=dict(size=22, family='serif', color='black', weight="bold"),
                legend_title_font=dict(size=18, family='serif', color='black', weight="bold"),
                legend_font=dict(size=16, family='serif', color='black'),
                margin=dict(l=60, r=40, t=80, b=60),
                paper_bgcolor='white', plot_bgcolor='white',
            )
            path_html = os.path.join(pasta_resultados if pasta_resultados else '', 'figura3b_noise_types_ic95.html')
            os.makedirs(os.path.dirname(path_html), exist_ok=True)
            fig3b.write_html(path_html)
            if pasta_resultados is not None:
                viz_meta['figuras'].append(path_html)
                path_png = os.path.join(pasta_resultados, 'figura3b_noise_types_ic95.png')
                path_pdf = os.path.join(pasta_resultados, 'figura3b_noise_types_ic95.pdf')
                path_svg = os.path.join(pasta_resultados, 'figura3b_noise_types_ic95.svg')
                for p in [path_png, path_pdf, path_svg]:
                    os.makedirs(os.path.dirname(p), exist_ok=True)
                fig3b.write_image(path_png, format='png', scale=3, width=1200, height=800)
                fig3b.write_image(path_pdf, format='pdf', width=1200, height=800)
                fig3b.write_image(path_svg, format='svg', width=1200, height=800)
                viz_meta['figuras'] += [path_png, path_pdf, path_svg]
        figuras['figura3b'] = fig3b
    except Exception as e:
        logger.warning(f"N√£o foi poss√≠vel gerar a Figura 3b (IC95%): {str(e)[:80]}")

    # FIGURA 4: Initialization Strategies
    logger.info("Gerando Figura 4: Estrat√©gias de Inicializa√ß√£o...")

    fig4 = px.box(
        df, x='estrategia_init', y='acuracia_teste', color='estrategia_init',
        title="Figura 4: Impacto da Estrat√©gia de Inicializa√ß√£o",
        labels={'estrategia_init': 'Estrat√©gia', 'acuracia_teste': 'Acur√°cia no Teste'},
        height=500
    )

    if salvar:
        fig4.update_layout(
            font=dict(family='serif', size=18, color='black'),
            title_font=dict(size=22, family='serif', color='black', weight="bold"),
            legend_title_font=dict(size=18, family='serif', color='black', weight="bold"),
            legend_font=dict(size=16, family='serif', color='black'),
            margin=dict(l=60, r=40, t=80, b=60),
            paper_bgcolor='white',
            plot_bgcolor='white',
        )
        fig4.update_traces(marker=dict(line=dict(width=1, color='black')))
        fig4.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        fig4.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        path_html = os.path.join(pasta_resultados if pasta_resultados else '', 'figura4_initialization.html')
        os.makedirs(os.path.dirname(path_html), exist_ok=True)
        fig4.write_html(path_html)
        if pasta_resultados is not None:
            viz_meta['figuras'].append(path_html)
            path_png = os.path.join(pasta_resultados, 'figura4_initialization.png')
            path_pdf = os.path.join(pasta_resultados, 'figura4_initialization.pdf')
            path_svg = os.path.join(pasta_resultados, 'figura4_initialization.svg')
            for p in [path_png, path_pdf, path_svg]:
                os.makedirs(os.path.dirname(p), exist_ok=True)
            fig4.write_image(path_png, format='png', scale=3, width=1200, height=800)
            fig4.write_image(path_pdf, format='pdf', width=1200, height=800)
            fig4.write_image(path_svg, format='svg', width=1200, height=800)
            viz_meta['figuras'] += [path_png, path_pdf, path_svg]
    figuras['figura4'] = fig4

    # FIGURA 5: Architecture Trade-offs
    logger.info("Gerando Figura 5: Trade-offs de Arquitetura...")

    # Check if tempo_segundos is available, otherwise use a placeholder
    if 'tempo_segundos' in df.columns:
        fig5 = px.scatter(
            df, x='tempo_segundos', y='acuracia_teste', color='arquitetura',
            title="Figura 5: Trade-off Tempo vs. Acur√°cia",
            labels={'tempo_segundos': 'Tempo (s)', 'acuracia_teste': 'Acur√°cia no Teste'},
            height=500
        )
    else:
        # Fallback: use index as x-axis if tempo_segundos not available
        fig5 = px.scatter(
            df, x=df.index, y='acuracia_teste', color='arquitetura',
            title="Figura 5: Acur√°cia por Arquitetura",
            labels={'x': 'Experimento', 'acuracia_teste': 'Acur√°cia no Teste'},
            height=500
        )

    if salvar:
        fig5.update_layout(
            font=dict(family='serif', size=18, color='black'),
            title_font=dict(size=22, family='serif', color='black', weight="bold"),
            legend_title_font=dict(size=18, family='serif', color='black', weight="bold"),
            legend_font=dict(size=16, family='serif', color='black'),
            margin=dict(l=60, r=40, t=80, b=60),
            paper_bgcolor='white',
            plot_bgcolor='white',
        )
        fig5.update_traces(marker=dict(line=dict(width=1, color='black')))
        fig5.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        fig5.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        path_html = os.path.join(pasta_resultados if pasta_resultados else '', 'figura5_architecture_tradeoffs.html')
        os.makedirs(os.path.dirname(path_html), exist_ok=True)
        fig5.write_html(path_html)
        if pasta_resultados is not None:
            viz_meta['figuras'].append(path_html)
            path_png = os.path.join(pasta_resultados, 'figura5_architecture_tradeoffs.png')
            path_pdf = os.path.join(pasta_resultados, 'figura5_architecture_tradeoffs.pdf')
            path_svg = os.path.join(pasta_resultados, 'figura5_architecture_tradeoffs.svg')
            for p in [path_png, path_pdf, path_svg]:
                os.makedirs(os.path.dirname(p), exist_ok=True)
            fig5.write_image(path_png, format='png', scale=3, width=1200, height=800)
            fig5.write_image(path_pdf, format='pdf', width=1200, height=800)
            fig5.write_image(path_svg, format='svg', width=1200, height=800)
            viz_meta['figuras'] += [path_png, path_pdf, path_svg]
    figuras['figura5'] = fig5

    # FIGURA 7: Overfitting Analysis
    logger.info("Gerando Figura 7: An√°lise de Overfitting...")

    # Check if required columns exist
    if 'gap_treino_teste' in df.columns and 'acuracia_treino' in df.columns:
        # Garantir que os tamanhos sejam positivos (usar valor absoluto)
        df_fig7 = df.copy()
        df_fig7['gap_abs'] = df_fig7['gap_treino_teste'].abs()

        fig7 = px.scatter(
            df_fig7, x='acuracia_treino', y='acuracia_teste', color='tipo_ruido',
            size='gap_abs', hover_data=['dataset', 'arquitetura', 'gap_treino_teste'],
            title="Figura 7: An√°lise de Overfitting (Gap Treino-Teste)",
            labels={'acuracia_treino': 'Acur√°cia Treino', 'acuracia_teste': 'Acur√°cia Teste'},
            height=500
        )

        # Adicionar linha diagonal (sem overfitting)
        fig7.add_trace(go.Scatter(
            x=[0, 1], y=[0, 1], mode='lines',
            line=dict(dash='dash', color='gray'),
            name='Sem Overfitting'
        ))
    else:
        # Fallback: create simple scatter plot without overfitting info
        logger.info("Coluna gap_treino_teste n√£o dispon√≠vel, gerando visualiza√ß√£o simplificada...")
        fig7 = px.scatter(
            df, x=df.index, y='acuracia_teste', color='tipo_ruido',
            title="Figura 7: Acur√°cia por Tipo de Ru√≠do",
            labels={'x': 'Experimento', 'acuracia_teste': 'Acur√°cia Teste'},
            height=500
        )

    if salvar:
        fig7.update_layout(
            font=dict(family='serif', size=18, color='black'),
            title_font=dict(size=22, family='serif', color='black', weight="bold"),
            legend_title_font=dict(size=18, family='serif', color='black', weight="bold"),
            legend_font=dict(size=16, family='serif', color='black'),
            margin=dict(l=60, r=40, t=80, b=60),
            paper_bgcolor='white',
            plot_bgcolor='white',
        )
        fig7.update_traces(marker=dict(line=dict(width=1, color='black')))
        fig7.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        fig7.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        path_html = os.path.join(pasta_resultados if pasta_resultados else '', 'figura7_overfitting.html')
        os.makedirs(os.path.dirname(path_html), exist_ok=True)
        fig7.write_html(path_html)
        if pasta_resultados is not None:
            viz_meta['figuras'].append(path_html)
            path_png = os.path.join(pasta_resultados, 'figura7_overfitting.png')
            path_pdf = os.path.join(pasta_resultados, 'figura7_overfitting.pdf')
            path_svg = os.path.join(pasta_resultados, 'figura7_overfitting.svg')
            for p in [path_png, path_pdf, path_svg]:
                os.makedirs(os.path.dirname(p), exist_ok=True)
            fig7.write_image(path_png, format='png', scale=3, width=1200, height=800)
            fig7.write_image(path_pdf, format='pdf', width=1200, height=800)
            fig7.write_image(path_svg, format='svg', width=1200, height=800)
            viz_meta['figuras'] += [path_png, path_pdf, path_svg]
    figuras['figura7'] = fig7

    # FIGURA 6: Effect Sizes Comparison
    logger.info("Gerando Figura 6: Compara√ß√£o de Effect Sizes...")

    # Calcular effect sizes para cada par ru√≠do vs. baseline
    effect_data = []
    for dataset in df['dataset'].unique():
        df_ds = df[df['dataset'] == dataset]
        sem_ruido = df_ds[df_ds['tipo_ruido'] == 'sem_ruido']['acuracia_teste'].values

        for tipo in df_ds['tipo_ruido'].unique():
            if tipo != 'sem_ruido':
                com_ruido = df_ds[df_ds['tipo_ruido'] == tipo]['acuracia_teste'].values

                if len(sem_ruido) > 0 and len(com_ruido) > 0:
                    cohen_d = TestesEstatisticosAvancados.cohen_d(com_ruido, sem_ruido)
                    glass_d = TestesEstatisticosAvancados.glass_delta(com_ruido, sem_ruido)
                    hedges_g = TestesEstatisticosAvancados.hedges_g(com_ruido, sem_ruido)

                    effect_data.append({
                        'dataset': dataset,
                        'tipo_ruido': tipo,
                        "Cohen's d": cohen_d,
                        "Glass's Œî": glass_d,
                        "Hedges' g": hedges_g
                    })

    if len(effect_data) > 0:
        df_effects = pd.DataFrame(effect_data)
        df_effects_melted = df_effects.melt(
            id_vars=['dataset', 'tipo_ruido'],
            value_vars=["Cohen's d", "Glass's Œî", "Hedges' g"],
            var_name='M√©trica',
            value_name='Effect Size'
        )

        fig6 = px.bar(
            df_effects_melted,
            x='tipo_ruido',
            y='Effect Size',
            color='M√©trica',
            facet_col='dataset',
            barmode='group',
            title="Figura 6: Compara√ß√£o de Effect Sizes (vs. Baseline)",
            height=500
        )

        # Adicionar linhas de refer√™ncia (small/medium/large effects)
        # Usar getattr para evitar warnings de tipo com atributos din√¢micos do Plotly
        annotations = getattr(fig6.layout, 'annotations', None)
        if annotations:
            for annotation in annotations:
                annotation.text = annotation.text.replace('dataset=', '')

        if salvar:
            path = os.path.join(pasta_resultados if pasta_resultados else '', 'figura6_effect_sizes.html')
            os.makedirs(os.path.dirname(path), exist_ok=True)
            fig6.write_html(path)
            if pasta_resultados is not None:
                viz_meta['figuras'].append(path)

        figuras['figura6'] = fig6

    logger.info(f"\n‚úì {len(figuras)} figuras geradas!")
    # Persistir metadata
    if pasta_resultados is not None and metadata_path is not None:
        try:
            viz_meta['arquivos_gerados'] = [f for f in os.listdir(pasta_resultados) if os.path.isfile(os.path.join(pasta_resultados, f))]
            os.makedirs(os.path.dirname(metadata_path), exist_ok=True)
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(viz_meta, f, indent=2, ensure_ascii=False, default=str)
                # Salvar DataFrame completo das visualiza√ß√µes
                csv_completo_path = os.path.join(pasta_resultados, 'visualizacoes_completo.csv')
                os.makedirs(os.path.dirname(csv_completo_path), exist_ok=True)
                df.to_csv(csv_completo_path, index=False)
                viz_meta['csv_completo'] = csv_completo_path
                # Salvar cada visualiza√ß√£o individualmente em CSV
                if pasta_individual is not None:
                    os.makedirs(pasta_individual, exist_ok=True)
                    for idx, row in df.iterrows():
                        id_vis = f"vis_{idx:05d}"
                        df_row = pd.DataFrame([row])
                        csv_vis_path = os.path.join(pasta_individual, f"{id_vis}.csv")
                        os.makedirs(os.path.dirname(csv_vis_path), exist_ok=True)
                        df_row.to_csv(csv_vis_path, index=False)
                    viz_meta['csvs_individuais'] = [os.path.join('visualizacoes_individuais', f) for f in os.listdir(pasta_individual) if f.endswith('.csv')]
        except Exception:
            pass

    return figuras


# ============================================================================
# M√ìDULO: AN√ÅLISES ESTAT√çSTICAS PROFUNDAS (v7.1)
# ============================================================================

def analise_correlacao_profunda(df: pd.DataFrame, save_path: str = 'figura_correlacao.html'):
    logger.info("\n" + "="*80)
    logger.info(" AN√ÅLISE DE CORRELA√á√ÉO")
    logger.info("="*80)

    # Selecionar colunas num√©ricas
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    corr_matrix = df[numeric_cols].corr()

    # Criar heatmap interativo
    fig = px.imshow(
        corr_matrix,
        labels=dict(color="Correla√ß√£o"),
        x=corr_matrix.columns,
        y=corr_matrix.columns,
        color_continuous_scale='RdBu_r',
        zmin=-1, zmax=1,
        title="Matriz de Correla√ß√£o - Todas as Vari√°veis",
        height=700, width=900
    )

    fig.write_html(save_path)
    logger.info(f"‚úì Heatmap salvo: {save_path}")

    # Top correla√ß√µes com acur√°cia
    if 'acuracia_teste' in corr_matrix.columns:
        logger.info("\nTop correla√ß√µes com Acur√°cia de Teste:")
        corr_acc = corr_matrix['acuracia_teste'].abs().sort_values(ascending=False)
        for var, corr in corr_acc.head(8).items():
            if var != 'acuracia_teste':
                # Usar acesso direto ao valor para evitar warnings de tipo
                try:
                    actual_corr = float(corr_matrix.loc['acuracia_teste', var])  # type: ignore[call-overload]
                    logger.info(f"  {var:25s}: {actual_corr:+.4f}")
                except (KeyError, TypeError, ValueError):
                    pass

    return corr_matrix


def analise_pca_profunda(df: pd.DataFrame, save_path: Optional[str] = None):
    if not SKLEARN_ADVANCED_AVAILABLE:
        logger.warning("Scikit-learn PCA n√£o dispon√≠vel")
        return None, None

    logger.info("\n" + "="*80)
    logger.info(" AN√ÅLISE DE COMPONENTES PRINCIPAIS (PCA)")
    logger.info("="*80)

    # Preparar dados
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    X = df[numeric_cols].dropna()

    # Normalizar
    scaler = SklearnStandardScaler()
    X_scaled = scaler.fit_transform(X)

    # PCA
    pca = SklearnPCA()
    X_pca = pca.fit_transform(X_scaled)

    var_exp = pca.explained_variance_ratio_
    var_cum = np.cumsum(var_exp)

    logger.info(f"\nVari√¢ncia explicada (primeiros 3 componentes): {var_cum[2]:.2%}")

    for i in range(min(5, len(var_exp))):
        logger.info(f"  PC{i+1}: {var_exp[i]:.2%} (acumulado: {var_cum[i]:.2%})")

    # Visualiza√ß√£o: Scree plot + Biplot

    fig = make_subplots(
        rows=1, cols=2,
        subplot_titles=('Scree Plot', 'Biplot PC1 vs PC2'),
        specs=[[{'type': 'scatter'}, {'type': 'scatter'}]]
    )

    # Scree plot
    fig.add_trace(
        go.Scatter(x=list(range(1, len(var_exp)+1)), y=var_exp,
                   mode='lines+markers', name='Vari√¢ncia'),
        row=1, col=1
    )

    # Biplot
    df_plot = df.copy()
    df_plot['PC1'] = X_pca[:, 0]
    df_plot['PC2'] = X_pca[:, 1]

    for dataset in df_plot['dataset'].unique():
        df_ds = df_plot[df_plot['dataset'] == dataset]
        fig.add_trace(
            go.Scatter(x=df_ds['PC1'], y=df_ds['PC2'],
                      mode='markers', name=dataset),
            row=1, col=2
        )

    fig.update_layout(height=500, width=1200, title_text="An√°lise PCA")
    if save_path is not None:
        fig.write_html(save_path)
        logger.info(f"‚úì PCA salvo: {save_path}")

    return pca, X_pca


def analise_clustering_profunda(df: pd.DataFrame, n_clusters: int = 3, save_path: Optional[str] = None):
    """An√°lise de clustering com K-means."""
    if not SKLEARN_ADVANCED_AVAILABLE:
        logger.warning("Scikit-learn KMeans n√£o dispon√≠vel")
        return None

    logger.info("\n" + "="*80)
    logger.info(f" AN√ÅLISE DE CLUSTERING (K-means, k={n_clusters})")
    logger.info("="*80)

    # Preparar dados
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    X = df[numeric_cols].fillna(0)

    # Normalizar
    scaler = SklearnStandardScaler()
    X_scaled = scaler.fit_transform(X)

    # K-means
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(X_scaled)

    # Adicionar ao dataframe
    df_analysis = df.copy()
    df_analysis['cluster'] = clusters

    # Analisar clusters
    logger.info("\nCaracter√≠sticas dos Clusters:")
    for i in range(n_clusters):
        df_cluster = df_analysis[df_analysis['cluster'] == i]
        logger.info(f"\n  Cluster {i}: {len(df_cluster)} experimentos")

        if 'dataset' in df_cluster.columns:
            logger.info(f"    Datasets: {df_cluster['dataset'].value_counts().to_dict()}")

        if 'acuracia_teste' in df_cluster.columns:
            logger.info(f"    Acur√°cia m√©dia: {df_cluster['acuracia_teste'].mean():.3f}")

        if 'tempo_segundos' in df_cluster.columns:
            logger.info(f"    Tempo m√©dio: {df_cluster['tempo_segundos'].mean():.1f}s")

    # Visualiza√ß√£o em PCA space
    pca = SklearnPCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)

    df_viz = df_analysis.copy()
    df_viz['PC1'] = X_pca[:, 0]
    df_viz['PC2'] = X_pca[:, 1]

    fig = px.scatter(
        df_viz, x='PC1', y='PC2', color='cluster',
        title=f'Clustering K-means (k={n_clusters}) em Espa√ßo PCA',
        labels={'cluster': 'Cluster'},
        height=600, width=800
    )

    # Adicionar centroides
    centroides_pca = pca.transform(kmeans.cluster_centers_)
    fig.add_trace(
        go.Scatter(
            x=centroides_pca[:, 0],
            y=centroides_pca[:, 1],
            mode='markers',
            marker=dict(size=15, symbol='x', color='black'),
            name='Centroides'
        )
    )

    if save_path is not None:
        fig.write_html(save_path)
        logger.info(f"\n‚úì Clustering salvo: {save_path}")

    return kmeans


def analise_sensibilidade_profunda(df: pd.DataFrame, save_path: Optional[str] = None):
    """An√°lise de sensibilidade ao ru√≠do."""
    logger.info("\n" + "="*80)
    logger.info(" AN√ÅLISE DE SENSIBILIDADE AO RU√çDO")
    logger.info("="*80)

    # Calcular sensibilidade (derivada acur√°cia/ru√≠do)
    if 'nivel_ruido' in df.columns and 'acuracia_teste' in df.columns:
        df_ruido = df[df['nivel_ruido'] > 0].copy()

        # Sensibilidade m√©dia
        sensibilidade = df_ruido.groupby('nivel_ruido')['acuracia_teste'].mean().diff().mean()
        logger.info(f"\nSensibilidade m√©dia (Œîacur√°cia/Œîru√≠do): {sensibilidade:+.4f}")

        # Visualiza√ß√£o: Box plots por dataset
        fig = px.box(
            df_ruido, x='dataset', y='acuracia_teste', color='nivel_ruido',
            title='Sensibilidade da Acur√°cia ao N√≠vel de Ru√≠do',
            labels={'acuracia_teste': 'Acur√°cia', 'dataset': 'Dataset'},
            height=600, width=1000
        )

        if save_path is not None:
            fig.write_html(save_path)
            logger.info(f"‚úì An√°lise de sensibilidade salva: {save_path}")

        return {'sensibilidade_media': sensibilidade}

    logger.warning("Colunas necess√°rias n√£o encontradas")
    return {}


def executar_analises_profundas(df: pd.DataFrame, salvar_figuras: bool = True, pasta_resultados=None):
    """Executa an√°lises profundas do artigo."""
    import os

    deep_meta: dict[str, Any] = {
        'tipo': 'analises_profundas',
        'data_execucao': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'arquivos_gerados': [],
        'artefatos': []
    }
    metadata_path: Optional[str] = None
    if pasta_resultados is not None:
        os.makedirs(pasta_resultados, exist_ok=True)
        # Forense: README e metadata
        readme_path = os.path.join(pasta_resultados, 'README_analises_profundas.md')
        metadata_path = os.path.join(pasta_resultados, 'metadata_analises_profundas.json')
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(
                "# An√°lises Estat√≠sticas Profundas (v7.1)\n\n"
                f"- Data de execu√ß√£o: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                "- Conte√∫do: Correla√ß√£o, PCA, Clustering, Sensibilidade.\n"
            )
        # deep_meta j√° inicializado
    logger.info("\n" + "="*80)
    logger.info(" EXECUTANDO AN√ÅLISES ESTAT√çSTICAS PROFUNDAS (v7.1)")
    logger.info("="*80)

    resultados = {}

    # 1. Correla√ß√£o
    if salvar_figuras:
        path_corr = os.path.join(pasta_resultados if pasta_resultados else '', 'figura_correlacao.html')
        corr_matrix = analise_correlacao_profunda(df, save_path=path_corr)
        if pasta_resultados is not None:
            deep_meta['artefatos'].append(path_corr)
        resultados['correlacao'] = corr_matrix

    # 2. PCA
    if salvar_figuras and SKLEARN_ADVANCED_AVAILABLE:
        path_pca = os.path.join(pasta_resultados if pasta_resultados else '', 'figura_pca.html')
        pca, X_pca = analise_pca_profunda(df, save_path=path_pca)
        if pasta_resultados is not None:
            deep_meta['artefatos'].append(path_pca)
        resultados['pca'] = {'model': pca, 'components': X_pca}

    # 3. Clustering
    if salvar_figuras and SKLEARN_ADVANCED_AVAILABLE:
        path_clust = os.path.join(pasta_resultados if pasta_resultados else '', 'figura_clustering.html')
        kmeans = analise_clustering_profunda(df, n_clusters=3, save_path=path_clust)
        if pasta_resultados is not None:
            deep_meta['artefatos'].append(path_clust)
        resultados['clustering'] = kmeans

    # 4. Sensibilidade
    if salvar_figuras:
        path_sens = os.path.join(pasta_resultados if pasta_resultados else '', 'figura_sensibilidade.html')
        sens = analise_sensibilidade_profunda(df, save_path=path_sens)
        if pasta_resultados is not None:
            deep_meta['artefatos'].append(path_sens)
        resultados['sensibilidade'] = sens

    logger.info("\n" + "="*80)
    logger.info(" AN√ÅLISES PROFUNDAS CONCLU√çDAS!")
    logger.info("="*80)

    # Persistir metadata
    if pasta_resultados is not None and metadata_path is not None:
        try:
            deep_meta['arquivos_gerados'] = [f for f in os.listdir(pasta_resultados) if os.path.isfile(os.path.join(pasta_resultados, f))]
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(deep_meta, f, indent=2, ensure_ascii=False, default=str)
        except Exception:
            pass
    return resultados


# ============================================================================
# SCRIPT PRINCIPAL
# ============================================================================

# ===============================
# Utilit√°rio interno de valida√ß√£o
# ===============================
def validar_exportacao_circuito(pasta_resultados: Optional[str] = None) -> str:
    """
    Gera uma imagem PNG de um circuito qu√¢ntico simples usando o QNode persistido (qnode_)
    para validar a exporta√ß√£o de diagramas via qml.draw_mpl. Salva em pasta_resultados/circuitos
    quando fornecido; caso contr√°rio, salva em ./test_outputs.

    Returns:
        Caminho absoluto do arquivo PNG gerado.
    """
    import numpy as np
    import pennylane as qml
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    import os

    # Diret√≥rio de sa√≠da
    if pasta_resultados:
        out_dir = os.path.join(pasta_resultados, 'circuitos')
    else:
        out_dir = os.path.join(os.getcwd(), 'test_outputs')
    os.makedirs(out_dir, exist_ok=True)

    # Modelo m√≠nimo
    vqc = ClassificadorVQC(
        n_qubits=4,
        n_camadas=2,
        arquitetura='basic_entangler',  # usar chave v√°lida de ARQUITETURAS
        detectar_barren=False,
        n_epocas=1,
        seed=42,
    )
    vqc._criar_circuito()

    # Entrada dummy
    x = np.zeros(vqc.n_qubits)

    # Desenhar e salvar
    fig, ax = qml.draw_mpl(vqc.qnode_, decimals=2)(vqc.weights_, x, None)
    out_path = os.path.join(out_dir, 'circuito_validacao.png')
    plt.savefig(out_path, dpi=150, bbox_inches='tight', facecolor='white')
    try:
        from matplotlib.figure import Figure as _MplFigure
        import matplotlib.pyplot as _plt
        if isinstance(fig, _MplFigure):
            _plt.close(fig)  # type: ignore[arg-type]
    except Exception:
        pass

    return out_path


# ============================================================================
# OTIMIZA√á√ÉO BAYESIANA INTELIGENTE DE HIPERPAR√ÇMETROS
# ============================================================================
def otimizar_ruido_benefico_bayesiano(
    datasets: Dict[str, Any],
    n_trials: int = 50,
    n_epocas: int = 10,
    timeout: Optional[int] = None,
    pasta_resultados: Optional[str] = None,
    verbose: bool = True,
    dataset_nome: Optional[str] = 'moons'
) -> Dict[str, Any]:
    """
    Busca inteligente de hiperpar√¢metros de ru√≠do ben√©fico usando Otimiza√ß√£o Bayesiana.

    Vantagens sobre Grid Search:
    - 10-20x mais eficiente (explora espa√ßo de forma inteligente)
    - Pruning adaptativo (descarta configura√ß√µes ruins cedo)
    - An√°lise de import√¢ncia de hiperpar√¢metros
    - Warm start com configura√ß√µes promissoras

    Args:
        datasets: Dicion√°rio com datasets de treino/teste
        n_trials: N√∫mero de trials Optuna (padr√£o: 50, ~10x mais eficiente que 540 do grid)
        n_epocas: √âpocas por trial (padr√£o: 10)
        timeout: Timeout em segundos (None = sem limite)
        pasta_resultados: Pasta para salvar resultados
        verbose: Verbosidade

    Returns:
        Dict com melhores hiperpar√¢metros e hist√≥rico de otimiza√ß√£o

    Refer√™ncia:
        Akiba et al. (2019). "Optuna: A Next-generation Hyperparameter Optimization Framework"
    """
    if not OPTUNA_AVAILABLE:
        logger.warning("‚ö†Ô∏è Optuna n√£o dispon√≠vel. Instale com: pip install optuna")
        return {}

    # Type hints para Pylance (s√≥ executa se OPTUNA_AVAILABLE=True)
    import optuna  # type: ignore
    from optuna.samplers import TPESampler  # type: ignore
    from optuna.pruners import MedianPruner  # type: ignore

    if verbose:
        logger.info("\n" + "="*80)
        logger.info(" OTIMIZA√á√ÉO BAYESIANA DE RU√çDO BEN√âFICO")
        logger.info("="*80)
        logger.info(f"  Trials: {n_trials} (vs 540 do grid search)")
        logger.info(f"  √âpocas por trial: {n_epocas}")
        logger.info("  Algoritmo: Tree-structured Parzen Estimator (TPE)")
        logger.info("  Pruning: Median-based early stopping")

    # Escolher dataset representativo (moons √© desafiador e r√°pido)
    ds_nome = dataset_nome or 'moons'
    if ds_nome not in datasets:
        logger.warning(f"Dataset '{ds_nome}' n√£o encontrado. Usando 'moons'.")
        ds_nome = 'moons'
    dataset = datasets[ds_nome]

    def objective(trial):
        """Fun√ß√£o objetivo para Optuna."""
        # Sugerir hiperpar√¢metros
        arquitetura = trial.suggest_categorical('arquitetura', list(ARQUITETURAS.keys()))
        estrategia_init = trial.suggest_categorical('estrategia_init',
            ['matematico', 'quantico', 'aleatoria', 'fibonacci_spiral'])
        tipo_ruido = trial.suggest_categorical('tipo_ruido',
            ['sem_ruido', 'depolarizante', 'amplitude', 'phase', 'crosstalk'])

        # N√≠vel de ru√≠do: busca logar√≠tmica (mais eficiente para explorar ordens de magnitude)
        if tipo_ruido == 'sem_ruido':
            nivel_ruido = 0.0
        else:
            nivel_ruido = trial.suggest_float('nivel_ruido', 0.001, 0.02, log=True)

        # Otimizador e taxa de aprendizado
        taxa_aprendizado = trial.suggest_float('taxa_aprendizado', 0.001, 0.1, log=True)

        # Schedule de ru√≠do (se aplic√°vel)
        ruido_schedule = None
        if tipo_ruido != 'sem_ruido' and nivel_ruido > 0:
            ruido_schedule = trial.suggest_categorical('ruido_schedule',
                ['linear', 'exponencial', 'cosine', 'adaptativo'])

        try:
            # Treinar VQC
            vqc = ClassificadorVQC(
                n_qubits=4,
                n_camadas=2,
                arquitetura=arquitetura,
                estrategia_init=estrategia_init,
                tipo_ruido=tipo_ruido,
                nivel_ruido=nivel_ruido,
                taxa_aprendizado=taxa_aprendizado,
                n_epocas=n_epocas,
                batch_size=32,
                seed=42,
                ruido_schedule=ruido_schedule,
                ruido_inicial=nivel_ruido if ruido_schedule else None,
                ruido_final=0.001 if ruido_schedule else None,
                early_stopping=True,
                patience=3,  # Stopping mais agressivo para trials
                min_delta=1e-3,
                val_split=0.15  # Valida√ß√£o maior para pruning confi√°vel
            )

            vqc.fit(dataset['X_train'], dataset['y_train'])

            # M√©trica: acur√°cia no teste
            acuracia_teste = vqc.score(dataset['X_test'], dataset['y_test'])

            # Report intermedi√°rio para pruning
            trial.report(acuracia_teste, step=n_epocas)

            # Pruning: descarta trials ruins cedo
            if trial.should_prune():
                raise optuna.TrialPruned()

            return acuracia_teste

        except Exception as e:
            logger.warning(f"Trial {trial.number} falhou: {str(e)[:50]}")
            return 0.0  # Penalizar trials com erro

    # Criar estudo Optuna
    study = optuna.create_study(
        direction='maximize',  # Maximizar acur√°cia
        sampler=TPESampler(seed=42, n_startup_trials=10),  # 10 trials aleat√≥rios iniciais
        pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=3)  # Pruning ap√≥s 3 √©pocas
    )

    # Executar otimiza√ß√£o
    study.optimize(
        objective,
        n_trials=n_trials,
        timeout=timeout,
        show_progress_bar=True if verbose else False,
        n_jobs=1  # Sequencial (PennyLane n√£o √© thread-safe)
    )

    # Resultados
    melhor_trial = study.best_trial
    melhor_valor = melhor_trial.value if melhor_trial.value is not None else 0.0

    if verbose:
        logger.info("\n" + "="*80)
        logger.info(" RESULTADOS DA OTIMIZA√á√ÉO BAYESIANA")
        logger.info("="*80)
        logger.info(f"  ‚úì Melhor acur√°cia: {melhor_valor:.4f}")
        logger.info(f"  ‚úì Trial: {melhor_trial.number}/{n_trials}")
        logger.info(f"  ‚úì Trials completos: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}")
        logger.info(f"  ‚úì Trials podados: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}")
        logger.info("\n  Melhores hiperpar√¢metros:")
        for key, value in melhor_trial.params.items():
            logger.info(f"    - {key}: {value}")

    # An√°lise de import√¢ncia
    try:
        importancias = optuna.importance.get_param_importances(study)
        if verbose:
            logger.info("\n  Import√¢ncia dos hiperpar√¢metros:")
            for param, imp in sorted(importancias.items(), key=lambda x: x[1], reverse=True):
                logger.info(f"    - {param}: {imp:.3f}")
    except Exception:
        importancias = {}

    # Salvar resultados
    resultado = {
        'melhor_acuracia': float(melhor_valor),
        'melhor_params': melhor_trial.params,
        'n_trials': n_trials,
        'trials_completos': len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]),
        'trials_podados': len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]),
        'importancias': importancias,
        'historico': [
            {
                'trial': t.number,
                'acuracia': float(t.value) if t.value is not None else 0.0,
                'params': t.params,
                'estado': str(t.state)
            }
            for t in study.trials
        ]
    }

    if pasta_resultados is not None:
        # Salvar JSON
        optuna_dir = os.path.join(pasta_resultados, 'otimizacao_bayesiana')
        os.makedirs(optuna_dir, exist_ok=True)

        resultado_path = os.path.join(optuna_dir, 'resultado_otimizacao.json')
        with open(resultado_path, 'w', encoding='utf-8') as f:
            json.dump(resultado, f, indent=2, ensure_ascii=False)

        # Salvar CSV do hist√≥rico
        historico_df = pd.DataFrame(resultado['historico'])
        historico_path = os.path.join(optuna_dir, 'historico_trials.csv')
        historico_df.to_csv(historico_path, index=False)

        # README
        readme_path = os.path.join(optuna_dir, 'README_otimizacao.md')
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(
                "# Otimiza√ß√£o Bayesiana de Ru√≠do Ben√©fico\n\n"
                f"## Configura√ß√£o\n"
                f"- Trials: {n_trials}\n"
                f"- √âpocas por trial: {n_epocas}\n"
                f"- Dataset: {ds_nome}\n"
                f"- Algoritmo: Tree-structured Parzen Estimator (TPE)\n\n"
                f"## Resultados\n"
                f"- **Melhor acur√°cia:** {melhor_valor:.4f}\n"
                f"- **Trials completos:** {resultado['trials_completos']}\n"
                f"- **Trials podados:** {resultado['trials_podados']}\n\n"
                "## Melhores Hiperpar√¢metros\n"
            )
            for key, value in melhor_trial.params.items():
                f.write(f"- `{key}`: {value}\n")

            if importancias:
                f.write("\n## Import√¢ncia dos Hiperpar√¢metros\n")
                for param, imp in sorted(importancias.items(), key=lambda x: x[1], reverse=True):
                    f.write(f"- `{param}`: {imp:.3f}\n")

            f.write(
                "\n## Arquivos Gerados\n"
                "- `resultado_otimizacao.json`: Resultado completo da otimiza√ß√£o\n"
                "- `historico_trials.csv`: Hist√≥rico de todos os trials\n"
            )

        if verbose:
            logger.info(f"\n  ‚úì Resultados salvos em: {optuna_dir}")

    return resultado


def main():
    """Execu√ß√£o principal do framework investigativo."""
    print("="*80)
    print(" FRAMEWORK INVESTIGATIVO COMPLETO v7.2 - ARTIGO NATURE/QUANTUM")
    print(" Beneficial Quantum Noise in Variational Quantum Classifiers")
    print(" + CONSOLIDA√á√ÉO E ORQUESTRA√á√ÉO AUTOM√ÅTICA INTEGRADA")
    print("="*80)

    # Criar pasta de resultados com timestamp (com suporte a Colab/Drive e vari√°vel de ambiente)
    pasta_resultados = _preparar_diretorio_resultados(_parse_resultados_base_from_args() or os.environ.get('RESULTS_BASE_DIR'))
    # Forense raiz: README e metadata
    raiz_readme = os.path.join(pasta_resultados, 'README.md')
    raiz_meta_path = os.path.join(pasta_resultados, 'metadata.json')
    with open(raiz_readme, 'w', encoding='utf-8') as f:
        f.write(
            "# Framework Investigativo Completo v7.1\n\n"
            f"- Data de execu√ß√£o: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            "- Este diret√≥rio cont√©m todos os resultados do experimento (grid search, an√°lises, figuras).\n"
        )
    raiz_meta = {
        'tipo': 'raiz_experimento',
        'data_execucao': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'arquivos_gerados': []
    }

    # 1. Carregar datasets
    print("\n[1/5] Carregando datasets...")
    datasets = carregar_datasets(seed=42)
    print(f"  ‚úì {len(datasets)} datasets carregados")
    for nome, data in datasets.items():
        print(f"    - {nome}: {len(data['y_train'])} treino, {len(data['y_test'])} teste")

    # 2. Executar grid search ou otimiza√ß√£o Bayesiana
    print("\n[2/5] Executando busca de hiperpar√¢metros...")
    n_epocas_padrao = 15
    # Garantir defini√ß√£o de df_resultados para Pylance e seguran√ßa de fluxo
    df_resultados: pd.DataFrame = pd.DataFrame()
    try:
        modo_rapido = os.environ.get('VQC_QUICK', '0') == '1'
        modo_bayesiano = os.environ.get('VQC_BAYESIAN', '0') == '1'
        executar_bayes_apos_grid = os.environ.get('VQC_BAYES_AFTER_GRID', '0') == '1'
    except Exception:
        modo_rapido = False
        modo_bayesiano = False
        executar_bayes_apos_grid = False

    # Flags de CLI (opcionais)
    import sys as _sys_cli
    _argv = _sys_cli.argv[1:]
    if '--bayes' in _argv:
        modo_bayesiano = True
    if '--bayes-after-grid' in _argv or '--run-both' in _argv:
        executar_bayes_apos_grid = True
    # Overrides de par√¢metros Bayesianos
    _cli_n_trials = None
    _cli_epocas_bayes = None
    _cli_dataset_bayes = None
    if '--trials' in _argv:
        try:
            i = _argv.index('--trials')
            _cli_n_trials = int(_argv[i+1])
        except Exception:
            pass
    if '--epocas-bayes' in _argv:
        try:
            i = _argv.index('--epocas-bayes')
            _cli_epocas_bayes = int(_argv[i+1])
        except Exception:
            pass
    if '--dataset-bayes' in _argv:
        try:
            i = _argv.index('--dataset-bayes')
            _cli_dataset_bayes = _argv[i+1]
        except Exception:
            pass

    if modo_rapido:
        print("  ‚ö° Modo r√°pido ativado (VQC_QUICK=1): n_epocas=5")

    # OTIMIZA√á√ÉO BAYESIANA (novo m√©todo inteligente)
    resultado_bayesiano: Optional[Dict[str, Any]] = None
    if modo_bayesiano and not executar_bayes_apos_grid:
        if not OPTUNA_AVAILABLE:
            print("  ‚ö†Ô∏è Optuna n√£o dispon√≠vel. Instale com: pip install optuna")
            print("  Continuando com grid search tradicional...")
        else:
            print("  üß† Modo Bayesiano ativado (VQC_BAYESIAN=1)")
            print("     Usando Otimiza√ß√£o Bayesiana (10-20x mais eficiente)")
            # 100 trials √© ~5x mais r√°pido que 540 do grid (cap = 200)
            n_trials = _cli_n_trials if _cli_n_trials is not None else (100 if modo_rapido else 200)
            n_trials = min(200, int(n_trials))
            # √âpocas m√°ximas por trial; efetivo ser√° determinado por Early Stopping
            n_epocas_bayes = _cli_epocas_bayes if _cli_epocas_bayes is not None else (5 if modo_rapido else n_epocas_padrao)
            ds_bayes = (_cli_dataset_bayes if _cli_dataset_bayes is not None else 'moons')

            if ds_bayes.lower() == 'all':
                print("  ‚ñ∂Ô∏è Executando Bayesiano para TODOS os datasets: moons, circles, iris, breast_cancer, wine (cap 200 trials cada)")
                resultado_bayesiano = {}
                for _ds in ['moons', 'circles', 'iris', 'breast_cancer', 'wine']:
                    res = otimizar_ruido_benefico_bayesiano(
                        datasets=datasets,
                        n_trials=n_trials,
                        n_epocas=n_epocas_bayes,
                        timeout=None,
                        pasta_resultados=pasta_resultados,
                        verbose=True,
                        dataset_nome=_ds
                    )
                    resultado_bayesiano[_ds] = res
            else:
                resultado_bayesiano = otimizar_ruido_benefico_bayesiano(
                    datasets=datasets,
                    n_trials=n_trials,
                    n_epocas=n_epocas_bayes,
                    timeout=None,
                    pasta_resultados=pasta_resultados,
                    verbose=True,
                    dataset_nome=ds_bayes
                )

            # Salvar/relatar resultado especial Bayesiano
            if resultado_bayesiano:
                print("\n  ‚úÖ Otimiza√ß√£o Bayesiana conclu√≠da!")
                # Pode ser um √∫nico dict de resultados ou um dict por dataset
                if isinstance(resultado_bayesiano, dict) and 'melhor_acuracia' not in resultado_bayesiano:
                    # Agregar por dataset
                    melhor_ds = None
                    melhor_acc = -1.0
                    for _ds, _res in resultado_bayesiano.items():
                        try:
                            acc = float(_res.get('melhor_acuracia', 0.0))
                            print(f"     - {_ds}: melhor_acuracia={acc:.4f} | trials={_res.get('trials_completos', 0)}/{_res.get('n_trials', n_trials)}")
                            if acc > melhor_acc:
                                melhor_acc = acc
                                melhor_ds = _ds
                        except Exception:
                            pass
                    if melhor_ds is not None:
                        print(f"     Melhor geral: {melhor_ds} com acur√°cia {melhor_acc:.4f}")
                    print("     Configura√ß√µes √≥timas salvas em: otimizacao_bayesiana/")
                else:
                    # Resultado √∫nico
                    print(f"     Melhor acur√°cia: {resultado_bayesiano['melhor_acuracia']:.4f}")
                    print(f"     Trials completos: {resultado_bayesiano['trials_completos']}/{n_trials}")
                    print("     Configura√ß√£o √≥tima salva em: otimizacao_bayesiana/")

    # GRID SEARCH TRADICIONAL (m√©todo original)
    if (not modo_bayesiano or not OPTUNA_AVAILABLE) and not executar_bayes_apos_grid:
        df_resultados = executar_grid_search(datasets, n_epocas=(5 if modo_rapido else n_epocas_padrao), verbose=True, pasta_resultados=pasta_resultados)
        # Rastreio fino do n√≠vel de ru√≠do ap√≥s grid search
        rastreio_fino_nivel_ruido(df_resultados, datasets, pasta_resultados, n_epocas=(5 if modo_rapido else n_epocas_padrao), verbose=True)

        # Salvar resultados brutos
        csv_path = os.path.join(pasta_resultados, 'resultados_completos_artigo.csv')
        df_resultados.to_csv(csv_path, index=False)
        print(f"\n  ‚úì Resultados salvos: {csv_path}")
    elif not executar_bayes_apos_grid:
        # Criar DataFrame m√≠nimo para compatibilidade com an√°lises
        if resultado_bayesiano is not None:
            # Pode ser um resultado √∫nico (um dataset) ou um dict por dataset
            if isinstance(resultado_bayesiano, dict) and 'melhor_acuracia' not in resultado_bayesiano:
                linhas = []
                for _ds, _res in resultado_bayesiano.items():
                    try:
                        _params = _res.get('melhor_params', {})
                        linhas.append({
                            'dataset': _ds,
                            'arquitetura': _params.get('arquitetura', 'desconhecida'),
                            'estrategia_init': _params.get('estrategia_init', 'desconhecida'),
                            'tipo_ruido': _params.get('tipo_ruido', 'sem_ruido'),
                            'nivel_ruido': _params.get('nivel_ruido', 0.0),
                            'acuracia_teste': _res.get('melhor_acuracia', 0.0),
                            'seed': 42
                        })
                    except Exception:
                        pass
                if len(linhas) == 0:
                    # Fallback de seguran√ßa caso algo d√™ errado
                    linhas = [{
                        'dataset': 'moons',
                        'arquitetura': 'basico',
                        'estrategia_init': 'matematico',
                        'tipo_ruido': 'sem_ruido',
                        'nivel_ruido': 0.0,
                        'acuracia_teste': 0.5,
                        'seed': 42
                    }]
                df_resultados = pd.DataFrame(linhas)
                print("\n  ‚ÑπÔ∏è Usando configura√ß√µes √≥timas Bayesianas (por dataset) para an√°lises subsequentes")
            else:
                melhor_params = resultado_bayesiano['melhor_params']
                df_resultados = pd.DataFrame([{
                    'dataset': 'moons',
                    'arquitetura': melhor_params['arquitetura'],
                    'estrategia_init': melhor_params['estrategia_init'],
                    'tipo_ruido': melhor_params['tipo_ruido'],
                    'nivel_ruido': melhor_params.get('nivel_ruido', 0.0),
                    'acuracia_teste': resultado_bayesiano['melhor_acuracia'],
                    'seed': 42
                }])
                print("\n  ‚ÑπÔ∏è Usando configura√ß√£o √≥tima Bayesiana para an√°lises subsequentes")
        else:
            # Fallback: usar configura√ß√£o padr√£o
            df_resultados = pd.DataFrame([{
                'dataset': 'moons',
                'arquitetura': 'basico',
                'estrategia_init': 'matematico',
                'tipo_ruido': 'sem_ruido',
                'nivel_ruido': 0.0,
                'acuracia_teste': 0.5,
                'seed': 42
            }])

    # Execu√ß√£o combinada: Grid Search seguido de Bayesiano
    if executar_bayes_apos_grid:
        # Primeiro executa o grid search completo
        df_resultados = executar_grid_search(datasets, n_epocas=(5 if modo_rapido else n_epocas_padrao), verbose=True, pasta_resultados=pasta_resultados)
        rastreio_fino_nivel_ruido(df_resultados, datasets, pasta_resultados, n_epocas=(5 if modo_rapido else n_epocas_padrao), verbose=True)
        csv_path = os.path.join(pasta_resultados, 'resultados_completos_artigo.csv')
        df_resultados.to_csv(csv_path, index=False)
        print(f"\n  ‚úì Resultados do grid salvos: {csv_path}")

        # Em seguida, executa a otimiza√ß√£o Bayesiana para refino
        if OPTUNA_AVAILABLE:
            print("\n  ‚ñ∂Ô∏è Iniciando Otimiza√ß√£o Bayesiana p√≥s-grid...")
            n_trials = _cli_n_trials if _cli_n_trials is not None else (60 if modo_rapido else 120)
            n_trials = min(200, int(n_trials))
            n_epocas_bayes = _cli_epocas_bayes if _cli_epocas_bayes is not None else (5 if modo_rapido else n_epocas_padrao)
            ds_bayes = _cli_dataset_bayes if _cli_dataset_bayes is not None else 'moons'
            if ds_bayes.lower() == 'all':
                for _ds in ['moons', 'circles', 'iris', 'breast_cancer', 'wine']:
                    _ = otimizar_ruido_benefico_bayesiano(
                        datasets=datasets,
                        n_trials=n_trials,
                        n_epocas=n_epocas_bayes,
                        timeout=None,
                        pasta_resultados=pasta_resultados,
                        verbose=True,
                        dataset_nome=_ds
                    )
            else:
                _ = otimizar_ruido_benefico_bayesiano(
                    datasets=datasets,
                    n_trials=n_trials,
                    n_epocas=n_epocas_bayes,
                    timeout=None,
                    pasta_resultados=pasta_resultados,
                    verbose=True,
                    dataset_nome=ds_bayes
                )
            print("  ‚úì Otimiza√ß√£o Bayesiana p√≥s-grid finalizada.")
        else:
            print("  ‚ö†Ô∏è Optuna n√£o dispon√≠vel; etapa Bayesiana p√≥s-grid foi pulada.")

    # Garantia final: se por algum motivo df_resultados n√£o foi definido, usar um fallback m√≠nimo
    if df_resultados is None or df_resultados.empty:
        df_resultados = pd.DataFrame([{
            'dataset': 'moons',
            'arquitetura': 'basico',
            'estrategia_init': 'matematico',
            'tipo_ruido': 'sem_ruido',
            'nivel_ruido': 0.0,
            'acuracia_teste': 0.5,
            'seed': 42
        }])

    # 3. An√°lises estat√≠sticas
    print("\n[3/6] Executando an√°lises estat√≠sticas...")
    analises = executar_analises_estatisticas(df_resultados, verbose=True, pasta_resultados=pasta_resultados)

    # 4. Gerar visualiza√ß√µes
    print("\n[4/6] Gerando visualiza√ß√µes...")
    gerar_visualizacoes(df_resultados, salvar=True, pasta_resultados=pasta_resultados)

    # 5. An√°lises profundas (v7.1)
    print("\n[5/6] Executando an√°lises profundas (v7.1)...")
    executar_analises_profundas(df_resultados, salvar_figuras=True, pasta_resultados=pasta_resultados)

    # 6. Resumo final
    print("\n[6/6] Resumo Final")
    print("="*80)

    print("\nüìä ESTAT√çSTICAS GERAIS:")
    print(f"  Total de experimentos: {len(df_resultados)}")
    print(f"  Datasets testados: {df_resultados['dataset'].nunique()}")
    print(f"  Configura√ß√µes por dataset: {len(df_resultados) // df_resultados['dataset'].nunique()}")

    print("\nüèÜ MELHOR CONFIGURA√á√ÉO GERAL:")
    idx_melhor = df_resultados['acuracia_teste'].idxmax()
    melhor = df_resultados.loc[idx_melhor]
    print(f"  Dataset: {melhor['dataset']}")
    print(f"  Arquitetura: {melhor['arquitetura']}")
    print(f"  Inicializa√ß√£o: {melhor['estrategia_init']}")
    print(f"  Ru√≠do: {melhor['tipo_ruido']} (n√≠vel={melhor['nivel_ruido']:.3f})")
    print(f"  Acur√°cia: {melhor['acuracia_teste']:.4f}")

    print("\nüåÄ RU√çDOS BEN√âFICOS:")
    baseline = df_resultados[df_resultados['tipo_ruido'] == 'sem_ruido']['acuracia_teste'].mean()
    for ruido in ['depolarizante', 'amplitude_damping', 'phase_damping']:
        df_ruido = df_resultados[(df_resultados['tipo_ruido'] == ruido) & (df_resultados['nivel_ruido'] > 0)]
        if len(df_ruido) > 0:
            media = df_ruido['acuracia_teste'].mean()
            delta = media - baseline
            status = "‚úì BEN√âFICO" if delta > 0 else "‚úó Prejudicial"
            print(f"  {ruido:20s}: {media:.4f} (Œî={delta:+.4f}) {status}")

    print("\nüìè EFFECT SIZES:")
    if 'effect_sizes' in analises:
        print(f"  Cohen's d: {analises['effect_sizes']['cohen_d']:.4f}")
        print(f"  Glass's Œî: {analises['effect_sizes']['glass_delta']:.4f}")
        print(f"  Hedges' g: {analises['effect_sizes']['hedges_g']:.4f}")

    print("\nüìÅ ARQUIVOS GERADOS:")
    print("  - resultados_completos_artigo.csv")
    print("  - comparacao_baselines.csv (NOVO)")
    print("  - figura2_beneficial_noise.html")
    print("  - figura2b_beneficial_noise_ic95.html (NOVO)")
    print("  - figura3_noise_types.html")
    print("  - figura3b_noise_types_ic95.html (NOVO)")
    print("  - figura4_initialization.html")
    print("  - figura5_architecture_tradeoffs.html")
    print("  - figura6_effect_sizes.html")
    print("  - figura7_overfitting.html")

    print("\nüöÄ FUNCIONALIDADES AVAN√áADAS ATIVADAS:")
    print("  ‚úì 4 schedules de ru√≠do (linear, exp, cosine, adaptativo)")
    print("  ‚úì Detec√ß√£o de Barren Plateaus")
    print("  ‚úì Monitoramento de emaranhamento")
    print("  ‚úì 3 otimizadores (Adam, SGD, QNG)")
    print("  ‚úì 3 fun√ß√µes de custo (MSE, Cross-Entropy, Hinge)")
    print("  ‚úì Effect sizes: Cohen's d, Glass's Œî, Hedges' g")
    print("  ‚úì Post-hoc: Bonferroni, Scheff√©")

    print("\n‚ú® NOVAS FUNCIONALIDADES (v7.1):")
    print("  ‚úì Autotuning com Optuna (otimiza√ß√£o bayesiana)")
    print("  ‚úì Modelagem de ru√≠do via Lindblad (5 canais)")
    print("  ‚úì PCA (An√°lise de Componentes Principais)")
    print("  ‚úì Clustering K-means")
    print("  ‚úì An√°lise de correla√ß√£o")
    print("  ‚úì An√°lise de sensibilidade")
    print("  ‚úì 9 visualiza√ß√µes interativas totais")

    print("\nü§ñ AUTOMA√á√ÉO INTEGRADA (v7.2):")
    print("  ‚úì Consolida√ß√£o autom√°tica de CSVs individuais")
    print("  ‚úì Gera√ß√£o autom√°tica de comparacao_baselines.csv")
    print("  ‚úì Gera√ß√£o autom√°tica de metadata_orchestrator.json")
    print("  ‚úì Invent√°rio completo de arquivos e estat√≠sticas")

    print("\n" + "="*80)
    print(" ‚úì FRAMEWORK INVESTIGATIVO COMPLETO v7.2 EXECUTADO COM SUCESSO!")
    print("="*80)

    # 7. AUTOMA√á√ÉO: Consolida√ß√£o e Metadados (v7.2)
    print("\n[7/7] Consolida√ß√£o autom√°tica e metadados...")
    consolidar_e_gerar_metadados(pasta_resultados, verbose=True)

    # Atualizar metadata raiz com a lista final de arquivos
    try:
        raiz_meta['arquivos_gerados'] = [f for f in os.listdir(pasta_resultados) if os.path.isfile(os.path.join(pasta_resultados, f))]
        with open(raiz_meta_path, 'w', encoding='utf-8') as f:
            json.dump(raiz_meta, f, indent=2, ensure_ascii=False, default=str)
    except Exception:
        pass


# =============================================================================
# CONSOLIDA√á√ÉO E AUTOMA√á√ÉO INTEGRADA (v7.2)
# =============================================================================

def consolidar_resultados_individuais(pasta_resultados: str, verbose: bool = True) -> Dict[str, Any]:
    """Consolida todos os CSVs individuais em um √∫nico arquivo consolidado.

    Par√¢metros:
    - pasta_resultados: caminho absoluto para a pasta de resultados (contendo 'experimentos_individuais/').
    - verbose: se True, imprime logs de progresso.

    Retorna:
    - dict com sum√°rio: caminhos gerados, contagens, colunas.
    """
    pasta_resultados = os.path.abspath(pasta_resultados)
    individual_dir = os.path.join(pasta_resultados, 'experimentos_individuais')

    if not os.path.isdir(individual_dir):
        msg = f"Pasta 'experimentos_individuais' n√£o encontrada em: {pasta_resultados}"
        if verbose:
            print(f"  ‚ö†Ô∏è {msg}")
        return {'status': 'error', 'message': msg}

    csv_files = sorted([os.path.join(individual_dir, f) for f in os.listdir(individual_dir) if f.endswith('.csv')])
    if len(csv_files) == 0:
        msg = f"Nenhum CSV individual encontrado em {individual_dir}"
        if verbose:
            print(f"  ‚ÑπÔ∏è {msg}")
        return {'status': 'skip', 'message': msg}

    if verbose:
        print(f"  üì¶ Encontrados {len(csv_files)} CSVs individuais. Consolidando...")

    dfs = []
    for p in csv_files:
        try:
            df = pd.read_csv(p)
            dfs.append(df)
        except Exception as e:
            if verbose:
                print(f"    ‚ö†Ô∏è Falha ao ler {os.path.basename(p)}: {str(e)[:100]}")

    if len(dfs) == 0:
        msg = 'Nenhum CSV lido com sucesso.'
        if verbose:
            print(f"  ‚ö†Ô∏è {msg}")
        return {'status': 'error', 'message': msg}

    df_all = pd.concat(dfs, ignore_index=True)
    consolidated_path = os.path.join(pasta_resultados, 'resultados_completos_artigo.csv')
    df_all.to_csv(consolidated_path, index=False)

    if verbose:
        print("  ‚úÖ CSV consolidado salvo: resultados_completos_artigo.csv")
        print(f"     Linhas: {len(df_all)} | Colunas: {len(df_all.columns)}")

    return {
        'status': 'ok',
        'num_csvs_individuais': len(csv_files),
        'consolidated_path': consolidated_path,
        'rows_consolidated': len(df_all),
        'columns': list(df_all.columns),
        'df': df_all
    }


def gerar_comparacao_baselines(df_all: pd.DataFrame, pasta_resultados: str, verbose: bool = True) -> Optional[str]:
    """Gera arquivo comparacao_baselines.csv comparando VQC vs. SVM/RF por dataset.

    Retorna:
    - caminho do arquivo gerado, ou None se n√£o foi poss√≠vel gerar.
    """
    required = {'dataset', 'arquitetura', 'tipo_ruido', 'nivel_ruido', 'acuracia_teste'}
    if not required.issubset(df_all.columns):
        if verbose:
            print("  ‚ÑπÔ∏è Colunas necess√°rias para compara√ß√£o de baselines n√£o encontradas.")
        return None

    try:
        vqc_melhor = df_all[df_all['tipo_ruido'] != 'classico'].groupby('dataset')['acuracia_teste'].max()
        vqc_sem_ruido = df_all[(df_all['tipo_ruido'].isin(['sem_ruido', 'none'])) | (df_all['nivel_ruido'] == 0.0)].groupby('dataset')['acuracia_teste'].mean()
        svm = df_all[df_all['arquitetura'].str.lower().str.contains('svm', na=False)].groupby('dataset')['acuracia_teste'].mean()
        rf = df_all[df_all['arquitetura'].str.lower().str.contains('randomforest|random_forest|rf', na=False)].groupby('dataset')['acuracia_teste'].mean()

        comparacao = pd.DataFrame({
            'dataset': vqc_melhor.index,
            'vqc_melhor': vqc_melhor.values,
            'vqc_sem_ruido_media': vqc_sem_ruido.reindex(vqc_melhor.index).values,
            'svm': svm.reindex(vqc_melhor.index).values,
            'rf': rf.reindex(vqc_melhor.index).values
        })
        comparacao['delta_vqc_svm'] = comparacao['vqc_melhor'] - comparacao['svm']
        comparacao['delta_vqc_rf'] = comparacao['vqc_melhor'] - comparacao['rf']

        comp_path = os.path.join(pasta_resultados, 'comparacao_baselines.csv')
        comparacao.to_csv(comp_path, index=False)

        if verbose:
            print("  ‚úÖ Compara√ß√£o de baselines salva: comparacao_baselines.csv")

        return comp_path
    except Exception as e:
        if verbose:
            print(f"  ‚ö†Ô∏è Falha ao gerar comparacao_baselines.csv: {str(e)[:150]}")
        return None


def gerar_metadata_orchestrator(pasta_resultados: str, consolidacao_info: Dict[str, Any], verbose: bool = True) -> str:
    """Gera metadata_orchestrator.json com invent√°rio de arquivos e estat√≠sticas.

    Retorna:
    - caminho do arquivo de metadados gerado.
    """
    pasta_resultados = os.path.abspath(pasta_resultados)

    # Listar arquivos na raiz do diret√≥rio de resultados
    try:
        arquivos = sorted([f for f in os.listdir(pasta_resultados) if os.path.isfile(os.path.join(pasta_resultados, f))])
    except Exception:
        arquivos = []

    # Listar subpastas
    try:
        subpastas = sorted([d for d in os.listdir(pasta_resultados) if os.path.isdir(os.path.join(pasta_resultados, d))])
    except Exception:
        subpastas = []

    metadata = {
        'tipo': 'metadata_orchestrator',
        'versao_framework': '7.2',
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'pasta_resultados': pasta_resultados,
        'arquivos_raiz': arquivos,
        'subpastas': subpastas,
        'consolidacao': {
            'status': consolidacao_info.get('status', 'unknown'),
            'num_csvs_individuais': consolidacao_info.get('num_csvs_individuais', 0),
            'rows_consolidated': consolidacao_info.get('rows_consolidated', 0),
            'columns': consolidacao_info.get('columns', [])
        }
    }

    meta_path = os.path.join(pasta_resultados, 'metadata_orchestrator.json')
    with open(meta_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    if verbose:
        print("  ‚úÖ Metadados salvos: metadata_orchestrator.json")

    return meta_path


def consolidar_e_gerar_metadados(pasta_resultados: str, verbose: bool = True) -> Dict[str, Any]:
    """Fun√ß√£o unificada que executa consolida√ß√£o e gera√ß√£o de metadados.

    Retorna:
    - dict com sum√°rio completo das opera√ß√µes.
    """
    if verbose:
        print("  üîÑ Iniciando consolida√ß√£o autom√°tica...")

    # 1. Consolidar CSVs individuais
    consolidacao_info = consolidar_resultados_individuais(pasta_resultados, verbose=verbose)

    # 2. Gerar compara√ß√£o de baselines (se consolida√ß√£o foi bem-sucedida)
    comp_path = None
    if consolidacao_info.get('status') == 'ok' and 'df' in consolidacao_info:
        comp_path = gerar_comparacao_baselines(consolidacao_info['df'], pasta_resultados, verbose=verbose)

    # 3. Gerar metadata orchestrator
    meta_path = gerar_metadata_orchestrator(pasta_resultados, consolidacao_info, verbose=verbose)

    result = {
        'consolidacao': consolidacao_info,
        'comparacao_baselines': comp_path,
        'metadata_orchestrator': meta_path
    }

    if verbose:
        print("  ‚úÖ Consolida√ß√£o e metadados conclu√≠dos!\n")

    return result


if __name__ == "__main__":
    # Suporte a execu√ß√£o especializada por vari√°vel de ambiente/argumento:
    # - VQC_VALIDATE_CIRCUIT=1: executa a valida√ß√£o de exporta√ß√£o de circuito
    # - VQC_ONLY_VALIDATE=1: executa somente a valida√ß√£o e encerra
    # - Argumentos equivalentes: --validar-circuito, --only-validate
    import sys as _sys
    _args = set(_sys.argv[1:])
    _env_validate = os.environ.get('VQC_VALIDATE_CIRCUIT', '0') == '1'
    _env_only = os.environ.get('VQC_ONLY_VALIDATE', '0') == '1'
    _flag_validate = ('--validar-circuito' in _args)
    _flag_only = ('--only-validate' in _args)

    if _env_validate or _flag_validate or _env_only or _flag_only:
        # Preparar pasta de resultados (coerente com execu√ß√£o principal)
        now = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
        pasta_resultados = f"resultados_{now}"
        os.makedirs(pasta_resultados, exist_ok=True)
        # Validar exporta√ß√£o
        png_path = validar_exportacao_circuito(pasta_resultados)
        print(f"‚úì Valida√ß√£o de circuito conclu√≠da: {png_path}")
        if _env_only or _flag_only:
            _sys.exit(0)

    # Execu√ß√£o normal
    main()

