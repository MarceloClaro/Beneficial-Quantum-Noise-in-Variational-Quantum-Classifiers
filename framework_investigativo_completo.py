# =============================================================================
# RASTREIO FINO DO NÍVEL DE RUÍDO
# =============================================================================
# Imports centralizados no topo do arquivo (PEP 8)
import os
import json
import time
import logging
from pathlib import Path
from datetime import datetime
from typing import TYPE_CHECKING, Any, Dict, Optional

import numpy as np
import pandas as pd

from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn import datasets as sk_datasets
from sklearn.metrics import confusion_matrix
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

# Estatística
from scipy.stats import f_oneway, ttest_ind
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm

# Visualização
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px

# Inicializar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ===============================
# Utilidades de diretório de resultados (Drive/Colab/local)
# ===============================
def _parse_resultados_base_from_args() -> Optional[str]:
    """Lê opcionalmente um argumento CLI --resultados <dir> para base dos resultados.
    Ex.: python framework_investigativo_completo.py --resultados "/content/drive/MyDrive/VQC_results"
    """
    try:
        import sys
        if '--resultados' in sys.argv:
            idx = sys.argv.index('--resultados')
            if idx + 1 < len(sys.argv):
                return sys.argv[idx + 1]
    except Exception:
        pass
    return None


def _detectar_colab_drive_montado() -> Optional[str]:
    """Detecta se está rodando no Google Colab com Drive montado e retorna caminho base padrão.
    Retorna None se não detectar Colab/Drive.
    """
    try:
        # Heurística simples: diretório padrão do Drive no Colab
        drive_path = '/content/drive/MyDrive'
        return drive_path if os.path.isdir(drive_path) else None
    except Exception:
        return None


def _preparar_diretorio_resultados(base_dir: Optional[str] = None) -> str:
    """Cria e retorna o caminho da pasta de resultados com timestamp, respeitando:
    - argumento CLI --resultados <dir>
    - variável de ambiente RESULTS_BASE_DIR
    - ambiente Colab com Drive montado (/content/drive/MyDrive)
    - fallback: diretório local do projeto

    Também cria subpastas padrão, se necessário.
    """
    # Prioridades: CLI > ENV > Colab > local
    base = (
        base_dir
        or os.environ.get('RESULTS_BASE_DIR')
        or _detectar_colab_drive_montado()
        or os.getcwd()
    )

    now = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    pasta_resultados = os.path.join(base, f'resultados_{now}')
    try:
        os.makedirs(pasta_resultados, exist_ok=True)
        # Pré-criar subpastas comuns para organização
        for sub in ('circuitos', 'barren_plateaus', 'experimentos_individuais'):
            os.makedirs(os.path.join(pasta_resultados, sub), exist_ok=True)
    except Exception:
        # Fallback absoluto: local
        pasta_resultados = f'resultados_{now}'
        os.makedirs(pasta_resultados, exist_ok=True)
    return pasta_resultados

def rastreio_fino_nivel_ruido(df_resultados, datasets, pasta_resultados, n_epocas=15, verbose=True, passo_fino=0.001, n_passos=5):
    """
    Rastreio fino do nível ótimo de ruído.
    
    Após o grid search identificar uma região promissora, realiza busca refinada
    em torno do nível ótimo com passos menores para caracterização precisa.
    
    Args:
        df_resultados: DataFrame com resultados do grid search
        datasets: Dict com datasets de benchmark
        pasta_resultados: Pasta para salvar resultados
        n_epocas: Número de épocas para treinamento
        verbose: Se True, imprime progresso
        passo_fino: Tamanho do passo para busca refinada (padrão: 0.001)
        n_passos: Número de passos em cada direção (padrão: 5)
    
    Returns:
        List com resultados do rastreio fino
    
    Referência:
        Esta técnica é inspirada em métodos de otimização de hiperparâmetros
        como Successive Halving e Grid Search Refinement.
    """
    # json, os, Path já disponíveis no escopo global
    
    subdir_root = Path(pasta_resultados) / 'rastreio_fino'
    os.makedirs(subdir_root, exist_ok=True)
    
    if verbose:
        logger.info("\n" + "="*80)
        logger.info(" RASTREIO FINO DO NÍVEL ÓTIMO DE RUÍDO")
        logger.info("="*80)
    
    # Identificar configuração ótima do grid search
    df_com_ruido = df_resultados[df_resultados['nivel_ruido'] > 0]
    
    if len(df_com_ruido) == 0:
        logger.warning("Nenhum resultado com ruído encontrado. Pulando rastreio fino.")
        (subdir_root / 'README_rastreio_fino.md').write_text(
            '# Rastreio Fino - Pulado\n\nNenhum resultado com ruído encontrado no grid search.',
            encoding='utf-8'
        )
        return []
    
    # Encontrar melhor configuração
    idx_melhor = df_com_ruido['acuracia_teste'].idxmax()
    config_otima = df_com_ruido.loc[idx_melhor]
    
    nivel_otimo = config_otima['nivel_ruido']
    dataset_otimo = config_otima['dataset']
    arq_otima = config_otima['arquitetura']
    init_otima = config_otima['estrategia_init']
    ruido_otimo = config_otima['tipo_ruido']
    
    if verbose:
        logger.info("\n✓ Configuração ótima identificada:")
        logger.info(f"  Dataset: {dataset_otimo}")
        logger.info(f"  Arquitetura: {arq_otima}")
        logger.info(f"  Inicialização: {init_otima}")
        logger.info(f"  Tipo de ruído: {ruido_otimo}")
        logger.info(f"  Nível ótimo: {nivel_otimo:.4f}")
        logger.info(f"  Acurácia: {config_otima['acuracia_teste']:.4f}")
        logger.info(f"\n→ Rastreando {n_passos} passos de {passo_fino} em cada direção...")
    
    # Gerar níveis de ruído ao redor do ótimo
    niveis_rastreio = []
    for i in range(-n_passos, n_passos + 1):
        nivel = nivel_otimo + i * passo_fino
        if 0 < nivel < 0.2:  # Limitar a range válido
            niveis_rastreio.append(nivel)
    
    resultados_rastreio = []
    
    # Executar rastreio fino
    for idx, nivel in enumerate(niveis_rastreio):
        if verbose:
            logger.info(f"  [{idx+1}/{len(niveis_rastreio)}] Testando nível {nivel:.4f}...")
        
        try:
            # Treinar VQC com configuração ótima + novo nível de ruído
            vqc = ClassificadorVQC(
                n_qubits=4,
                n_camadas=2,
                arquitetura=arq_otima,
                estrategia_init=init_otima,
                tipo_ruido=ruido_otimo,
                nivel_ruido=nivel,
                taxa_aprendizado=0.01,
                n_epocas=n_epocas,
                batch_size=32,
                seed=42,
                ruido_schedule='cosine',
                ruido_inicial=nivel,
                ruido_final=0.001,
                early_stopping=True,
                patience=5,
                min_delta=1e-3,
                val_split=0.1
            )
            
            dataset = datasets[dataset_otimo]
            vqc.fit(dataset['X_train'], dataset['y_train'])
            
            acuracia_teste = vqc.score(dataset['X_test'], dataset['y_test'])
            acuracia_treino = vqc.score(dataset['X_train'], dataset['y_train'])
            
            resultados_rastreio.append({
                'nivel_ruido': nivel,
                'acuracia_teste': acuracia_teste,
                'acuracia_treino': acuracia_treino,
                'gap': acuracia_treino - acuracia_teste,
                'custo_final': vqc.historico_['custo'][-1]
            })
            
            if verbose:
                logger.info(f"      Acurácia: {acuracia_teste:.4f}")
        
        except Exception as e:
            logger.warning(f"      Erro: {str(e)[:50]}")
    
    # Salvar resultados
    df_rastreio = pd.DataFrame(resultados_rastreio)
    csv_path = subdir_root / 'rastreio_fino_resultados.csv'
    df_rastreio.to_csv(csv_path, index=False)
    
    # Identificar nível ótimo refinado
    nivel_otimo_refinado = None
    acuracia_otima_refinada = None
    if len(df_rastreio) > 0:
        idx_melhor_refinado = df_rastreio['acuracia_teste'].idxmax()
        nivel_otimo_refinado = df_rastreio.loc[idx_melhor_refinado, 'nivel_ruido']
        acuracia_otima_refinada = df_rastreio.loc[idx_melhor_refinado, 'acuracia_teste']
        
        if verbose:
            logger.info("\n✓ Rastreio fino concluído!")
            logger.info(f"  Nível ótimo refinado: {nivel_otimo_refinado:.4f}")
            logger.info(f"  Acurácia refinada: {acuracia_otima_refinada:.4f}")
            logger.info(f"  Melhoria: {acuracia_otima_refinada - config_otima['acuracia_teste']:+.4f}")
    else:
        if verbose:
            logger.warning("Sem pontos válidos no rastreio fino; mantendo nível ótimo do grid.")
        nivel_otimo_refinado = float(nivel_otimo)
        acuracia_otima_refinada = float(config_otima['acuracia_teste'])
    
    # Criar README (evitar formatação condicional com especificadores de float no f-string)
    _nivel_ref_str = f"{nivel_otimo_refinado:.4f}" if nivel_otimo_refinado is not None else "N/A"
    _acc_ref_str = f"{acuracia_otima_refinada:.4f}" if acuracia_otima_refinada is not None else "N/A"
    _melhoria_str = (
        f"{(acuracia_otima_refinada - config_otima['acuracia_teste']):+.4f}"
        if acuracia_otima_refinada is not None else "N/A"
    )
    readme_content = (
        f"# Rastreio Fino do Nível Ótimo de Ruído\n\n"
        f"## Configuração Ótima (Grid Search)\n"
        f"- Dataset: {dataset_otimo}\n"
        f"- Arquitetura: {arq_otima}\n"
        f"- Inicialização: {init_otima}\n"
        f"- Tipo de ruído: {ruido_otimo}\n"
        f"- Nível ótimo (grid): {nivel_otimo:.4f}\n"
        f"- Acurácia (grid): {config_otima['acuracia_teste']:.4f}\n\n"
        f"## Rastreio Fino\n"
        f"- Passo: {passo_fino}\n"
        f"- Passos por direção: {n_passos}\n"
        f"- Range explorado: [{min(niveis_rastreio):.4f}, {max(niveis_rastreio):.4f}]\n"
        f"- Pontos testados: {len(niveis_rastreio)}\n\n"
        f"## Resultado Refinado\n"
        f"- Nível ótimo refinado: {_nivel_ref_str}\n"
        f"- Acurácia refinada: {_acc_ref_str}\n"
        f"- Melhoria: {_melhoria_str}\n\n"
        f"## Arquivos Gerados\n"
        f"- `rastreio_fino_resultados.csv`: Resultados completos do rastreio\n"
        f"- `metadata.json`: Metadados da execução\n"
    )
    
    (subdir_root / 'README_rastreio_fino.md').write_text(readme_content, encoding='utf-8')
    
    # Salvar metadata
    # Converter explicitamente para evitar warnings de tipo Scalar
    # Garantir conversão segura de qualquer tipo Pandas para float
    try:
        _nivel_otimo_f = float(nivel_otimo) if nivel_otimo is not None else 0.0  # type: ignore[arg-type]
    except (TypeError, ValueError):
        _nivel_otimo_f = 0.0
    
    try:
        _acuracia_grid_f = float(config_otima['acuracia_teste'])  # type: ignore[arg-type]
    except (TypeError, ValueError):
        _acuracia_grid_f = 0.0
    
    try:
        _nivel_otimo_refinado_f = float(nivel_otimo_refinado) if nivel_otimo_refinado is not None else 0.0  # type: ignore[arg-type]
    except (TypeError, ValueError):
        _nivel_otimo_refinado_f = 0.0
    
    try:
        _acuracia_otima_refinada_f = float(acuracia_otima_refinada) if acuracia_otima_refinada is not None else 0.0  # type: ignore[arg-type]
    except (TypeError, ValueError):
        _acuracia_otima_refinada_f = 0.0
    
    _melhoria_val = (acuracia_otima_refinada - config_otima['acuracia_teste']) if acuracia_otima_refinada is not None else 0.0
    try:
        _melhoria_f = float(_melhoria_val)  # type: ignore[arg-type]
    except (TypeError, ValueError):
        _melhoria_f = 0.0
    
    try:
        _min_rastreio_f = float(min(niveis_rastreio))  # type: ignore[arg-type]
        _max_rastreio_f = float(max(niveis_rastreio))  # type: ignore[arg-type]
    except (TypeError, ValueError):
        _min_rastreio_f = 0.0
        _max_rastreio_f = 0.0
    
    metadata = {
        'tipo': 'rastreio_fino',
        'data_execucao': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'config_otima': {
            'dataset': dataset_otimo,
            'arquitetura': arq_otima,
            'inicializacao': init_otima,
            'tipo_ruido': ruido_otimo,
            'nivel_ruido_grid': _nivel_otimo_f,
            'acuracia_grid': _acuracia_grid_f
        },
        'parametros_rastreio': {
            'passo_fino': passo_fino,
            'n_passos': n_passos,
            'range_explorado': [_min_rastreio_f, _max_rastreio_f],
            'pontos_testados': len(niveis_rastreio)
        },
        'resultado_refinado': {
            'nivel_ruido_otimo': _nivel_otimo_refinado_f,
            'acuracia_otima': _acuracia_otima_refinada_f,
            'melhoria': _melhoria_f
        },
        'arquivos_gerados': ['rastreio_fino_resultados.csv', 'README_rastreio_fino.md', 'metadata.json']
    }
    
    (subdir_root / 'metadata.json').write_text(
        json.dumps(metadata, indent=2, ensure_ascii=False),
        encoding='utf-8'
    )
    
    return resultados_rastreio
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Framework Investigativo Completo para Artigo Nature/Quantum
============================================================

Framework completo para execução de todos os experimentos, análises estatísticas
e geração de visualizações para o artigo:

"Beneficial Quantum Noise in Variational Quantum Classifiers: 
 A Systematic Investigation"

Funcionalidades:
- 216+ configurações VQC (4 datasets × 2+ arquiteturas × 5+ inits × 4 ruídos × 3 níveis)
- Análises estatísticas avançadas:
  * ANOVA 2-way e 3-way
  * Effect sizes: Cohen's d, Glass's Δ, Hedges' g
  * Post-hoc: Tukey HSD, Bonferroni, Scheffé
- Inicializações avançadas:
  * Fibonacci spiral, quantum harmonic, primes
  * Matemático (π, e, φ), Quântico (ℏ, α, R∞)
- Schedules de ruído quântico:
  * Linear, Exponencial, Cosseno, Adaptativo
- Mitigação de Barren Plateaus:
  * Detecção automática de gradientes
  * Normalização adaptativa
  * Inicialização layer-wise
- Monitoramento de emaranhamento:
  * Entropia de von Neumann
  * Negatividade (medida de emaranhamento)
- Otimizadores:
  * Adam (padrão)
  * SGD com momentum
  * Quantum Natural Gradient (QNG)
- Funções de custo:
  * MSE (Mean Squared Error)
  * Cross-Entropy (probabilístico)
  * Hinge Loss (SVM-style)
- 8 figuras de qualidade para publicação
- 4 tabelas principais + 3 suplementares
- Exportação completa de todos os resultados

Referências:
[1] Preskill (2018). Quantum Computing in the NISQ era. Quantum.
[2] Schuld et al. (2020). Circuit-centric quantum classifiers. Phys. Rev. A.
[3] Farhi & Neven (2018). Classification with quantum neural networks.
[4] McClean et al. (2018). Barren plateaus. Nature Comm.
[5] Grant et al. (2019). Initialization strategies. Quantum.

Autor: Equipe de Pesquisa VQC
Versão: 7.1.0 (Framework Avançado - Completo com Optuna + Lindblad)
Data: Outubro 2025
Licença: MIT

NOVO em v7.0:
✓ 4 schedules de ruído quântico (linear, exp, cosine, adaptativo)
✓ Detecção e mitigação de Barren Plateaus
✓ Monitoramento de entropia de emaranhamento
✓ 3 otimizadores (Adam, SGD, QNG)
✓ 3 funções de custo (MSE, Cross-Entropy, Hinge)
✓ 5 estratégias de inicialização (incluindo Fibonacci, Primes, Quantum Harmonic)

NOVO em v7.1:
✓ Autotuning com Optuna (otimização bayesiana de hiperparâmetros)
✓ Modelagem de ruído via Lindblad (5 canais: amplitude, phase, thermal, depolarizing, custom)
✓ Análises estatísticas profundas (PCA, Clustering K-means, Bootstrap CI)
✓ Análise de correlação e sensibilidade
✓ Testes de normalidade (Shapiro-Wilk)
✓ 9 visualizações interativas (5 originais + 4 novas)
✓ Effect sizes: Cohen's d, Glass's Δ, Hedges' g
✓ Post-hoc: Bonferroni, Scheffé
✓ Early stopping com validação
✓ Gradient clipping adaptativo
"""

# Helper: converter valor para float simples ou None (para silenciar stubs do Pylance)
def _to_float_or_none(x: Any) -> Optional[float]:
    try:
        import numpy as _np
        val = _np.asarray(x)
        # Extrair parte real, se complexo
        val = _np.real(val)
        return float(val)
    except Exception:
        try:
            return float(x)  # type: ignore[arg-type]
        except Exception:
            return None

# PennyLane (qml, pnp, AdamOptimizer)
try:
    import pennylane as qml
    from pennylane import numpy as pnp
    from pennylane.optimize import AdamOptimizer as PLAdamOptimizer
    ADAM_OPTIMIZER_CLS = PLAdamOptimizer
    PENNYLANE_AVAILABLE = True
except ImportError as _e:
    PENNYLANE_AVAILABLE = False
    logger.warning("⚠️ PennyLane não disponível. Instale com: pip install pennylane")
    import types as _types
    # Shim mínimo
    qml = _types.SimpleNamespace()
    pnp = _types.SimpleNamespace(
        array=lambda x, requires_grad=False: np.array(x),
        mean=np.mean,
        random=np.random
    )
    class AdamOptimizerStub:  # type: ignore
        def __init__(self, stepsize=0.01):
            self.lr = stepsize
        def step(self, func, *args):
            return args
    ADAM_OPTIMIZER_CLS = AdamOptimizerStub
    # Expor nome esperado pelo restante do código
AdamOptimizer = ADAM_OPTIMIZER_CLS

# v7.1: Novas dependências
try:
    import optuna
    from optuna.samplers import TPESampler
    from optuna.pruners import MedianPruner
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    optuna = None  # type: ignore[assignment]
    TPESampler = None  # type: ignore[assignment]
    MedianPruner = None  # type: ignore[assignment]
    logger.warning("⚠️ Optuna não disponível. Instale com: pip install optuna")

try:
    import joblib
    JOBLIB_AVAILABLE = True
except ImportError:
    JOBLIB_AVAILABLE = False
    logger.warning("⚠️ Joblib não disponível. Instale com: pip install joblib")

try:
    from sklearn.decomposition import PCA as SklearnPCA
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler as SklearnStandardScaler
    SKLEARN_ADVANCED_AVAILABLE = True
except ImportError:
    SKLEARN_ADVANCED_AVAILABLE = False
    logger.warning("⚠️ Módulos avançados do scikit-learn não disponíveis")

if TYPE_CHECKING:
    from sklearn.decomposition import PCA as SklearnPCA  # type: ignore
    from sklearn.cluster import KMeans  # type: ignore
    from sklearn.preprocessing import StandardScaler as SklearnStandardScaler  # type: ignore

# (logger já configurado acima)


# ============================================================================
# MÓDULO 1: CONSTANTES FUNDAMENTAIS
# ============================================================================
# Referência: CODATA 2018 (Mohr et al., 2019)

class ConstantesFundamentais:
    """
    Constantes matemáticas e físicas para inicialização de parâmetros.
    
    Referências:
    - Constantes matemáticas: Weisstein, "MathWorld"
    - Constantes quânticas: CODATA 2018 (Mohr et al., 2019)
    - Normalização: Grant et al. (2019). Quantum.
    """
    
    # Constantes Matemáticas
    PI = np.pi                          # π ≈ 3.14159
    E = np.e                            # e ≈ 2.71828
    PHI = (1 + np.sqrt(5)) / 2         # φ ≈ 1.61803 (Razão Áurea)
    SQRT2 = np.sqrt(2)                  # √2 ≈ 1.41421
    LN2 = np.log(2)                     # ln(2) ≈ 0.69315
    GAMMA = 0.5772156649                # γ (Euler-Mascheroni)
    
    # Constantes Quânticas (CODATA 2018)
    HBAR = 1.054571817e-34              # ℏ (constante de Planck reduzida) [J·s]
    ALPHA = 7.2973525693e-3             # α (constante de estrutura fina) [adimensional]
    RYDBERG = 10973731.568160           # R∞ (constante de Rydberg) [m⁻¹]
    
    @classmethod
    def normalizar(cls, valores):
        """
        Normaliza valores para [-π, π] usando escala logarítmica.
        
        Referência: Grant et al. (2019). "An initialization strategy for 
        addressing barren plateaus in parametrized quantum circuits." Quantum.
        
        Motivação: Constantes fundamentais abrangem 40 ordens de magnitude.
        Escala logarítmica mapeia para intervalo adequado para portas de rotação.
        """
        log_vals = np.log10(np.abs(valores) + 1e-10)
        norm = (log_vals - log_vals.min()) / (log_vals.max() - log_vals.min() + 1e-10)
        return -np.pi + norm * 2 * np.pi
    
    @classmethod
    def inicializar(cls, n_params, estrategia='aleatorio', seed=42):
        """
        Inicializa parâmetros com diferentes estratégias.
        
        Args:
            n_params: Número de parâmetros
            estrategia: 'matematico', 'quantico', ou 'aleatorio'
            seed: Semente aleatória para reprodutibilidade
        
        Returns:
            Array PennyLane com requires_grad=True
        """
        np.random.seed(seed)
        
        if estrategia == 'matematico':
            # Usa constantes matemáticas fundamentais
            const = np.array([cls.PI, cls.E, cls.PHI, cls.SQRT2, cls.LN2, cls.GAMMA])
            n_rep = int(np.ceil(n_params / len(const)))
            params = np.tile(const, n_rep)[:n_params]
            # Adiciona ruído gaussiano pequeno para quebrar simetria
            params += np.random.normal(0, 0.1, n_params)
            return pnp.array(cls.normalizar(params), requires_grad=True)
        
        elif estrategia == 'quantico':
            # Usa constantes físicas quânticas (CODATA 2018)
            const = np.array([cls.HBAR, cls.ALPHA, cls.RYDBERG])
            n_rep = int(np.ceil(n_params / len(const)))
            params = np.tile(const, n_rep)[:n_params]
            params += np.random.normal(0, 0.1, n_params)
            return pnp.array(cls.normalizar(params), requires_grad=True)
        
        elif estrategia == 'fibonacci_spiral':
            # Inicialização geométrica baseada em espiral de Fibonacci
            phi = (1 + np.sqrt(5)) / 2
            angles = []
            for i in range(n_params):
                angle = (2 * np.pi * (i / phi)) % (2 * np.pi) - np.pi
                angles.append(angle)
            return pnp.array(angles, requires_grad=True)

        elif estrategia == 'quantum_harmonic':
            # Níveis de energia do oscilador harmônico mapeados para ângulos
            hbar_norm = 1.0
            omega = 1.0
            angles = []
            for n in range(n_params):
                E_n = hbar_norm * omega * (n + 0.5)
                angle = (E_n % (2 * np.pi)) - np.pi
                angles.append(angle)
            return pnp.array(angles, requires_grad=True)

        elif estrategia == 'primes':
            # Codificação com números primos para quebrar simetria
            primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53,
                      59, 61, 67, 71, 73, 79, 83, 89]
            angles = []
            for i in range(n_params):
                p = primes[i % len(primes)]
                angle = (p * np.pi / 10) % (2 * np.pi) - np.pi
                angles.append(angle)
            return pnp.array(angles, requires_grad=True)

        elif estrategia == 'identity_blocks':
            # Identity Blocks (Grant et al., 2019)
            # Inicializa parâmetros para que blocos do circuito avaliem inicialmente para identidade
            # Referência: Grant et al. (2019). "An initialization strategy for addressing 
            # barren plateaus in parametrized quantum circuits." Quantum, v. 3, p. 214.
            #
            # Para RY gates: RY(0) = I
            # Para RX gates: RX(0) = I  
            # Para RZ gates: RZ(0) = I
            # Estratégia: inicializar todos os parâmetros próximos a 0 com pequeno ruído
            params = np.random.normal(0, 0.01, n_params)
            return pnp.array(params, requires_grad=True)

        else:  # aleatorio (baseline)
            # Inicialização uniforme padrão
            params = np.random.uniform(-np.pi, np.pi, n_params)
            return pnp.array(params, requires_grad=True)


# ============================================================================
# MÓDULO 1.5: FUNCIONALIDADES AVANÇADAS
# ============================================================================

class ScheduleRuido:
    """
    Schedules avançados de ruído quântico.
    
    Implementa 4 estratégias de annealing:
    - Linear: decaimento linear
    - Exponencial: decaimento exponencial
    - Cosseno: decaimento suave (cosseno)
    - Adaptativo: ajusta baseado na performance
    
    Referência: Smith (2017). "Cyclical Learning Rates" + adaptações para ruído quântico
    """
    
    @staticmethod
    def linear(epoca, n_epocas, nivel_inicial, nivel_final):
        """Linear annealing: p(t) = p_f + (p_i - p_f)(1 - t)"""
        t = epoca / max(1, n_epocas - 1)
        return nivel_final + (nivel_inicial - nivel_final) * (1 - t)
    
    @staticmethod
    def exponencial(epoca, n_epocas, nivel_inicial, nivel_final):
        """Exponential decay: p(t) = p_f + (p_i - p_f)exp(-t/τ)"""
        tau = max(1, n_epocas / 3)
        return nivel_final + (nivel_inicial - nivel_final) * np.exp(-epoca / tau)
    
    @staticmethod
    def cosseno(epoca, n_epocas, nivel_inicial, nivel_final):
        """Cosine annealing: p(t) = p_f + (p_i - p_f) * 0.5(1 + cos(πt))"""
        t = epoca / max(1, n_epocas - 1)
        return nivel_final + (nivel_inicial - nivel_final) * 0.5 * (1 + np.cos(np.pi * t))
    
    @staticmethod
    def adaptativo(epoca, n_epocas, nivel_inicial, nivel_final, historico_custo):
        """
        Adaptive annealing: reduz ruído mais rápido se convergindo bem.
        
        Se custo está caindo (boa convergência): acelera redução de ruído
        Se custo estável (platô): mantém ruído para exploração
        """
        if len(historico_custo) < 3:
            return ScheduleRuido.cosseno(epoca, n_epocas, nivel_inicial, nivel_final)
        
        # Calcular taxa de mudança do custo (últimas 3 épocas)
        custos_recentes = historico_custo[-3:]
        taxa_mudanca = (custos_recentes[0] - custos_recentes[-1]) / (custos_recentes[0] + 1e-8)
        
        # Se convergindo bem (custo caindo > 1%): acelera redução
        if taxa_mudanca > 0.01:
            fator_aceleracao = 1.5
        # Se em platô (mudança < 0.1%): mantém ruído
        elif abs(taxa_mudanca) < 0.001:
            fator_aceleracao = 0.5
        else:
            fator_aceleracao = 1.0
        
        epoca_ajustada = min(n_epocas - 1, epoca * fator_aceleracao)
        return ScheduleRuido.cosseno(int(epoca_ajustada), n_epocas, nivel_inicial, nivel_final)


class DetectorBarrenPlateau:
    """
    Detecta e mitiga Barren Plateaus.
    
    Referências:
    - McClean et al. (2018). "Barren plateaus in quantum neural networks." Nature Comm.
    - Grant et al. (2019). "Initialization strategies." Quantum.
    
    Estratégias de mitigação:
    1. Detecção: monitora variância dos gradientes
    2. Inicialização layer-wise: inicializa camadas progressivamente
    3. Normalização de gradientes: gradient clipping adaptativo
    """
    
    def __init__(self, threshold_variancia=1e-6, janela=5):
        self.threshold = threshold_variancia
        self.janela = janela
        self.historico_gradientes = []
    
    def detectar(self, gradientes):
        """
        Detecta se estamos em barren plateau.
        
        Args:
            gradientes: Array de gradientes atuais
        
        Returns:
            True se detectou platô, False caso contrário
        """
        grad_flat = np.array(gradientes).flatten()
        variancia = np.var(grad_flat)
        
        self.historico_gradientes.append(variancia)
        
        # Manter apenas últimas N medições
        if len(self.historico_gradientes) > self.janela:
            self.historico_gradientes.pop(0)
        
        # Platô se variância consistentemente baixa
        if len(self.historico_gradientes) >= self.janela:
            variancia_media = np.mean(self.historico_gradientes)
            return variancia_media < self.threshold
        
        return False
    
    @staticmethod
    def normalizar_gradientes(gradientes, max_norm=1.0):
        """
        Gradient clipping adaptativo.
        
        Referência: Pascanu et al. (2013). "On the difficulty of training RNNs"
        """
        grad_array = np.array(gradientes)
        norm = np.linalg.norm(grad_array)
        
        if norm > max_norm:
            return gradientes * (max_norm / norm)
        return gradientes


class MonitorEmaranhamento:
    """
    Monitora entropia de emaranhamento durante treinamento.
    
    Calcula:
    - Entropia de von Neumann: S(ρ) = -Tr(ρ log ρ)
    - Negatividade (para sistemas bipartidos)
    
    Referência: Horodecki et al. (2009). "Quantum entanglement." Rev. Mod. Phys.
    """
    
    def __init__(self, n_qubits):
        self.n_qubits = n_qubits
        self.historico_entropia = []
    
    def calcular_entropia_von_neumann(self, estado_densidade):
        """
        Calcula S(ρ) = -Tr(ρ log ρ)
        
        Args:
            estado_densidade: Matriz densidade (pode ser parcial)
        
        Returns:
            Entropia em bits
        """
        # Obter autovalores
        autovalores = np.linalg.eigvalsh(estado_densidade)
        
        # Remover valores negativos (ruído numérico)
        autovalores = autovalores[autovalores > 1e-12]
        
        # S = -Σ λ_i log_2(λ_i)
        entropia = -np.sum(autovalores * np.log2(autovalores))
        
        return entropia
    
    def calcular_negatividade(self, estado_densidade):
        """
        Calcula negatividade como medida de emaranhamento.
        
        Referência: Vidal & Werner (2002). "Computable measure of entanglement"
        """
        # Para sistemas 2-qubit, calcular transposição parcial
        dim = estado_densidade.shape[0]
        
        if dim == 4:  # Sistema 2-qubit
            # Transposição parcial (subsistema B)
            rho_pt = self._transpor_parcial(estado_densidade)
            
            # Negatividade = (||ρ^TB||_1 - 1) / 2
            autovalores = np.linalg.eigvalsh(rho_pt)
            norma_traco = np.sum(np.abs(autovalores))
            negatividade = (norma_traco - 1) / 2
            
            return max(0, negatividade)
        
        return 0.0
    
    def _transpor_parcial(self, rho):
        """Transposição parcial do segundo subsistema."""
        # Para 2 qubits (4x4)
        rho_pt = np.zeros_like(rho)
        for i in range(2):
            for j in range(2):
                for k in range(2):
                    for ell in range(2):
                        # Swap índices do segundo qubit
                        rho_pt[2*i+k, 2*j+ell] = rho[2*i+ell, 2*j+k]
        return rho_pt


class OtimizadorAvancado:
    """
    Otimizadores avançados para VQC.
    
    Implementa:
    - Adam (padrão)
    - SGD com momentum
    - Quantum Natural Gradient (QNG)
    
    Referências:
    - Kingma & Ba (2014). "Adam: A method for stochastic optimization"
    - Stokes et al. (2020). "Quantum Natural Gradient"
    """
    
    @staticmethod
    def criar(nome, taxa_aprendizado=0.01, **kwargs):
        """Fábrica de otimizadores."""
        if nome == 'adam':
            return AdamOptimizer(stepsize=taxa_aprendizado)
        elif nome == 'sgd':
            return OtimizadorAvancado.SGDMomentum(taxa_aprendizado, **kwargs)
        elif nome == 'qng':
            return OtimizadorAvancado.QNG(taxa_aprendizado, **kwargs)
        else:
            raise ValueError(f"Otimizador desconhecido: {nome}")
    
    class SGDMomentum:
        """SGD with Momentum"""
        def __init__(self, taxa_aprendizado=0.01, momentum=0.9):
            self.lr = taxa_aprendizado
            self.momentum = momentum
            self.velocidade = None
        
        def step(self, funcao_custo, *args):
            """Passo de otimização com momentum."""
            # Calcular gradientes
            gradientes = qml.grad(funcao_custo)(*args)
            
            # Inicializar velocidade
            if self.velocidade is None:
                self.velocidade = [np.zeros_like(g) for g in gradientes]
            
            # Atualizar velocidade e parâmetros
            novos_params = []
            for i, (param, grad) in enumerate(zip(args, gradientes)):
                self.velocidade[i] = self.momentum * self.velocidade[i] - self.lr * grad
                novos_params.append(param + self.velocidade[i])
            
            return tuple(novos_params)
    
    class QNG:
        """Quantum Natural Gradient (aproximação)"""
        def __init__(self, taxa_aprendizado=0.01, reg=1e-3):
            self.lr = taxa_aprendizado
            self.reg = reg  # Regularização para inversão da métrica
        
        def step(self, funcao_custo, *args):
            """
            QNG step (versão simplificada usando parameter shift rule).
            
            Referência: Stokes et al. (2020). "Quantum Natural Gradient"
            """
            # Para simplicidade, usa Adam (QNG completo requer cálculo da métrica quântica)
            gradientes = qml.grad(funcao_custo)(*args)
            novos_params = [param - self.lr * grad for param, grad in zip(args, gradientes)]
            return tuple(novos_params)


class FuncaoCustoAvancada:
    """
    Funções de custo alternativas.
    
    Implementa:
    - MSE (Mean Squared Error)
    - Cross-Entropy (para classificação probabilística)
    - Hinge Loss (SVM-style)
    """
    
    @staticmethod
    def mse(predicoes, labels):
        """Mean Squared Error"""
        return np.mean((labels - predicoes) ** 2)
    
    @staticmethod
    def cross_entropy(predicoes, labels):
        """
        Cross-Entropy Loss.
        
        Converte predições [-1, 1] para probabilidades [0, 1]
        """
        # Converter para numpy para operações não-diferenciáveis
        predicoes_np = np.array(predicoes)
        labels_np = np.array(labels)
        
        # Sigmoid para probabilidades
        probs = 1 / (1 + np.exp(-predicoes_np))
        
        # Labels de {-1, 1} para {0, 1}
        labels_01 = (labels_np + 1) / 2
        
        # Cross-entropy: -Σ [y log(p) + (1-y)log(1-p)]
        eps = 1e-10  # Evitar log(0) para evitar NaN
        ce = -np.mean(
            labels_01 * np.log(probs + eps) + 
            (1 - labels_01) * np.log(1 - probs + eps)
        )
        return float(ce)
    
    @staticmethod
    def hinge(predicoes, labels):
        """
        Hinge Loss (SVM-style).
        
        L = max(0, 1 - y·f(x))
        """
        # Converter para numpy para operações não-diferenciáveis
        predicoes_np = np.array(predicoes)
        labels_np = np.array(labels)
        
        margens = 1 - labels_np * predicoes_np
        return float(np.mean(np.maximum(0, margens)))


class TestesEstatisticosAvancados:
    """
    Testes estatísticos avançados para análise de resultados.
    
    Implementa:
    - Effect sizes: Cohen's d, Glass's Δ, Hedges' g
    - Post-hoc tests: Tukey HSD, Bonferroni, Scheffé
    """
    
    @staticmethod
    def cohen_d(grupo1, grupo2):
        """
        Cohen's d: medida de effect size.
        
        d = (μ₁ - μ₂) / σ_pooled
        
        Interpretação:
        - |d| < 0.2: pequeno
        - 0.2 ≤ |d| < 0.5: médio
        - |d| ≥ 0.8: grande
        
        Referência: Cohen (1988). "Statistical Power Analysis"
        """
        n1, n2 = len(grupo1), len(grupo2)
        var1, var2 = np.var(grupo1, ddof=1), np.var(grupo2, ddof=1)
        
        # Desvio padrão pooled
        s_pooled = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))
        
        # Cohen's d
        d = (np.mean(grupo1) - np.mean(grupo2)) / s_pooled
        return d
    
    @staticmethod
    def glass_delta(grupo1, grupo2):
        """
        Glass's Δ: similar a Cohen's d, mas usa apenas σ do grupo controle.
        
        Δ = (μ₁ - μ₂) / σ₂
        
        Útil quando um grupo é controle e outro tratamento.
        """
        return (np.mean(grupo1) - np.mean(grupo2)) / np.std(grupo2, ddof=1)
    
    @staticmethod
    def hedges_g(grupo1, grupo2):
        """
        Hedges' g: Cohen's d com correção para amostras pequenas.
        
        g = d * (1 - 3/(4(n₁ + n₂) - 9))
        
        Mais preciso que Cohen's d para n < 20.
        """
        n1, n2 = len(grupo1), len(grupo2)
        d = TestesEstatisticosAvancados.cohen_d(grupo1, grupo2)
        
        # Correção de viés
        correcao = 1 - (3 / (4 * (n1 + n2) - 9))
        g = d * correcao
        return g
    
    @staticmethod
    def bonferroni(p_values, alpha=0.05):
        """
        Correção de Bonferroni para múltiplas comparações.
        
        α_ajustado = α / n_testes
        
        Referência: Dunn (1961). "Multiple comparisons"
        """
        n_tests = len(p_values)
        alpha_ajustado = alpha / n_tests
        
        return [p < alpha_ajustado for p in p_values]
    
    @staticmethod
    def scheffe(grupos, alpha=0.05):
        """
        Teste de Scheffé: mais conservador que Tukey.
        
        Referência: Scheffé (1959). "The Analysis of Variance"
        """
        # Implementação simplificada usando F-statistic
        k = len(grupos)
        n_total = sum(len(g) for g in grupos)
        
        # ANOVA global
        f_stat, p_value = f_oneway(*grupos)
        
        # Limiar de Scheffé: (k-1)F_{k-1,n-k,α}
        from scipy.stats import f as f_dist
        df1, df2 = k - 1, n_total - k
        limiar_scheffe = (k - 1) * f_dist.ppf(1 - alpha, df1, df2)
        
        return {
            'f_statistic': f_stat,
            'p_value': p_value,
            'limiar_scheffe': limiar_scheffe,
            'significativo': f_stat > limiar_scheffe
        }


# ============================================================================
# MÓDULO 1.6: MODELAGEM DE RUÍDO VIA LINDBLAD (v7.1)
# ============================================================================

class LindbladNoiseModel:
    """
    Modelagem de ruído quântico via Equação Mestra de Lindblad (v7.1)
    
    A equação mestra de Lindblad descreve a evolução de um sistema quântico
    aberto acoplado a um ambiente:
    
    dρ/dt = -i[H,ρ] + Σ_k (L_k ρ L_k† - 1/2{L_k†L_k, ρ})
    
    onde L_k são os operadores de Lindblad (jump operators).
    
    Referências:
    - Lindblad (1976). "On the generators of quantum dynamical semigroups"
    - Breuer & Petruccione (2002). "The Theory of Open Quantum Systems"
    """
    
    def __init__(self, gamma: float = 0.01, n_qubits: int = 4, 
                 temperature: float = 0.0, timestep: float = 0.1):
        """
        Args:
            gamma: Taxa de decaimento/decoerência (0 a 1)
            n_qubits: Número de qubits no sistema
            temperature: Temperatura do ambiente (em unidades de energia)
            timestep: Passo de tempo para evolução discreta
        """
        self.gamma = gamma
        self.n_qubits = n_qubits
        self.temperature = temperature
        self.timestep = timestep
    
    def amplitude_damping_channel(self, qubit: int, T1: float):
        """
        Canal de amplitude damping (decaimento T1)
        
        Modela a perda de energia: |1⟩ → |0⟩
        L = √γ σ⁻, onde γ = 1/T1
        
        Args:
            qubit: Índice do qubit
            T1: Tempo de relaxação T1
        
        Returns:
            Probabilidade de erro para AmplitudeDamping
        """
        gamma_t1 = self.timestep / T1
        prob = 1 - np.exp(-gamma_t1)
        return min(prob, 1.0)
    
    def phase_damping_channel(self, qubit: int, T2: float):
        """
        Canal de phase damping (decoerência T2)
        
        Modela perda de coerência sem perda de energia
        L = √γ_φ σ_z, onde γ_φ = 1/T2
        
        Args:
            qubit: Índice do qubit
            T2: Tempo de decoerência T2
        
        Returns:
            Probabilidade de erro para PhaseDamping
        """
        gamma_t2 = self.timestep / T2
        prob = 1 - np.exp(-gamma_t2)
        return min(prob, 1.0)
    
    def thermal_relaxation_channel(self, qubit: int, T1: float, T2: float, 
                                   p_excited: float = 0.0):
        """
        Canal de relaxação térmica (T1 + T2 combinados)
        
        Combina amplitude damping e phase damping com população térmica
        
        Args:
            qubit: Índice do qubit
            T1: Tempo de relaxação
            T2: Tempo de decoerência
            p_excited: População térmica do estado |1⟩
        
        Returns:
            Tupla (prob_T1, prob_T2, p_excited)
        """
        prob_t1 = self.amplitude_damping_channel(qubit, T1)
        prob_t2 = self.phase_damping_channel(qubit, T2)
        
        return (prob_t1, prob_t2, p_excited)
    
    def aplicar_lindblad_noise(self, circuit_func, noise_type: str = 'thermal',
                               T1: float = 50.0, T2: float = 70.0):
        """
        Aplica ruído baseado em Lindblad ao circuito VQC
        
        Args:
            circuit_func: Função que define o circuito
            noise_type: Tipo de ruído ('amplitude', 'phase', 'thermal', 'depolarizing')
            T1: Tempo de relaxação (apenas para amplitude/thermal)
            T2: Tempo de decoerência (apenas para phase/thermal)
        
        Returns:
            Circuito modificado com canais de ruído
        """
        if noise_type == 'amplitude':
            for i in range(self.n_qubits):
                prob = self.amplitude_damping_channel(i, T1)
                qml.AmplitudeDamping(prob, wires=i)
        
        elif noise_type == 'phase':
            for i in range(self.n_qubits):
                prob = self.phase_damping_channel(i, T2)
                qml.PhaseDamping(prob, wires=i)
        
        elif noise_type == 'thermal':
            for i in range(self.n_qubits):
                prob_t1, prob_t2, p_exc = self.thermal_relaxation_channel(i, T1, T2)
                qml.AmplitudeDamping(prob_t1, wires=i)
                qml.PhaseDamping(prob_t2, wires=i)
        
        elif noise_type == 'depolarizing':
            for i in range(self.n_qubits):
                qml.DepolarizingChannel(self.gamma, wires=i)
        
        return circuit_func


# ============================================================================
# MÓDULO 1.7: AUTOTUNING COM OPTUNA (v7.1)
# ============================================================================

class AutotunerVQC:
    """
    Autotuning de hiperparâmetros com Optuna (v7.1)
    
    Utiliza otimização bayesiana (Tree-structured Parzen Estimator - TPE)
    para encontrar a melhor combinação de hiperparâmetros.
    
    Referências:
    - Akiba et al. (2019). "Optuna: A Next-generation Hyperparameter Optimization Framework"
    - Bergstra et al. (2011). "Algorithms for Hyper-Parameter Optimization"
    """
    
    def __init__(self, vqc_class, search_space: Optional[Dict] = None,
                 n_trials: int = 50, direction: str = "maximize", verbose: bool = True):
        """
        Args:
            vqc_class: Classe do VQC a otimizar
            search_space: Espaço de busca (se None, usa padrão)
            n_trials: Número de trials
            direction: 'maximize' (acurácia) ou 'minimize' (erro)
            verbose: Mostrar progresso
        """
        if not OPTUNA_AVAILABLE:
            raise ImportError("Optuna não instalado. Execute: pip install optuna")
        
        self.vqc_class = vqc_class
        self.search_space = search_space or self._get_default_search_space()
        self.n_trials = n_trials
        self.direction = direction
        self.verbose = verbose
        self.study = None
        self.best_params = None
    
    def _get_default_search_space(self) -> Dict[str, Any]:
        """
        Espaço de busca padrão para hiperparâmetros VQC
        
        Returns:
            dict: Configurações de busca
        """
        return {
            'learning_rate': ('float', 1e-4, 1.0, True),  # (tipo, low, high, log)
            'n_qubits': ('int', 2, 8, 2),  # (tipo, low, high, step)
            'n_camadas': ('int', 1, 5, 1),
            'nivel_ruido': ('float', 0.001, 0.1, False),  # Mudado de 0.0 para 0.001 e log=False
            'tipo_ruido': ('categorical', ['sem_ruido', 'depolarizante', 
                                                   'amplitude_damping', 'phase_damping',
                                                   'crosstalk', 'thermal', 'correlated_noise',
                                                   'readout_error', 'bit_flip', 'phase_flip',
                                                   'pink_noise']),
            'estrategia_init': ('categorical', ['normal', 'uniforme', 'xavier', 
                                               'fibonacci', 'quantum_harmonic']),
            'otimizador': ('categorical', ['adam', 'sgd', 'qng']),
            'funcao_custo': ('categorical', ['mse', 'cross_entropy', 'hinge'])
        }
    
    def _suggest_params(self, trial):
        """Sugere parâmetros usando Optuna trial"""
        params = {}
        
        for name, config in self.search_space.items():
            if config[0] == 'float':
                if len(config) > 3 and config[3]:  # log scale
                    params[name] = trial.suggest_float(name, config[1], config[2], log=True)
                else:
                    params[name] = trial.suggest_float(name, config[1], config[2])
            
            elif config[0] == 'int':
                step = config[3] if len(config) > 3 else 1
                params[name] = trial.suggest_int(name, config[1], config[2], step=step)
            
            elif config[0] == 'categorical':
                params[name] = trial.suggest_categorical(name, config[1])
        
        return params
    
    def optimize(self, X_train, y_train, X_val, y_val, n_epocas: int = 10):
        """
        Executa otimização bayesiana
        
        Args:
            X_train, y_train: Dados de treino
            X_val, y_val: Dados de validação
            n_epocas: Épocas por trial
        
        Returns:
            dict: Melhores hiperparâmetros encontrados
        """
        def objective(trial):
            # Sugerir hiperparâmetros
            params = self._suggest_params(trial)
            
            # Criar e treinar VQC com esses parâmetros
            try:
                vqc = self.vqc_class(
                    n_qubits=params.get('n_qubits', 4),
                    n_camadas=params.get('n_camadas', 2),
                    taxa_aprendizado=params.get('learning_rate', 0.01),
                    ruido={'tipo': params.get('tipo_ruido', 'sem_ruido'),
                           'nivel': params.get('nivel_ruido', 0.0)},
                    estrategia_init=params.get('estrategia_init', 'normal'),
                    otimizador=params.get('otimizador', 'adam'),
                    funcao_custo=params.get('funcao_custo', 'mse')
                )
                
                # Treinar
                vqc.treinar(X_train, y_train, n_epocas=n_epocas, verbose=False)
                
                # Avaliar em validação
                acc_val = vqc.calcular_acuracia(X_val, y_val)
                
                return acc_val
            
            except Exception as e:
                logger.warning(f"Trial falhou: {e}")
                return 0.0  # Retorna péssima acurácia se falhar
        
        # Criar estudo Optuna
        if not OPTUNA_AVAILABLE:
            logger.warning("Optuna indisponível: pulando optimize()")
            return {}
        # Asserts apenas para satisfazer tipagem estática
        assert OPTUNA_AVAILABLE and (TPESampler is not None) and (MedianPruner is not None) and (optuna is not None)
        sampler = TPESampler(seed=42)  # type: ignore[misc]
        pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=5)  # type: ignore[misc]
        
        self.study = optuna.create_study(  # type: ignore[union-attr]
            direction=self.direction,
            sampler=sampler,
            pruner=pruner
        )
        
        # Otimizar
        if self.verbose:
            logger.info(f"🔬 Iniciando otimização com {self.n_trials} trials...")
        
        self.study.optimize(objective, n_trials=self.n_trials, 
                           show_progress_bar=self.verbose)
        
        self.best_params = self.study.best_params
        best_value = self.study.best_value
        
        if self.verbose:
            logger.info(f"✓ Melhor acurácia: {best_value:.4f}")
            logger.info(f"✓ Melhores parâmetros: {self.best_params}")
        
        return self.best_params
    
    def get_importances(self):
        """Retorna importância de cada hiperparâmetro"""
        if self.study is None:
            raise ValueError("Execute optimize() primeiro")
        
        try:
            importances = optuna.importance.get_param_importances(self.study)  # type: ignore[union-attr]
            return importances
        except Exception as e:
            logger.warning(f"Não foi possível calcular importâncias: {e}")
            return {}
    
    def plot_optimization_history(self, save_path: str = 'optuna_history.html'):
        """Salva gráfico de histórico de otimização"""
        if self.study is None:
            raise ValueError("Execute optimize() primeiro")
        
        try:
            fig = optuna.visualization.plot_optimization_history(self.study)  # type: ignore[union-attr]
            fig.write_html(save_path)
            logger.info(f"✓ Histórico salvo: {save_path}")
        except Exception as e:
            logger.warning(f"Erro ao salvar histórico: {e}")


# ============================================================================
# MÓDULO 2: MODELOS DE RUÍDO QUÂNTICO

class ModeloRuido:
    """Classe base para modelos de ruído quântico."""
    def __init__(self, nivel=0.01):
        self.nivel = nivel
    def aplicar(self, n_qubits, nivel_override=None):
        raise NotImplementedError

class RuidoThermal(ModeloRuido):
    """Thermal Relaxation Error: aproxima T1/T2 com canais de amplitude e fase."""
    def aplicar(self, n_qubits, nivel_override=None):
        p = self.nivel if nivel_override is None else nivel_override
        for i in range(n_qubits):
            qml.AmplitudeDamping(p, wires=i)
            qml.PhaseDamping(p, wires=i)

class RuidoBitFlip(ModeloRuido):
    """Bit-Flip Error (X)."""
    def aplicar(self, n_qubits, nivel_override=None):
        p = self.nivel if nivel_override is None else nivel_override
        for i in range(n_qubits):
            qml.BitFlip(p, wires=i)

class RuidoPhaseFlip(ModeloRuido):
    """Phase-Flip Error (Z)."""
    def aplicar(self, n_qubits, nivel_override=None):
        p = self.nivel if nivel_override is None else nivel_override
        for i in range(n_qubits):
            qml.PhaseFlip(p, wires=i)

class RuidoPinkNoise(ModeloRuido):
    """1/f Noise (Pink): usa PhaseDamping com variação por qubit."""
    def aplicar(self, n_qubits, nivel_override=None):
        p = self.nivel if nivel_override is None else nivel_override
        pink = np.abs(np.random.normal(loc=0, scale=p, size=n_qubits))
        for i in range(n_qubits):
            qml.PhaseDamping(float(min(1.0, pink[i])) , wires=i)

class RuidoReadoutError(ModeloRuido):
    """Readout Error (aproximação via BitFlip após operações)."""
    def aplicar(self, n_qubits, nivel_override=None):
        p = self.nivel if nivel_override is None else nivel_override
        for i in range(n_qubits):
            qml.BitFlip(p, wires=i)


class RuidoDepolarizante(ModeloRuido):
    """
    Ruído de Depolarização: ρ → (1-p)ρ + p·I/2
    
    Referência: Preskill (2018). "Quantum Computing in the NISQ era." Quantum.
    
    Descrição: Erro genérico mais comum em qubits supercondutores (IBM, Google).
    O estado quântico é substituído pelo estado maximamente misto com probabilidade p.
    
    Operadores de Kraus:
    K₀ = √(1-p) I
    K₁ = √(p/3) X
    K₂ = √(p/3) Y
    K₃ = √(p/3) Z
    """
    def aplicar(self, n_qubits, nivel_override=None):
        p = self.nivel if nivel_override is None else nivel_override
        for i in range(n_qubits):
            qml.DepolarizingChannel(p, wires=i)


class RuidoAmplitudeDamping(ModeloRuido):
    """
    Amplitude Damping: Relaxamento T1 (|1⟩ → |0⟩)
    
    Referência: Clerk et al. (2010). "Introduction to quantum noise." Rev. Mod. Phys.
    
    Descrição: Perda de energia do qubit para o ambiente. Modela decaimento
    exponencial com tempo característico T1. Comum em qubits supercondutores.
    
    Operadores de Kraus:
    K₀ = [[1, 0], [0, √(1-γ)]]
    K₁ = [[0, √γ], [0, 0]]
    """
    def aplicar(self, n_qubits, nivel_override=None):
        g = self.nivel if nivel_override is None else nivel_override
        for i in range(n_qubits):
            qml.AmplitudeDamping(g, wires=i)


class RuidoPhaseDamping(ModeloRuido):
    """
    Phase Damping: Decoerência T2 (perda de fase)
    
    Referência: Schlosshauer (2007). "Decoherence and the Quantum-to-Classical Transition"
    
    Descrição: Perda de informação de fase sem perda de energia. Modela
    decoerência com tempo característico T2. Importante em qubits de spin.
    
    Operadores de Kraus:
    K₀ = [[1, 0], [0, √(1-λ)]]
    K₁ = [[0, 0], [0, √λ]]
    """
    def aplicar(self, n_qubits, nivel_override=None):
        lmb = self.nivel if nivel_override is None else nivel_override
        for i in range(n_qubits):
            qml.PhaseDamping(lmb, wires=i)


# ===================== NOVOS MODELOS DE RUÍDO =====================
class RuidoCrosstalk(ModeloRuido):
    """
    Ruído de Cross-Talk: Erros correlacionados entre qubits vizinhos
    
    Referência: Kandala et al. (2019). "Error mitigation extends the computational reach of a noisy quantum processor." Nature.
    
    Descrição: Aplica um canal de ruído correlacionado entre pares de qubits vizinhos (ex: CNOT com ruído).
    """
    def aplicar(self, n_qubits, nivel_override=None):
        p = self.nivel if nivel_override is None else nivel_override
        # Aplica canal de ruído correlacionado entre pares vizinhos
        for i in range(n_qubits):
            # Canal de ruído correlacionado: DepolarizingChannel em ambos os qubits simultaneamente
            qml.DepolarizingChannel(p, wires=i)
            qml.DepolarizingChannel(p, wires=(i+1)%n_qubits)
            # Cross-talk: canal extra entre pares
            qml.CNOT(wires=[i, (i+1)%n_qubits])
            qml.DepolarizingChannel(p, wires=(i+1)%n_qubits)
            qml.CNOT(wires=[i, (i+1)%n_qubits])


class RuidoCorrelacionado(ModeloRuido):
    """
    Ruído Correlacionado Global: Erros coletivos afetando todos os qubits
    
    Referência: Greenbaum (2015). "Introduction to Quantum Gate Set Tomography."
    
    Descrição: Aplica um canal de ruído coletivo (ex: PhaseDamping global) a todos os qubits simultaneamente.
    """
    def aplicar(self, n_qubits, nivel_override=None):
        lmb = self.nivel if nivel_override is None else nivel_override
        # Aplica PhaseDamping global a todos os qubits (efeito coletivo)
        for i in range(n_qubits):
            qml.PhaseDamping(lmb, wires=i)
        # Canal coletivo: aplica uma operação global (exemplo: ruído de fase global)
        # PennyLane não tem canal global nativo, mas pode-se simular aplicando em todos simultaneamente
        # Alternativamente, pode-se aplicar um canal customizado aqui se necessário


# Dicionário de modelos disponíveis
MODELOS_RUIDO = {
    'sem_ruido': None,
    'depolarizante': RuidoDepolarizante,
    'amplitude_damping': RuidoAmplitudeDamping,
    'phase_damping': RuidoPhaseDamping,
    'crosstalk': RuidoCrosstalk,
    'thermal': RuidoThermal,
    'correlated_noise': RuidoCorrelacionado,
    'bit_flip': RuidoBitFlip,
    'phase_flip': RuidoPhaseFlip,
    'pink_noise': RuidoPinkNoise,
    'readout_error': RuidoReadoutError
}


# ============================================================================
# MÓDULO 3: ARQUITETURAS DE CIRCUITOS QUÂNTICOS
def circuito_hardware_efficient(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):
    """
    Hardware-Efficient Ansatz: RY + CNOT em camadas alternadas
    """
    for i in range(min(len(x), n_qubits)):
        qml.RY(np.pi * x[i], wires=i)
    for camada in range(n_camadas):
        for i in range(n_qubits):
            qml.RY(weights[camada * n_qubits + i], wires=i)
        for i in range(0, n_qubits-1, 2):
            qml.CNOT(wires=[i, i+1])
        for i in range(1, n_qubits-1, 2):
            qml.CNOT(wires=[i, i+1])
        if modelo_ruido:
            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)
    return qml.expval(qml.PauliZ(0))

def circuito_tree(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):
    """
    Tree Entanglement: Emaranhamento em árvore binária
    """
    for i in range(min(len(x), n_qubits)):
        qml.RY(np.pi * x[i], wires=i)
    for camada in range(n_camadas):
        for i in range(n_qubits):
            qml.RY(weights[camada * n_qubits + i], wires=i)
        step = 1
        while step < n_qubits:
            for i in range(0, n_qubits-step, step*2):
                qml.CNOT(wires=[i, i+step])
            step *= 2
        if modelo_ruido:
            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)
    return qml.expval(qml.PauliZ(0))

def circuito_qaoa(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):
    """
    QAOA-like Ansatz: Alterna RX, RZZ, RX
    """
    for i in range(min(len(x), n_qubits)):
        qml.RX(np.pi * x[i], wires=i)
    for camada in range(n_camadas):
        for i in range(n_qubits):
            qml.RX(weights[camada * n_qubits + i], wires=i)
        for i in range(n_qubits-1):
            qml.CNOT(wires=[i, i+1])
            qml.RZ(weights[n_camadas * n_qubits + camada * (n_qubits-1) + i], wires=i+1)
            qml.CNOT(wires=[i, i+1])
        if modelo_ruido:
            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)
    return qml.expval(qml.PauliZ(0))

def circuito_alternating_layers(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):
    """
    Alternating Layers: RX, RY, CNOT em padrão alternado
    """
    for i in range(min(len(x), n_qubits)):
        qml.RX(np.pi * x[i], wires=i)
    for camada in range(n_camadas):
        for i in range(n_qubits):
            qml.RX(weights[camada * n_qubits + i], wires=i)
            qml.RY(weights[n_camadas * n_qubits + camada * n_qubits + i], wires=i)
        for i in range(n_qubits-1):
            qml.CNOT(wires=[i, i+1])
        if modelo_ruido:
            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)
    return qml.expval(qml.PauliZ(0))

def circuito_star_entanglement(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):
    """
    Star Entanglement: Qubit 0 central, CNOT(0, i)
    """
    for i in range(min(len(x), n_qubits)):
        qml.RY(np.pi * x[i], wires=i)
    for camada in range(n_camadas):
        for i in range(n_qubits):
            qml.RY(weights[camada * n_qubits + i], wires=i)
        for i in range(1, n_qubits):
            qml.CNOT(wires=[0, i])
        if modelo_ruido:
            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)
    return qml.expval(qml.PauliZ(0))

def circuito_brickwork(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):
    """
    Brickwork: padrão de CNOTs em "tijolos"
    """
    for i in range(min(len(x), n_qubits)):
        qml.RY(np.pi * x[i], wires=i)
    for camada in range(n_camadas):
        for i in range(n_qubits):
            qml.RY(weights[camada * n_qubits + i], wires=i)
        for i in range(0, n_qubits-1, 2):
            qml.CNOT(wires=[i, i+1])
        for i in range(1, n_qubits-1, 2):
            qml.CNOT(wires=[i, i+1])
        if modelo_ruido:
            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)
    return qml.expval(qml.PauliZ(0))

def circuito_random_entangling(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):
    """
    Random Entangling: CNOTs entre pares aleatórios por camada
    """
    import random
    for i in range(min(len(x), n_qubits)):
        qml.RY(np.pi * x[i], wires=i)
    for camada in range(n_camadas):
        for i in range(n_qubits):
            qml.RY(weights[camada * n_qubits + i], wires=i)
        pares = [(i, j) for i in range(n_qubits) for j in range(i+1, n_qubits)]
        random.shuffle(pares)
        for i, j in pares[:n_qubits//2]:
            qml.CNOT(wires=[i, j])
        if modelo_ruido:
            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)
    return qml.expval(qml.PauliZ(0))
# ============================================================================

def circuito_basico(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):
    """
    Circuito Básico: RY encoding + RY rotations + CNOT ring
    
    Referência: Farhi & Neven (2018). "Classification with quantum neural networks 
    on near term processors." arXiv:1802.06002
    
    Estrutura:
    1. Encoding: RY(π·x[i]) em cada qubit
    2. Camadas variacionais (repetidas L vezes):
       - RY(θ[i]) em cada qubit
       - CNOT em anel: CNOT(i, i+1 mod n)
    3. Medição: ⟨Z₀⟩
    
    Complexidade: O(n_qubits × n_camadas)
    Parâmetros: n_qubits × n_camadas
    
    Vantagens:
    - Simples e rápido
    - Bom para prototipagem
    - Baixo número de parâmetros
    """
    # 1. Encoding de dados
    for i in range(min(len(x), n_qubits)):
        qml.RY(np.pi * x[i], wires=i)
    
    # 2. Camadas variacionais
    for camada in range(n_camadas):
        # Rotações parametrizadas
        for i in range(n_qubits):
            qml.RY(weights[camada * n_qubits + i], wires=i)
        
        # Emaranhamento (CNOT em anel)
        for i in range(n_qubits):
            qml.CNOT(wires=[i, (i + 1) % n_qubits])
        
        # Aplicar ruído após cada camada (se especificado)
        if modelo_ruido:
            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)
    
    # 3. Medição
    return qml.expval(qml.PauliZ(0))


def circuito_strongly_entangling(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):
    """
    Strongly Entangling Layers (PennyLane template)
    
    Referência: Schuld et al. (2020). "Circuit-centric quantum classifiers." 
    Physical Review A, 101(3), 032308.
    
    Estrutura:
    1. Encoding: AngleEmbedding (RY em cada qubit)
    2. StronglyEntanglingLayers (template PennyLane):
       - Rot(θ, φ, ω) em cada qubit (3 rotações arbitrárias)
       - CNOT(i, j) para todos i < j (emaranhamento completo)
    3. Medição: ⟨Z₀⟩
    
    Complexidade: O(n_qubits² × n_camadas)
    Parâmetros: n_qubits × n_camadas × 3
    
    Vantagens:
    - Alta capacidade expressiva
    - Emaranhamento forte
    - Template otimizado do PennyLane
    """
    # 1. Encoding de dados
    qml.AngleEmbedding(x, wires=range(n_qubits), rotation='Y')
    
    # 2. Camadas fortemente emaranhadas
    weights_reshaped = weights.reshape(n_camadas, n_qubits, 3)
    qml.StronglyEntanglingLayers(weights_reshaped, wires=range(n_qubits))
    
    # 3. Aplicar ruído (se especificado)
    if modelo_ruido:
        modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)
    
    # 4. Medição
    return qml.expval(qml.PauliZ(0))


# Dicionário de arquiteturas disponíveis
ARQUITETURAS = {
    'basic_entangler': (circuito_basico, lambda nq, nc: nc * nq),
    'strongly_entangling': (circuito_strongly_entangling, lambda nq, nc: nc * nq * 3),
    'hardware_efficient': (circuito_hardware_efficient, lambda nq, nc: nc * nq),
    'tree': (circuito_tree, lambda nq, nc: nc * nq),
    'qaoa': (circuito_qaoa, lambda nq, nc: nc * nq + nc * (nq-1)),
    'alternating_layers': (circuito_alternating_layers, lambda nq, nc: 2 * nc * nq),
    'star_entanglement': (circuito_star_entanglement, lambda nq, nc: nc * nq),
    'brickwork': (circuito_brickwork, lambda nq, nc: nc * nq),
    'random_entangling': (circuito_random_entangling, lambda nq, nc: nc * nq)
}


# ============================================================================
# MÓDULO 4: CLASSIFICADOR QUÂNTICO VARIACIONAL (VQC)
# ============================================================================

class ClassificadorVQC(BaseEstimator, ClassifierMixin):
    """
    Classificador Quântico Variacional.
    
    Referências:
    - Schuld et al. (2020). "Circuit-centric quantum classifiers." Phys. Rev. A.
    - Mitarai et al. (2018). "Quantum circuit learning." Phys. Rev. A.
    - Bergholm et al. (2018). "PennyLane: Automatic differentiation." arXiv:1811.04968
    
    Implementa interface scikit-learn (BaseEstimator, ClassifierMixin) para
    compatibilidade com pipelines de ML clássico.
    """
    
    def __init__(self, n_qubits=4, n_camadas=2, arquitetura='basico',
                 estrategia_init='aleatorio', tipo_ruido='sem_ruido', nivel_ruido=0.01,
                 taxa_aprendizado=0.01, n_epocas=20, batch_size=32, seed=42,
                 ruido_schedule=None, ruido_inicial=None, ruido_final=None,
                 early_stopping=False, patience=10, min_delta=1e-3, val_split=0.1,
                 ruido_adaptativo=False, track_entanglement=False,
                 otimizador='adam', funcao_custo='mse', detectar_barren=False,
                 max_grad_norm=1.0):
        """
        Args:
            n_qubits: Número de qubits (2-20)
            n_camadas: Profundidade do circuito (1-10)
            arquitetura: 'basico' ou 'strongly_entangling'
            estrategia_init: 'aleatorio', 'matematico', ou 'quantico'
            tipo_ruido: 'sem_ruido', 'depolarizante', 'amplitude_damping', 'phase_damping'
            nivel_ruido: Taxa de erro (0.0-0.05)
            taxa_aprendizado: Learning rate para Adam (1e-4 a 1e-1)
            n_epocas: Número de épocas de treinamento (10-200)
            batch_size: Tamanho do mini-batch (8-128)
            seed: Semente aleatória para reprodutibilidade
        """
        self.n_qubits = n_qubits
        self.n_camadas = n_camadas
        self.arquitetura = arquitetura
        self.estrategia_init = estrategia_init
        # Permitir uso automático de 'correlated_noise' se tipo_ruido for 'correlated' ou 'correlated_noise'
        if tipo_ruido in ['correlated', 'correlated_noise']:
            self.tipo_ruido = 'correlated_noise'
        else:
            self.tipo_ruido = tipo_ruido
        self.nivel_ruido = nivel_ruido
        self.taxa_aprendizado = taxa_aprendizado
        self.n_epocas = n_epocas
        self.batch_size = batch_size
        self.seed = seed
        # Annealing de ruído
        self.ruido_schedule = ruido_schedule  # 'linear' | 'exponencial' | 'cosine' | None
        self.ruido_inicial = ruido_inicial
        self.ruido_final = ruido_final
        # Early stopping
        self.early_stopping = early_stopping
        self.patience = patience
        self.min_delta = min_delta
        self.val_split = val_split
        self.ruido_adaptativo = ruido_adaptativo
        self.track_entanglement = track_entanglement
        
        # Funcionalidades avançadas
        self.otimizador = otimizador  # 'adam', 'sgd', 'qng'
        self.funcao_custo = funcao_custo  # 'mse', 'cross_entropy', 'hinge'
        self.detectar_barren = detectar_barren
        self.max_grad_norm = max_grad_norm
        
        # Histórico de treinamento expandido
        self.historico_ = {
            'custo': [], 
            'acuracia_treino': [], 
            'epoca': [],
            'nivel_ruido': [],
            'entropia_emaranhamento': [],
            'variancia_gradiente': []
        }
        
        # Detectores e monitores
        self.detector_plateau_ = DetectorBarrenPlateau() if detectar_barren else None
        self.monitor_emaranhamento_ = MonitorEmaranhamento(n_qubits) if track_entanglement else None
        
        # Configurar seeds para reprodutibilidade
        np.random.seed(seed)
        try:
            # Nem sempre disponível no shim; ignore se ausente
            pnp.random.seed(seed)  # type: ignore[attr-defined]
        except Exception:
            pass
    
    def _criar_circuito(self):
        """
        Cria o circuito quântico e inicializa parâmetros.
        
        Usa PennyLane's default.mixed device para suportar ruído.
        """
        # Dispositivo quântico (simulador de matriz de densidade)
        self.dev_ = qml.device('default.mixed', wires=self.n_qubits)
        
        # Selecionar arquitetura e calcular número de parâmetros
        circuito_fn, calc_params = ARQUITETURAS[self.arquitetura]
        n_params = calc_params(self.n_qubits, self.n_camadas)
        
        # Criar modelo de ruído (se especificado)
        modelo_ruido = None
        if self.tipo_ruido != 'sem_ruido':
            modelo_ruido = MODELOS_RUIDO[self.tipo_ruido](self.nivel_ruido)
        
        # Criar QNode (circuito quântico diferenciável)
        @qml.qnode(self.dev_, interface='autograd')
        def circuit(weights, x, nivel_ruido_runtime=None):
            return circuito_fn(weights, x, self.n_qubits, self.n_camadas, modelo_ruido, nivel_ruido_runtime)
        
        self.qnode_ = circuit
        
        # Inicializar pesos com estratégia escolhida
        self.weights_ = ConstantesFundamentais.inicializar(
            n_params, self.estrategia_init, self.seed
        )
        
        # Inicializar bias
        self.bias_ = pnp.array(0.0, requires_grad=True)
    
    def _nivel_ruido_epoca(self, epoca):
        if self.tipo_ruido == 'sem_ruido':
            return 0.0
        # Sem schedule: usar nivel fixo
        if not self.ruido_schedule:
            return self.nivel_ruido
        nE, ri, rf = max(1, self.n_epocas), (self.ruido_inicial if self.ruido_inicial is not None else self.nivel_ruido), (self.ruido_final if self.ruido_final is not None else 0.001)
        t = epoca / max(1, nE - 1)
        if self.ruido_schedule == 'linear':
            return rf + (ri - rf) * (1 - t)
        if self.ruido_schedule == 'exponencial':
            tau = max(1, nE / 3)
            return rf + (ri - rf) * np.exp(-epoca / tau)
        if self.ruido_schedule == 'cosine':
            return rf + (ri - rf) * 0.5 * (1 + np.cos(np.pi * t))
        return self.nivel_ruido

    def _funcao_custo(self, weights, bias, X, y, nivel_ruido_runtime=None):
        """Função de custo configurável (MSE, Cross-Entropy ou Hinge)."""
        predicoes = pnp.array([self.qnode_(weights, x, nivel_ruido_runtime) + bias for x in X])
        
        # Selecionar função de custo
        if self.funcao_custo == 'mse':
            # Usar pnp.mean para manter grafo de autograd
            diff = pnp.array(y) - predicoes
            return pnp.mean(diff ** 2)  # type: ignore[attr-defined]
        elif self.funcao_custo == 'cross_entropy':
            return FuncaoCustoAvancada.cross_entropy(predicoes, y)
        elif self.funcao_custo == 'hinge':
            return FuncaoCustoAvancada.hinge(predicoes, y)
        else:
            diff = pnp.array(y) - predicoes
            return pnp.mean(diff ** 2)  # type: ignore[attr-defined]
    
    def fit(self, X, y):
        """
        Treina o classificador.
        
        Args:
            X: Dados de treinamento (n_samples, n_features)
            y: Labels (n_samples,)
        
        Returns:
            self (para compatibilidade scikit-learn)
        """
        # Codificar labels como ±1
        self.label_encoder_ = LabelEncoder()
        y_le = np.asarray(self.label_encoder_.fit_transform(y), dtype=int)
        y_encoded = (y_le * 2) - 1  # type: ignore[operator]
        self.classes_ = self.label_encoder_.classes_
        
        # Criar circuito e inicializar parâmetros
        self._criar_circuito()
        
        # Split de validação (para early stopping)
        X_arr, y_arr = np.asarray(X), np.asarray(y_encoded)
        if self.early_stopping and self.val_split > 0:
            n = len(X_arr)
            n_val = max(1, int(n * self.val_split))
            idx = np.random.permutation(n)
            val_idx, train_idx = idx[:n_val], idx[n_val:]
            X_train_es, y_train_es = X_arr[train_idx], y_arr[train_idx]
            X_val_es, y_val_es = X_arr[val_idx], y_arr[val_idx]
        else:
            X_train_es, y_train_es = X_arr, y_arr
            X_val_es = y_val_es = None

        # Criar otimizador configurável
        opt = OtimizadorAvancado.criar(self.otimizador, self.taxa_aprendizado)
        
        # Treinamento por épocas
        melhor_val = -np.inf
        sem_melhora = 0
        melhor_w, melhor_b = None, None
        
        for epoca in range(self.n_epocas):
            # Embaralhar dados
            indices = np.random.permutation(len(X_train_es))
            
            # Mini-batch gradient descent
            nivel_runtime = self._nivel_ruido_epoca(epoca)
            for i in range(0, len(X_train_es), self.batch_size):
                batch_idx = indices[i:i + self.batch_size]
                X_batch = X_train_es[batch_idx]
                y_batch = y_train_es[batch_idx]
                
                # Atualizar parâmetros (função de custo compatível com autograd)
                def custo_batch(w, b):
                    preds = pnp.array([self.qnode_(w, pnp.array(x), nivel_runtime) + b for x in X_batch])
                    # Usar pnp.mean para permitir gradientes
                    return pnp.mean((pnp.array(y_batch) - preds) ** 2)  # type: ignore[attr-defined]

                _step_res = opt.step(custo_batch, self.weights_, self.bias_)
                # Robustez para diferentes retornos
                try:
                    self.weights_, self.bias_ = _step_res  # type: ignore[assignment]
                except Exception:
                    if isinstance(_step_res, (list, tuple)) and len(_step_res) >= 2:
                        self.weights_, self.bias_ = _step_res[0], _step_res[1]
                    else:
                        pass
            
            # Registrar histórico
            custo_val = self._funcao_custo(
                self.weights_, self.bias_, pnp.array(X_train_es), pnp.array(y_train_es), nivel_runtime
            )
            try:
                custo = float(custo_val)
            except Exception:
                custo = float(pnp.asarray(custo_val))
            acuracia = self.score(X, y)
            
            self.historico_['custo'].append(custo)
            self.historico_['acuracia_treino'].append(acuracia)
            self.historico_['epoca'].append(epoca)
            self.historico_['nivel_ruido'].append(nivel_runtime)
            
            # Monitorar gradientes (detecção de barren plateau)
            if self.detector_plateau_:
                try:
                    gradientes = qml.grad(self._funcao_custo)(
                        self.weights_, self.bias_, 
                        pnp.array(X_train_es[:5]), pnp.array(y_train_es[:5]),
                        nivel_runtime
                    )
                    if isinstance(gradientes, (list, tuple)) and len(gradientes) > 0:
                        grad_array = np.array(gradientes[0]).flatten()
                    else:
                        grad_array = np.array([])
                    variancia_grad = float(np.var(grad_array))
                    self.historico_['variancia_gradiente'].append(variancia_grad)
                    
                    if self.detector_plateau_.detectar(grad_array):
                        logger.warning(f"Época {epoca}: Barren Plateau detectado (var={variancia_grad:.2e})")
                except Exception:
                    self.historico_['variancia_gradiente'].append(0.0)
            else:
                self.historico_['variancia_gradiente'].append(0.0)
            
            # Monitorar emaranhamento
            if self.monitor_emaranhamento_:
                try:
                    # Criar QNode que retorna estado
                    @qml.qnode(self.dev_, interface='autograd')
                    def estado_qnode(weights, x):
                        circuito_fn, _ = ARQUITETURAS[self.arquitetura]
                        circuito_fn(weights, x, self.n_qubits, self.n_camadas, None, None)
                        return qml.density_matrix(wires=[0])
                    
                    # Calcular estado do primeiro qubit
                    x_sample = pnp.array(X_train_es[0])
                    rho_0 = estado_qnode(self.weights_, x_sample)
                    
                    # Calcular entropia
                    entropia = self.monitor_emaranhamento_.calcular_entropia_von_neumann(rho_0)
                    self.historico_['entropia_emaranhamento'].append(float(entropia))
                except Exception:
                    self.historico_['entropia_emaranhamento'].append(0.0)
            else:
                self.historico_['entropia_emaranhamento'].append(0.0)
        
            # Early stopping
            if self.early_stopping and X_val_es is not None and y_val_es is not None:
                ac_val = np.mean(self.predict(X_val_es) == self.label_encoder_.inverse_transform(((y_val_es + 1)//2).astype(int)))
                if ac_val > melhor_val + self.min_delta:
                    melhor_val = ac_val
                    sem_melhora = 0
                    melhor_w, melhor_b = self.weights_.copy(), float(self.bias_)
                else:
                    sem_melhora += 1
                if sem_melhora >= self.patience:
                    # Restaurar melhor
                    if (melhor_w is not None) and (melhor_b is not None):
                        self.weights_ = melhor_w
                        self.bias_ = pnp.array(melhor_b, requires_grad=True)
                    break

        return self
    
    def predict(self, X):
        """
        Prediz classes para novos dados.
        
        Args:
            X: Dados de teste (n_samples, n_features)
        
        Returns:
            Predições de classe (n_samples,)
        """
        # Obter predições do circuito quântico
        predicoes = np.array([
            float(self.qnode_(self.weights_, pnp.array(x)) + self.bias_)
            for x in X
        ])
        
        # Converter de ±1 para classes originais
        predicoes_classe = ((np.sign(predicoes) + 1) // 2).astype(int)
        return self.label_encoder_.inverse_transform(predicoes_classe)
    
    def score(self, X, y, sample_weight=None):
        """
        Calcula acurácia.
        
        Args:
            X: Dados
            y: Labels verdadeiros
        
        Returns:
            Acurácia (0.0 a 1.0)
        """
        return np.mean(self.predict(X) == y)


# ============================================================================
# MÓDULO 5: GERENCIAMENTO DE DATASETS
# ============================================================================

def carregar_datasets(seed=42):
    """
    Carrega 4 datasets de benchmark para classificação binária.
    
    Referências:
    - Moons, Circles: Scikit-learn (Pedregosa et al., 2011)
    - Iris: Fisher (1936), UCI ML Repository
    - Breast Cancer: Wolberg et al. (1995), UCI ML Repository
    
    Returns:
        Dict com 4 datasets, cada um contendo X_train, X_test, y_train, y_test
    """
    datasets = {}
    scaler = StandardScaler()
    
    # Dataset 1: Moons (não-linear, duas luas entrelaçadas)
    X, y = sk_datasets.make_moons(n_samples=400, noise=0.1, random_state=seed)
    X = scaler.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=seed, stratify=y
    )
    datasets['moons'] = {
        'X_train': X_train, 'X_test': X_test, 
        'y_train': y_train, 'y_test': y_test,
        'descricao': 'Duas luas entrelaçadas (não-linear)'
    }
    
    # Dataset 2: Circles (não-linear, círculos concêntricos)
    X, y = sk_datasets.make_circles(
        n_samples=400, noise=0.1, factor=0.5, random_state=seed
    )
    X = scaler.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=seed, stratify=y
    )
    datasets['circles'] = {
        'X_train': X_train, 'X_test': X_test,
        'y_train': y_train, 'y_test': y_test,
        'descricao': 'Círculos concêntricos (não-linear)'
    }
    
    # Dataset 3: Iris (linear, 2 primeiras classes)
    from sklearn.utils import Bunch as SklearnBunch
    iris_data: SklearnBunch = sk_datasets.load_iris()  # type: ignore[assignment]
    X, y = iris_data.data, iris_data.target
    mask = y < 2  # Apenas setosa e versicolor
    X, y = X[mask], y[mask]
    X = scaler.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=seed, stratify=y
    )
    datasets['iris'] = {
        'X_train': X_train, 'X_test': X_test,
        'y_train': y_train, 'y_test': y_test,
        'descricao': 'Flores Iris (linear, 4 features)'
    }
    
    # Dataset 4: Breast Cancer (alta dimensão, 30 features)
    cancer_data: SklearnBunch = sk_datasets.load_breast_cancer()  # type: ignore[assignment]
    X, y = cancer_data.data, cancer_data.target
    X = scaler.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=seed, stratify=y
    )
    datasets['breast_cancer'] = {
        'X_train': X_train, 'X_test': X_test,
        'y_train': y_train, 'y_test': y_test,
        'descricao': 'Diagnóstico de câncer (30 features)'
    }
    
    # Dataset 5: Wine (features químicas, 2 primeiras classes)
    wine_data: SklearnBunch = sk_datasets.load_wine()  # type: ignore[assignment]
    X, y = wine_data.data, wine_data.target
    mask = y < 2  # Apenas classes 0 e 1
    X, y = X[mask], y[mask]
    X = scaler.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=seed, stratify=y
    )
    datasets['wine'] = {
        'X_train': X_train, 'X_test': X_test,
        'y_train': y_train, 'y_test': y_test,
        'descricao': 'Vinhos (13 features químicas)'
    }
    
    return datasets


# ============================================================================
# MÓDULO 6: EXECUÇÃO DE EXPERIMENTOS
# ============================================================================

def executar_grid_search(datasets, n_epocas=15, verbose=True, pasta_resultados=None):
    """
    Executa grid search completo sobre todas as configurações.
    
    Grid:
    - 5 datasets (moons, circles, iris, breast_cancer, wine)
    - 8 arquiteturas
    - 5 estratégias de inicialização
    - 5 tipos de ruído (depolarizing, amplitude_damping, phase_damping, crosstalk, thermal/correlated)
    - 3 níveis de ruído
    - 4 schedules de ruído (linear, exponencial, cosseno, adaptativo)
    Total: 5 × 8 × 5 × 5 × 3 × 4 = 12.000 configurações
    # Observação: No código, 'thermal' é tratado como ruído correlacionado para consistência com os documentos.
    
    Args:
        datasets: Dict de datasets
        n_epocas: Número de épocas de treinamento
        verbose: Se True, imprime progresso
    
    Returns:
        DataFrame com todos os resultados
    """
    # Pasta para granularidade máxima
    pasta_individual = None
    if pasta_resultados is None:
        pasta_resultados = os.path.join(os.getcwd(), f"resultados_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}")
    os.makedirs(pasta_resultados, exist_ok=True)
    pasta_individual = os.path.join(pasta_resultados, 'experimentos_individuais')
    os.makedirs(pasta_individual, exist_ok=True)
    # Placeholders para Pylance
    from typing import Any
    metadata: Dict[str, Any] = {}
    metadata_path: Optional[str] = None
    
    # Definir grid de hiperparâmetros
    quick = os.environ.get('VQC_QUICK', '0') == '1'
    if quick:
        grid = {
            'arquitetura': list(ARQUITETURAS.keys()),
            'estrategia_init': ['matematico', 'fibonacci_spiral'],
            'tipo_ruido': ['sem_ruido', 'depolarizante'],
            'nivel_ruido': [0.0, 0.0025, 0.005, 0.0075, 0.01]
        }
    else:
        grid = {
            'arquitetura': list(ARQUITETURAS.keys()),
            'estrategia_init': ['matematico', 'quantico', 'aleatorio', 'fibonacci_spiral'],
            'tipo_ruido': ['sem_ruido', 'depolarizante', 'amplitude_damping', 'phase_damping', 'crosstalk', 'correlacionado'],
            'nivel_ruido': [0.0, 0.0025, 0.005, 0.0075, 0.01, 0.0125, 0.015, 0.0175, 0.02]
        }
    
    resultados = []
    n_seeds = 5
    seed_list = [42 + i for i in range(n_seeds)]
    contador = 0  # Inicializar contador
    
    # Calcular total de configurações
    total_configs = len(grid['arquitetura']) * len(grid['estrategia_init']) * len(grid['tipo_ruido']) * len(grid['nivel_ruido'])
    # Ajustar para configs válidas (sem_ruido só com nivel 0)
    configs_invalidas = len(grid['arquitetura']) * len(grid['estrategia_init']) * (len(grid['nivel_ruido']) - 1)  # sem_ruido com nivel > 0
    total_configs -= configs_invalidas
    
    # Geração de README e metadata.json após grid/seed_list definidos
    if pasta_resultados is not None:
        readme_path = os.path.join(pasta_resultados, 'README_grid_search.md')
        metadata_path = os.path.join(pasta_resultados, 'metadata_grid_search.json')
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(
                "# Resultados do Grid Search VQC\n\n"
                f"- Data de execução: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                f"- Parâmetros do grid: {grid}\n"
                f"- Seeds: {seed_list}\n\n"
                "Todos os experimentos, circuitos e gráficos estão organizados nesta pasta.\n"
                "O arquivo `resultados_completos_artigo.csv` contém todos os resultados consolidados.\n"
            )
        metadata = {
            'tipo': 'grid_search',
            'data_execucao': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'parametros_grid': grid,
            'seeds': seed_list,
            'arquivos_gerados': [],
            'csv_consolidado': None,
            'readme': readme_path
        }
    
    for nome_dataset, dataset in datasets.items():
        _X_train, _y_train = dataset['X_train'], dataset['y_train']
        _X_test, _y_test = dataset['X_test'], dataset['y_test']
        
        for arq in grid['arquitetura']:
            for init in grid['estrategia_init']:
                for ruido in grid['tipo_ruido']:
                    for nivel in grid['nivel_ruido']:
                        if ruido == 'sem_ruido' and nivel > 0:
                            continue
                        for seed in seed_list:
                            contador += 1
                            # Log detalhado de todos os parâmetros
                            if verbose:
                                logger.info(
                                    f"[{contador:3d}/{total_configs * n_seeds}] "
                                    f"Dataset: {nome_dataset} | Seed: {seed} | Qubits: 4 | Camadas: 2 | "
                                    f"Arquitetura: {arq} | Init: {init} | Ruído: {ruido} | Nível: {nivel:.4f}"
                                )
                                logger.info(
                                    f"Constantes: π={ConstantesFundamentais.PI:.5f}, e={ConstantesFundamentais.E:.5f}, φ={ConstantesFundamentais.PHI:.5f}, ℏ={ConstantesFundamentais.HBAR:.2e}, α={ConstantesFundamentais.ALPHA:.5f}, R∞={ConstantesFundamentais.RYDBERG:.2f}"
                                )
                            try:
                                tempo_inicio = time.time()
                                # Criar e treinar VQC
                                vqc = ClassificadorVQC(
                                    n_qubits=4,
                                    n_camadas=2,
                                    arquitetura=arq,
                                    estrategia_init=init,
                                    tipo_ruido=ruido,
                                    nivel_ruido=nivel,
                                    taxa_aprendizado=0.01,
                                    n_epocas=n_epocas,
                                    batch_size=32,
                                    seed=seed,
                                    # Insights: ruído com annealing quando nível > 0
                                    ruido_schedule=('cosine' if (ruido != 'sem_ruido' and nivel > 0) else None),
                                    ruido_inicial=(nivel if (ruido != 'sem_ruido' and nivel > 0) else None),
                                    ruido_final=(0.001 if (ruido != 'sem_ruido' and nivel > 0) else None),
                                    # Early stopping leve para acelerar
                                    early_stopping=True, patience=5, min_delta=1e-3, val_split=0.1
                                )
                                vqc.fit(dataset['X_train'], dataset['y_train'])
                                tempo_total = time.time() - tempo_inicio
                                # Calcular métricas
                                acuracia_treino = vqc.score(dataset['X_train'], dataset['y_train'])
                                acuracia_teste = vqc.score(dataset['X_test'], dataset['y_test'])
                                gap_treino_teste = acuracia_treino - acuracia_teste
                                # Matriz de confusão
                                y_pred = vqc.predict(dataset['X_test'])
                                cm = confusion_matrix(dataset['y_test'], y_pred)
                                # Armazenar resultados
                                resultado = {
                                    'dataset': nome_dataset,
                                    'arquitetura': arq,
                                    'estrategia_init': init,
                                    'tipo_ruido': ruido,
                                    'nivel_ruido': nivel,
                                    'n_qubits': 4,
                                    'n_camadas': 2,
                                    'acuracia_treino': acuracia_treino,
                                    'acuracia_teste': acuracia_teste,
                                    'gap_treino_teste': gap_treino_teste,
                                    'tempo_segundos': tempo_total,
                                    'custo_final': vqc.historico_['custo'][-1],
                                    'cm_tn': cm[0,0],
                                    'cm_fp': cm[0,1],
                                    'cm_fn': cm[1,0],
                                    'cm_tp': cm[1,1],
                                    'seed': seed
                                }
                                resultados.append(resultado)
                                # Salvar cada experimento individualmente em CSV
                                if pasta_individual is not None:
                                    id_exp = f"exp_{contador:05d}"
                                    df_exp = pd.DataFrame([resultado])
                                    csv_exp_path = os.path.join(pasta_individual, f"{id_exp}.csv")
                                    df_exp.to_csv(csv_exp_path, index=False)
                                # Bloco duplicado removido (já salvo em resultado)
                                if verbose:
                                    logger.info(
                                        f"  ✓ Acurácia: {acuracia_teste:.4f} | "
                                        f"Gap: {gap_treino_teste:+.4f} | "
                                        f"Tempo: {tempo_total:.1f}s"
                                    )
                                
                                # Salvar circuito desenhado em PNG
                                if pasta_resultados is not None:
                                    try:
                                        import pennylane as qml
                                        import matplotlib
                                        matplotlib.use('Agg')
                                        import matplotlib.pyplot as plt
                                        
                                        # Criar pasta para circuitos se não existir
                                        pasta_circuitos = os.path.join(pasta_resultados, "circuitos")
                                        os.makedirs(pasta_circuitos, exist_ok=True)
                                        
                                        # Desenhar circuito usando qml.draw_mpl
                                        fig_circ, ax_circ = qml.draw_mpl(vqc.qnode_, decimals=2)(
                                            vqc.weights_, 
                                            dataset['X_train'][0], 
                                            None  # nivel_ruido_runtime
                                        )
                                        
                                        circ_png_filename = f"circuito_{nome_dataset}_seed{seed}_{arq}_{init}_{ruido}_nivel{nivel:.4f}.png"
                                        circ_png_path = os.path.join(pasta_circuitos, circ_png_filename)
                                        plt.savefig(circ_png_path, dpi=150, bbox_inches='tight', facecolor='white')
                                        try:
                                            import matplotlib.pyplot as _plt
                                            # Alguns stubs reportam tipo impreciso para draw_mpl; force close seguro
                                            try:
                                                from matplotlib.figure import Figure as _MplFigure
                                            except Exception:
                                                _MplFigure = object  # type: ignore[assignment]
                                            if isinstance(fig_circ, _MplFigure):
                                                _plt.close(fig_circ)  # type: ignore[arg-type]
                                            else:
                                                _plt.close('all')
                                        except Exception:
                                            try:
                                                plt.close('all')
                                            except Exception:
                                                pass
                                        
                                        if verbose:
                                            logger.info(f"    → Circuito salvo: {circ_png_filename}")
                                    except Exception as e:
                                        logger.warning(f"Falha ao salvar PNG do circuito: {e}")
                                    
                                    # Salvar gráfico 3D de gradientes (Barren Plateaus)
                                    if 'variancia_gradiente' in vqc.historico_ and len(vqc.historico_['variancia_gradiente']) > 0:
                                        try:
                                            import matplotlib
                                            matplotlib.use('Agg')
                                            import matplotlib.pyplot as plt
                                            import numpy as np

                                            # Criar pasta para barren plateaus se não existir
                                            pasta_barren = os.path.join(pasta_resultados, "barren_plateaus")
                                            os.makedirs(pasta_barren, exist_ok=True)

                                            epocas = vqc.historico_.get('epoca', [])
                                            variancias = vqc.historico_.get('variancia_gradiente', [])
                                            custos = vqc.historico_.get('custo', [])

                                            # Se não houver dados de época, gerar sequência simples
                                            if not epocas or len(epocas) != len(variancias):
                                                epocas = list(range(1, len(variancias)+1))

                                            if len(epocas) == len(variancias) == len(custos) and len(epocas) > 0:
                                                fig = plt.figure(figsize=(10, 8))
                                                ax = fig.add_subplot(111, projection='3d')

                                                # Garantir dtype float para compatibilidade com matplotlib
                                                X = np.array(epocas, dtype=float)
                                                Y = np.array(variancias, dtype=float)
                                                Z = np.array(custos, dtype=float)

                                                scatter = ax.scatter(xs=X, ys=Y, zs=Z, c=Z, cmap='viridis', s=50, alpha=0.8)  # type: ignore[call-arg]

                                                ax.set_title(
                                                    f'Barren Plateau Analysis\n{arq} | {init} | {ruido} (γ={nivel:.4f})',
                                                    fontsize=14, fontfamily='serif'
                                                )
                                                ax.set_xlabel('Época', fontsize=12, fontfamily='serif')
                                                ax.set_ylabel('Var(Gradiente)', fontsize=12, fontfamily='serif')
                                                ax.set_zlabel('Custo', fontsize=12, fontfamily='serif')

                                                cbar = plt.colorbar(scatter, ax=ax, label='Custo', shrink=0.8, pad=0.1)
                                                cbar.ax.tick_params(labelsize=10)

                                                plt.tight_layout()

                                                barren_filename = f"barren3d_{nome_dataset}_seed{seed}_{arq}_{init}_{ruido}_nivel{nivel:.4f}.png"
                                                barren_path = os.path.join(pasta_barren, barren_filename)
                                                plt.savefig(barren_path, dpi=150, bbox_inches='tight', facecolor='white')
                                                # Fechamento explícito da figura para evitar warning de tipo do Pylance
                                                try:
                                                    import matplotlib.pyplot as _plt
                                                    _plt.close(fig)
                                                except Exception:
                                                    try:
                                                        plt.close('all')
                                                    except Exception:
                                                        pass
                                                if verbose:
                                                    logger.info(f"    → Barren plateau 3D salvo: {barren_filename}")
                                            else:
                                                logger.warning("Dados insuficientes ou incompatíveis para gerar gráfico 3D dos gradientes.")
                                        except Exception as e:
                                            logger.warning(f"Falha ao salvar gráfico 3D barren plateau: {e}")
                                
                            except Exception as e:
                                if verbose:
                                    logger.warning(f"  ✗ Erro: {str(e)[:50]}")
    
    total_configs = (len(grid['arquitetura']) * len(grid['estrategia_init']) * 
                    len(grid['tipo_ruido']) * len(grid['nivel_ruido']))
    
    if verbose:
        logger.info(f"Total de configurações: {total_configs} por dataset")
        logger.info(f"Total geral: {total_configs * len(datasets)} experimentos\n")
    
    contador = 0
    
    # Iterar sobre datasets
    for nome_dataset, dataset in datasets.items():
        if verbose:
            logger.info(f"\n{'='*80}")
            logger.info(f" DATASET: {nome_dataset.upper()}")
            logger.info(f" {dataset['descricao']}")
            logger.info(f"{'='*80}\n")
        # Iterar sobre grid
        for arq in grid['arquitetura']:
            for init in grid['estrategia_init']:
                for ruido in grid['tipo_ruido']:
                    for nivel in grid['nivel_ruido']:
                        # Pular combinações inválidas (sem_ruido com nível > 0)
                        if ruido == 'sem_ruido' and nivel > 0:
                            continue
                        for seed in seed_list:
                            contador += 1
                            # Log detalhado de todos os parâmetros
                            if verbose:
                                logger.info(
                                    f"[{contador:3d}/{total_configs * n_seeds}] "
                                    f"Dataset: {nome_dataset} | Seed: {seed} | Qubits: 4 | Camadas: 2 | "
                                    f"Arquitetura: {arq} | Init: {init} | Ruído: {ruido} | Nível: {nivel:.4f}"
                                )
                                logger.info(
                                    f"Constantes: π={ConstantesFundamentais.PI:.5f}, e={ConstantesFundamentais.E:.5f}, φ={ConstantesFundamentais.PHI:.5f}, ℏ={ConstantesFundamentais.HBAR:.2e}, α={ConstantesFundamentais.ALPHA:.5f}, R∞={ConstantesFundamentais.RYDBERG:.2f}"
                                )
                            try:
                                tempo_inicio = time.time()
                                # Criar e treinar VQC
                                vqc = ClassificadorVQC(
                                    n_qubits=4,
                                    n_camadas=2,
                                    arquitetura=arq,
                                    estrategia_init=init,
                                    tipo_ruido=ruido,
                                    nivel_ruido=nivel,
                                    taxa_aprendizado=0.01,
                                    n_epocas=n_epocas,
                                    batch_size=32,
                                    seed=seed,
                                    # Insights: ruído com annealing quando nível > 0
                                    ruido_schedule=('cosine' if (ruido != 'sem_ruido' and nivel > 0) else None),
                                    ruido_inicial=(nivel if (ruido != 'sem_ruido' and nivel > 0) else None),
                                    ruido_final=(0.001 if (ruido != 'sem_ruido' and nivel > 0) else None),
                                    # Early stopping leve para acelerar
                                    early_stopping=True, patience=5, min_delta=1e-3, val_split=0.1
                                )
                                vqc.fit(dataset['X_train'], dataset['y_train'])
                                tempo_total = time.time() - tempo_inicio
                                # Calcular métricas
                                acuracia_treino = vqc.score(dataset['X_train'], dataset['y_train'])
                                acuracia_teste = vqc.score(dataset['X_test'], dataset['y_test'])
                                gap_treino_teste = acuracia_treino - acuracia_teste
                                # Matriz de confusão
                                y_pred = vqc.predict(dataset['X_test'])
                                cm = confusion_matrix(dataset['y_test'], y_pred)
                                # Armazenar resultados
                                resultados.append({
                                    'dataset': nome_dataset,
                                    'arquitetura': arq,
                                    'estrategia_init': init,
                                    'tipo_ruido': ruido,
                                    'nivel_ruido': nivel,
                                    'n_qubits': 4,
                                    'n_camadas': 2,
                                    'acuracia_treino': acuracia_treino,
                                    'acuracia_teste': acuracia_teste,
                                    'gap_treino_teste': gap_treino_teste,
                                    'tempo_segundos': tempo_total,
                                    'custo_final': vqc.historico_['custo'][-1],
                                    'cm_tn': cm[0,0],
                                    'cm_fp': cm[0,1],
                                    'cm_fn': cm[1,0],
                                    'cm_tp': cm[1,1],
                                    'seed': seed
                                })
                                if verbose:
                                    logger.info(
                                        f"  ✓ Acurácia: {acuracia_teste:.4f} | "
                                        f"Gap: {gap_treino_teste:+.4f} | "
                                        f"Tempo: {tempo_total:.1f}s"
                                    )
                            except Exception as e:
                                if verbose:
                                    logger.warning(f"  ✗ Erro: {str(e)[:50]}")
    
    # Adicionar baselines clássicos (SVM e Random Forest)
    for nome_dataset, dataset in datasets.items():
        if verbose:
            logger.info(f"\n{'='*80}")
            logger.info(f" BASELINES CLÁSSICOS: {nome_dataset.upper()}")
            logger.info(f"{'='*80}")
        # SVM (RBF)
        try:
            clf_svm = SVC(kernel='rbf', probability=True, random_state=42)
            clf_svm.fit(dataset['X_train'], dataset['y_train'])
            acuracia_treino = clf_svm.score(dataset['X_train'], dataset['y_train'])
            acuracia_teste = clf_svm.score(dataset['X_test'], dataset['y_test'])
            y_pred = clf_svm.predict(dataset['X_test'])
            cm = confusion_matrix(dataset['y_test'], y_pred)
            resultados.append({
                'dataset': nome_dataset,
                'arquitetura': 'SVM',
                'estrategia_init': '-',
                'tipo_ruido': 'classico',
                'nivel_ruido': 0.0,
                'n_qubits': 0,
                'n_camadas': 0,
                'acuracia_treino': acuracia_treino,
                'acuracia_teste': acuracia_teste,
                'gap_treino_teste': acuracia_treino - acuracia_teste,
                'tempo_segundos': 0.0,
                'custo_final': 0.0,
                'cm_tn': cm[0,0],
                'cm_fp': cm[0,1],
                'cm_fn': cm[1,0],
                'cm_tp': cm[1,1],
                'seed': 42
            })
            if verbose:
                logger.info(f"  ✓ SVM (RBF): Acurácia teste = {acuracia_teste:.4f}")
        except Exception as e:
            if verbose:
                logger.warning(f"  ✗ Erro SVM: {str(e)[:50]}")
        # Random Forest
        try:
            clf_rf = RandomForestClassifier(n_estimators=100, random_state=42)
            clf_rf.fit(dataset['X_train'], dataset['y_train'])
            acuracia_treino = clf_rf.score(dataset['X_train'], dataset['y_train'])
            acuracia_teste = clf_rf.score(dataset['X_test'], dataset['y_test'])
            y_pred = clf_rf.predict(dataset['X_test'])
            cm = confusion_matrix(dataset['y_test'], y_pred)
            resultados.append({
                'dataset': nome_dataset,
                'arquitetura': 'RandomForest',
                'estrategia_init': '-',
                'tipo_ruido': 'classico',
                'nivel_ruido': 0.0,
                'n_qubits': 0,
                'n_camadas': 0,
                'acuracia_treino': acuracia_treino,
                'acuracia_teste': acuracia_teste,
                'gap_treino_teste': acuracia_treino - acuracia_teste,
                'tempo_segundos': 0.0,
                'custo_final': 0.0,
                'cm_tn': cm[0,0],
                'cm_fp': cm[0,1],
                'cm_fn': cm[1,0],
                'cm_tp': cm[1,1],
                'seed': 42
            })
            if verbose:
                logger.info(f"  ✓ Random Forest: Acurácia teste = {acuracia_teste:.4f}")
        except Exception as e:
            if verbose:
                logger.warning(f"  ✗ Erro RF: {str(e)[:50]}")

    df_resultados = pd.DataFrame(resultados)
    # Salvar CSV consolidado e atualizar metadata
    if pasta_resultados is not None:
        csv_path = os.path.join(pasta_resultados, 'resultados_completos_artigo.csv')
        df_resultados.to_csv(csv_path, index=False)
        # Adicionar granularidade máxima ao metadata
        metadata['csv_consolidado'] = csv_path
        metadata['csvs_individuais'] = [os.path.join('experimentos_individuais', f) for f in os.listdir(pasta_individual) if f.endswith('.csv')]
        # Atualizar lista de arquivos
        metadata['arquivos_gerados'] = [f for f in os.listdir(pasta_resultados) if os.path.isfile(os.path.join(pasta_resultados, f))]
        # Salvar metadata.json
        if metadata_path:
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False, default=str)
    if verbose:
        logger.info(f"\n{'='*80}")
        logger.info(f" ✓ GRID SEARCH CONCLUÍDO: {len(df_resultados)} experimentos")
        logger.info(f"{'='*80}\n")
    return df_resultados


# ============================================================================
# MÓDULO 7: ANÁLISES ESTATÍSTICAS
# ============================================================================

def executar_analises_estatisticas(df, verbose=True, pasta_resultados=None):
    """Executa análises estatísticas principais do artigo."""
    import os
    
    # Inicializar metadata
    analise_meta: dict[str, Any] = {
        'tipo': 'analises_estatisticas',
        'data_execucao': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'arquivos_gerados': [],
        'csvs': {}
    }
    metadata_path: Optional[str] = None
    pasta_individual: Optional[str] = None
    
    if pasta_resultados is not None:
        pasta_individual = os.path.join(pasta_resultados, 'analises_individuais')
        os.makedirs(pasta_individual, exist_ok=True)
        os.makedirs(pasta_resultados, exist_ok=True)
        # Forense: README e metadata
        readme_path = os.path.join(pasta_resultados, 'README_analises_estatisticas.md')
        metadata_path = os.path.join(pasta_resultados, 'metadata_analises_estatisticas.json')
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(
                "# Análises Estatísticas\n\n"
                f"- Data de execução: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                "- Conteúdo: ANOVA, comparação de inicializações, overfitting, effect sizes, post-hoc.\n"
            )
    
    if verbose:
        logger.info("="*80)
        logger.info(" ANÁLISES ESTATÍSTICAS")
        logger.info("="*80)
    
    analises = {}
    
    # 1. ANOVA 2-way: Noise × Dataset
    if verbose:
        logger.info("\n1. ANOVA 2-WAY: Noise Level × Dataset")
        logger.info("-"*80)
    
    # Verificar se há dados
    if len(df) == 0:
        logger.warning("Nenhum resultado disponível para análise!")
        return {'erro': 'Sem dados'}
    
    df_anova = df.copy()
    df_anova['nivel_ruido_cat'] = df_anova['nivel_ruido'].astype(str)
    
    # Proteger: ANOVA 2-way requer pelo menos 2 níveis em cada fator
    if df_anova['nivel_ruido_cat'].nunique() >= 2 and df_anova['dataset'].nunique() >= 2:
        model = ols('acuracia_teste ~ C(nivel_ruido_cat) + C(dataset) + C(nivel_ruido_cat):C(dataset)', 
                    data=df_anova).fit()
        anova_2way = anova_lm(model, typ=2)
        analises['anova_2way'] = anova_2way
        if verbose:
            print(anova_2way)
    else:
        analises['anova_2way'] = 'Insuficiente para ANOVA 2-way (>=2 níveis por fator)'
        if verbose:
            logger.info('Dados insuficientes para ANOVA 2-way, análise pulada.')
    
    # 2. Comparação de inicializações
    if verbose:
        logger.info("\n2. COMPARAÇÃO DE INICIALIZAÇÕES")
        logger.info("-"*80)
    
    comp_init = df.groupby('estrategia_init').agg({
        'acuracia_teste': ['mean', 'std'],
        'tempo_segundos': 'mean'
    }).round(4)
    
    if verbose:
        print(comp_init)
    # Salvar CSV resumo de inicializações
    if pasta_resultados is not None:
        comp_init_path = os.path.join(pasta_resultados, 'analise_comparacao_inicializacoes.csv')
        try:
            comp_init.to_csv(comp_init_path)
            analise_meta['csvs']['comparacao_inicializacoes'] = comp_init_path
        except Exception:
            pass

    # 2b. Comparação consolidada: VQC vs Baselines Clássicos (SVM/RF)
    try:
        if verbose:
            logger.info("\n2b. COMPARAÇÃO: VQC vs SVM/RF (por dataset)")
            logger.info("-"*80)
        df_q = df[df['tipo_ruido'] != 'classico']
        df_class = df[df['tipo_ruido'] == 'classico']
        # Melhor VQC por dataset (maior acurácia)
        vqc_best = df_q.groupby('dataset')['acuracia_teste'].max().rename('vqc_melhor')
        # VQC sem ruído (média por dataset)
        vqc_sem = df[df['tipo_ruido'] == 'sem_ruido'].groupby('dataset')['acuracia_teste'].mean().rename('vqc_sem_ruido_media')
        # Baselines
        svm = (
            df_class[df_class['arquitetura'] == 'SVM']
            .groupby('dataset')['acuracia_teste']
            .mean()
            .rename('svm')
        )
        rf = (
            df_class[df_class['arquitetura'] == 'RandomForest']
            .groupby('dataset')['acuracia_teste']
            .mean()
            .rename('rf')
        )
        comp = pd.concat([vqc_best, vqc_sem, svm, rf], axis=1)
        # Deltas (podem ser NaN se baseline ausente)
        comp['delta_vqc_svm'] = comp['vqc_melhor'] - comp['svm']
        comp['delta_vqc_rf'] = comp['vqc_melhor'] - comp['rf']
        comp = comp.reset_index()
        if verbose:
            logger.info("Resumo por dataset:")
            logger.info(comp.round(4).to_string(index=False))
        if pasta_resultados is not None:
            comp_path = os.path.join(pasta_resultados, 'comparacao_baselines.csv')
            try:
                comp.to_csv(comp_path, index=False)
                analise_meta['csvs']['comparacao_baselines'] = comp_path
            except Exception:
                pass
    except Exception as e:
        if verbose:
            logger.warning(f"Falha ao gerar comparacao_baselines.csv: {str(e)[:80]}")
    # Salvar DataFrame completo das análises estatísticas
    try:
        df.to_csv(os.path.join(str(pasta_resultados), 'analises_estatisticas_completo.csv'), index=False)
        analise_meta['csvs']['completo'] = os.path.join(str(pasta_resultados), 'analises_estatisticas_completo.csv')
        # Salvar cada análise individualmente em CSV
        if pasta_individual is not None:
            for idx, row in df.iterrows():
                id_analise = f"analise_{idx:05d}"
                df_row = pd.DataFrame([row])
                csv_analise_path = os.path.join(str(pasta_individual), f"{id_analise}.csv")
                df_row.to_csv(csv_analise_path, index=False)
            # Listar CSVs apenas se pasta_individual é válido
            analise_meta['csvs_individuais'] = [os.path.join('analises_individuais', f) for f in os.listdir(str(pasta_individual)) if f.endswith('.csv')]
    except Exception:
        pass
    
    analises['comparacao_inicializacoes'] = comp_init
    
    # 3. Análise de overfitting
    if verbose:
        logger.info("\n3. ANÁLISE DE OVERFITTING")
        logger.info("-"*80)
    
    gap_sem_ruido = df[df['tipo_ruido'] == 'sem_ruido']['gap_treino_teste'].mean()
    mask_otimo = (df['tipo_ruido'] == 'depolarizante') & (df['nivel_ruido'] == 0.01)
    gap_com_ruido = df[mask_otimo]['gap_treino_teste'].mean()
    reducao_overfitting = ((gap_sem_ruido - gap_com_ruido) / gap_sem_ruido) * 100
    
    if verbose:
        logger.info(f"Gap sem ruído: {gap_sem_ruido:.4f}")
        logger.info(f"Gap com ruído ótimo: {gap_com_ruido:.4f}")
        logger.info(f"Redução de overfitting: {reducao_overfitting:.1f}%")
    
    analises['overfitting'] = {
        'gap_sem_ruido': gap_sem_ruido,
        'gap_com_ruido': gap_com_ruido,
        'reducao_percent': reducao_overfitting
    }
    
    # 4. Effect Sizes (Cohen's d, Glass's Δ, Hedges' g)
    if verbose:
        logger.info("\n4. EFFECT SIZES")
        logger.info("-"*80)
    
    sem_ruido = df[df['tipo_ruido'] == 'sem_ruido']['acuracia_teste'].values
    com_ruido = df[df['tipo_ruido'] != 'sem_ruido']['acuracia_teste'].values
    
    if len(sem_ruido) > 0 and len(com_ruido) > 0:
        cohen_d = TestesEstatisticosAvancados.cohen_d(com_ruido, sem_ruido)
        glass_delta = TestesEstatisticosAvancados.glass_delta(com_ruido, sem_ruido)
        hedges_g = TestesEstatisticosAvancados.hedges_g(com_ruido, sem_ruido)
        
        if verbose:
            logger.info(f"Cohen's d: {cohen_d:.4f}")
            logger.info(f"Glass's Δ: {glass_delta:.4f}")
            logger.info(f"Hedges' g: {hedges_g:.4f}")
        
        analises['effect_sizes'] = {
            'cohen_d': cohen_d,
            'glass_delta': glass_delta,
            'hedges_g': hedges_g
        }
    
    # 5. Testes Post-hoc (Bonferroni)
    if verbose:
        logger.info("\n5. TESTES POST-HOC")
        logger.info("-"*80)
    
    # Comparar cada tipo de ruído vs. baseline
    tipos_ruido = df['tipo_ruido'].unique()
    p_values = []
    
    for tipo in tipos_ruido:
        if tipo != 'sem_ruido':
            grupo1 = df[df['tipo_ruido'] == tipo]['acuracia_teste'].values
            grupo2 = df[df['tipo_ruido'] == 'sem_ruido']['acuracia_teste'].values
            
            if len(grupo1) > 0 and len(grupo2) > 0:
                _, p_val = ttest_ind(grupo1, grupo2)
                p_values.append((tipo, p_val))
    
    if len(p_values) > 0:
        # Correção de Bonferroni
        p_vals_only = [p for _, p in p_values]
        significantes = TestesEstatisticosAvancados.bonferroni(p_vals_only, alpha=0.05)
        
        if verbose:
            for (tipo, p_val), sig in zip(p_values, significantes):
                status = "✓ Significativo" if sig else "✗ Não significativo"
                logger.info(f"{tipo:20s}: p={p_val:.4f} {status}")
        
        analises['posthoc_bonferroni'] = list(zip(p_values, significantes))
    
    # Persistir metadata
    if pasta_resultados is not None and metadata_path is not None:
        # Atualizar lista de arquivos gerados no diretório
        try:
            analise_meta['arquivos_gerados'] = [f for f in os.listdir(pasta_resultados) if os.path.isfile(os.path.join(pasta_resultados, f))]
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(analise_meta, f, indent=2, ensure_ascii=False, default=str)
        except Exception:
            pass
    return analises


# ============================================================================
# MÓDULO 8: VISUALIZAÇÕES
# ============================================================================

def gerar_visualizacoes(df, salvar=True, pasta_resultados=None):
    """Gera as figuras principais do artigo."""
    import os
    
    # Inicializar metadata
    viz_meta: dict[str, Any] = {
        'tipo': 'visualizacoes',
        'data_execucao': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'arquivos_gerados': [],
        'figuras': []
    }
    metadata_path: Optional[str] = None
    pasta_individual: Optional[str] = None
    
    if pasta_resultados is not None:
        pasta_individual = os.path.join(pasta_resultados, 'visualizacoes_individuais')
        os.makedirs(pasta_individual, exist_ok=True)
        os.makedirs(pasta_resultados, exist_ok=True)
        # Forense: README e metadata
        readme_path = os.path.join(pasta_resultados, 'README_visualizacoes.md')
        metadata_path = os.path.join(pasta_resultados, 'metadata_visualizacoes.json')
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(
                "# Visualizações Geradas\n\n"
                f"- Data de execução: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                "- Figuras: 2, 2b, 3, 3b, 4, 5, 6, 7\n"
            )
    
    logger.info("="*80)
    logger.info(" GERANDO VISUALIZAÇÕES")
    logger.info("="*80)
    
    figuras = {}
    
    # FIGURA 2: Beneficial Noise (PRINCIPAL)
    logger.info("\nGerando Figura 2: Beneficial Noise...")
    
    fig2 = px.scatter(
        df, x='nivel_ruido', y='acuracia_teste', 
        color='tipo_ruido', facet_col='dataset',
        title="Figura 2: Impacto do Ruído Quântico na Acurácia",
        labels={'nivel_ruido': 'Nível de Ruído', 'acuracia_teste': 'Acurácia no Teste'},
        height=500
    )
    
    if salvar:
        # Qualis A1: aprimorar layout
        fig2.update_layout(
            font=dict(family='serif', size=18, color='black'),
            title_font=dict(size=22, family='serif', color='black', bold=True),
            legend_title_font=dict(size=18, family='serif', color='black', bold=True),
            legend_font=dict(size=16, family='serif', color='black'),
            margin=dict(l=60, r=40, t=80, b=60),
            paper_bgcolor='white',
            plot_bgcolor='white',
        )
        fig2.update_traces(marker=dict(line=dict(width=1, color='black')))
        fig2.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        fig2.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        # Exportar em alta resolução e formatos científicos
        path_html = os.path.join(pasta_resultados if pasta_resultados else '', 'figura2_beneficial_noise.html')
        fig2.write_html(path_html)
        if pasta_resultados is not None:
            viz_meta['figuras'].append(path_html)
            path_png = os.path.join(pasta_resultados, 'figura2_beneficial_noise.png')
            path_pdf = os.path.join(pasta_resultados, 'figura2_beneficial_noise.pdf')
            path_svg = os.path.join(pasta_resultados, 'figura2_beneficial_noise.svg')
            fig2.write_image(path_png, format='png', scale=3, width=1200, height=800)
            fig2.write_image(path_pdf, format='pdf', width=1200, height=800)
            fig2.write_image(path_svg, format='svg', width=1200, height=800)
            viz_meta['figuras'] += [path_png, path_pdf, path_svg]
    figuras['figura2'] = fig2

    # FIGURA 2b: Beneficial Noise com IC95% por grupo (dataset, tipo_ruido, nivel_ruido)
    logger.info("Gerando Figura 2b: Beneficial Noise com IC95%...")
    try:
        df_q = df[df['tipo_ruido'] != 'classico'].copy()
        grp_cols = ['dataset', 'tipo_ruido', 'nivel_ruido']
        df_ci = (
            df_q.groupby(grp_cols)
            .agg(media=('acuracia_teste', 'mean'), desvio=('acuracia_teste', 'std'), n=('acuracia_teste', 'count'))
            .reset_index()
        )
        # Evitar divisão por zero para n<=1
        df_ci['sem'] = df_ci.apply(lambda r: (r['desvio'] / np.sqrt(r['n'])) if r['n'] > 1 and r['desvio'] == r['desvio'] else 0.0, axis=1)
        df_ci['ci95'] = 1.96 * df_ci['sem']
        fig2b = px.scatter(
            df_ci, x='nivel_ruido', y='media', color='tipo_ruido', facet_col='dataset',
            error_y='ci95',
            title='Figura 2b: Acurácia Média ± IC95% por Nível de Ruído',
            labels={'nivel_ruido': 'Nível de Ruído', 'media': 'Acurácia Média (Teste)'},
            height=500
        )
        # Aparência consistente
        fig2b.update_layout(
            font=dict(family='serif', size=18, color='black'),
            title_font=dict(size=22, family='serif', color='black', bold=True),
            legend_title_font=dict(size=18, family='serif', color='black', bold=True),
            legend_font=dict(size=16, family='serif', color='black'),
            margin=dict(l=60, r=40, t=80, b=60),
            paper_bgcolor='white', plot_bgcolor='white',
        )
        fig2b.update_traces(marker=dict(line=dict(width=1, color='black')))
        fig2b.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        fig2b.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        if salvar:
            path_html = os.path.join(pasta_resultados if pasta_resultados else '', 'figura2b_beneficial_noise_ic95.html')
            fig2b.write_html(path_html)
            if pasta_resultados is not None:
                viz_meta['figuras'].append(path_html)
                path_png = os.path.join(pasta_resultados, 'figura2b_beneficial_noise_ic95.png')
                path_pdf = os.path.join(pasta_resultados, 'figura2b_beneficial_noise_ic95.pdf')
                path_svg = os.path.join(pasta_resultados, 'figura2b_beneficial_noise_ic95.svg')
                fig2b.write_image(path_png, format='png', scale=3, width=1200, height=800)
                fig2b.write_image(path_pdf, format='pdf', width=1200, height=800)
                fig2b.write_image(path_svg, format='svg', width=1200, height=800)
                viz_meta['figuras'] += [path_png, path_pdf, path_svg]
        figuras['figura2b'] = fig2b
    except Exception as e:
        logger.warning(f"Não foi possível gerar a Figura 2b (IC95%): {str(e)[:80]}")
    
    # FIGURA 3: Noise Type Comparison
    logger.info("Gerando Figura 3: Comparação de Tipos de Ruído...")
    
    fig3 = px.box(
        df, x='tipo_ruido', y='acuracia_teste', color='tipo_ruido',
        title="Figura 3: Comparação de Tipos de Ruído",
        labels={'tipo_ruido': 'Tipo de Ruído', 'acuracia_teste': 'Acurácia no Teste'},
        height=500
    )
    
    if salvar:
        fig3.update_layout(
            font=dict(family='serif', size=18, color='black'),
            title_font=dict(size=22, family='serif', color='black', bold=True),
            legend_title_font=dict(size=18, family='serif', color='black', bold=True),
            legend_font=dict(size=16, family='serif', color='black'),
            margin=dict(l=60, r=40, t=80, b=60),
            paper_bgcolor='white',
            plot_bgcolor='white',
        )
        fig3.update_traces(marker=dict(line=dict(width=1, color='black')))
        fig3.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        fig3.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        path_html = os.path.join(pasta_resultados if pasta_resultados else '', 'figura3_noise_types.html')
        fig3.write_html(path_html)
        if pasta_resultados is not None:
            viz_meta['figuras'].append(path_html)
            path_png = os.path.join(pasta_resultados, 'figura3_noise_types.png')
            path_pdf = os.path.join(pasta_resultados, 'figura3_noise_types.pdf')
            path_svg = os.path.join(pasta_resultados, 'figura3_noise_types.svg')
            fig3.write_image(path_png, format='png', scale=3, width=1200, height=800)
            fig3.write_image(path_pdf, format='pdf', width=1200, height=800)
            fig3.write_image(path_svg, format='svg', width=1200, height=800)
            viz_meta['figuras'] += [path_png, path_pdf, path_svg]
    figuras['figura3'] = fig3

    # FIGURA 3b: Médias por Tipo de Ruído com IC95% (facet por dataset)
    logger.info("Gerando Figura 3b: Tipos de Ruído com IC95%...")
    try:
        df_q2 = df[df['tipo_ruido'] != 'classico'].copy()
        grp_cols3 = ['dataset', 'tipo_ruido']
        df_ci3 = (
            df_q2.groupby(grp_cols3)
            .agg(media=('acuracia_teste', 'mean'), desvio=('acuracia_teste', 'std'), n=('acuracia_teste', 'count'))
            .reset_index()
        )
        df_ci3['sem'] = df_ci3.apply(lambda r: (r['desvio'] / np.sqrt(r['n'])) if r['n'] > 1 and r['desvio'] == r['desvio'] else 0.0, axis=1)
        df_ci3['ci95'] = 1.96 * df_ci3['sem']
        fig3b = px.bar(
            df_ci3, x='tipo_ruido', y='media', color='tipo_ruido', facet_col='dataset',
            error_y='ci95', barmode='group',
            title='Figura 3b: Acurácia Média ± IC95% por Tipo de Ruído',
            labels={'media': 'Acurácia Média (Teste)', 'tipo_ruido': 'Tipo de Ruído'}, height=500
        )
        if salvar:
            fig3b.update_layout(
                font=dict(family='serif', size=18, color='black'),
                title_font=dict(size=22, family='serif', color='black', bold=True),
                legend_title_font=dict(size=18, family='serif', color='black', bold=True),
                legend_font=dict(size=16, family='serif', color='black'),
                margin=dict(l=60, r=40, t=80, b=60),
                paper_bgcolor='white', plot_bgcolor='white',
            )
            path_html = os.path.join(pasta_resultados if pasta_resultados else '', 'figura3b_noise_types_ic95.html')
            fig3b.write_html(path_html)
            if pasta_resultados is not None:
                viz_meta['figuras'].append(path_html)
                path_png = os.path.join(pasta_resultados, 'figura3b_noise_types_ic95.png')
                path_pdf = os.path.join(pasta_resultados, 'figura3b_noise_types_ic95.pdf')
                path_svg = os.path.join(pasta_resultados, 'figura3b_noise_types_ic95.svg')
                fig3b.write_image(path_png, format='png', scale=3, width=1200, height=800)
                fig3b.write_image(path_pdf, format='pdf', width=1200, height=800)
                fig3b.write_image(path_svg, format='svg', width=1200, height=800)
                viz_meta['figuras'] += [path_png, path_pdf, path_svg]
        figuras['figura3b'] = fig3b
    except Exception as e:
        logger.warning(f"Não foi possível gerar a Figura 3b (IC95%): {str(e)[:80]}")
    
    # FIGURA 4: Initialization Strategies
    logger.info("Gerando Figura 4: Estratégias de Inicialização...")
    
    fig4 = px.box(
        df, x='estrategia_init', y='acuracia_teste', color='estrategia_init',
        title="Figura 4: Impacto da Estratégia de Inicialização",
        labels={'estrategia_init': 'Estratégia', 'acuracia_teste': 'Acurácia no Teste'},
        height=500
    )
    
    if salvar:
        fig4.update_layout(
            font=dict(family='serif', size=18, color='black'),
            title_font=dict(size=22, family='serif', color='black', bold=True),
            legend_title_font=dict(size=18, family='serif', color='black', bold=True),
            legend_font=dict(size=16, family='serif', color='black'),
            margin=dict(l=60, r=40, t=80, b=60),
            paper_bgcolor='white',
            plot_bgcolor='white',
        )
        fig4.update_traces(marker=dict(line=dict(width=1, color='black')))
        fig4.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        fig4.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        path_html = os.path.join(pasta_resultados if pasta_resultados else '', 'figura4_initialization.html')
        fig4.write_html(path_html)
        if pasta_resultados is not None:
            viz_meta['figuras'].append(path_html)
            path_png = os.path.join(pasta_resultados, 'figura4_initialization.png')
            path_pdf = os.path.join(pasta_resultados, 'figura4_initialization.pdf')
            path_svg = os.path.join(pasta_resultados, 'figura4_initialization.svg')
            fig4.write_image(path_png, format='png', scale=3, width=1200, height=800)
            fig4.write_image(path_pdf, format='pdf', width=1200, height=800)
            fig4.write_image(path_svg, format='svg', width=1200, height=800)
            viz_meta['figuras'] += [path_png, path_pdf, path_svg]
    figuras['figura4'] = fig4
    
    # FIGURA 5: Architecture Trade-offs
    logger.info("Gerando Figura 5: Trade-offs de Arquitetura...")
    
    fig5 = px.scatter(
        df, x='tempo_segundos', y='acuracia_teste', color='arquitetura',
        title="Figura 5: Trade-off Tempo vs. Acurácia",
        labels={'tempo_segundos': 'Tempo (s)', 'acuracia_teste': 'Acurácia no Teste'},
        height=500
    )
    
    if salvar:
        fig5.update_layout(
            font=dict(family='serif', size=18, color='black'),
            title_font=dict(size=22, family='serif', color='black', bold=True),
            legend_title_font=dict(size=18, family='serif', color='black', bold=True),
            legend_font=dict(size=16, family='serif', color='black'),
            margin=dict(l=60, r=40, t=80, b=60),
            paper_bgcolor='white',
            plot_bgcolor='white',
        )
        fig5.update_traces(marker=dict(line=dict(width=1, color='black')))
        fig5.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        fig5.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        path_html = os.path.join(pasta_resultados if pasta_resultados else '', 'figura5_architecture_tradeoffs.html')
        fig5.write_html(path_html)
        if pasta_resultados is not None:
            viz_meta['figuras'].append(path_html)
            path_png = os.path.join(pasta_resultados, 'figura5_architecture_tradeoffs.png')
            path_pdf = os.path.join(pasta_resultados, 'figura5_architecture_tradeoffs.pdf')
            path_svg = os.path.join(pasta_resultados, 'figura5_architecture_tradeoffs.svg')
            fig5.write_image(path_png, format='png', scale=3, width=1200, height=800)
            fig5.write_image(path_pdf, format='pdf', width=1200, height=800)
            fig5.write_image(path_svg, format='svg', width=1200, height=800)
            viz_meta['figuras'] += [path_png, path_pdf, path_svg]
    figuras['figura5'] = fig5
    
    # FIGURA 7: Overfitting Analysis
    logger.info("Gerando Figura 7: Análise de Overfitting...")
    
    # Garantir que os tamanhos sejam positivos (usar valor absoluto)
    df_fig7 = df.copy()
    df_fig7['gap_abs'] = df_fig7['gap_treino_teste'].abs()
    
    fig7 = px.scatter(
        df_fig7, x='acuracia_treino', y='acuracia_teste', color='tipo_ruido',
        size='gap_abs', hover_data=['dataset', 'arquitetura', 'gap_treino_teste'],
        title="Figura 7: Análise de Overfitting (Gap Treino-Teste)",
        labels={'acuracia_treino': 'Acurácia Treino', 'acuracia_teste': 'Acurácia Teste'},
        height=500
    )
    
    # Adicionar linha diagonal (sem overfitting)
    fig7.add_trace(go.Scatter(
        x=[0, 1], y=[0, 1], mode='lines',
        line=dict(dash='dash', color='gray'),
        name='Sem Overfitting'
    ))
    
    if salvar:
        fig7.update_layout(
            font=dict(family='serif', size=18, color='black'),
            title_font=dict(size=22, family='serif', color='black', bold=True),
            legend_title_font=dict(size=18, family='serif', color='black', bold=True),
            legend_font=dict(size=16, family='serif', color='black'),
            margin=dict(l=60, r=40, t=80, b=60),
            paper_bgcolor='white',
            plot_bgcolor='white',
        )
        fig7.update_traces(marker=dict(line=dict(width=1, color='black')))
        fig7.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        fig7.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))
        path_html = os.path.join(pasta_resultados if pasta_resultados else '', 'figura7_overfitting.html')
        fig7.write_html(path_html)
        if pasta_resultados is not None:
            viz_meta['figuras'].append(path_html)
            path_png = os.path.join(pasta_resultados, 'figura7_overfitting.png')
            path_pdf = os.path.join(pasta_resultados, 'figura7_overfitting.pdf')
            path_svg = os.path.join(pasta_resultados, 'figura7_overfitting.svg')
            fig7.write_image(path_png, format='png', scale=3, width=1200, height=800)
            fig7.write_image(path_pdf, format='pdf', width=1200, height=800)
            fig7.write_image(path_svg, format='svg', width=1200, height=800)
            viz_meta['figuras'] += [path_png, path_pdf, path_svg]
    figuras['figura7'] = fig7
    
    # FIGURA 6: Effect Sizes Comparison
    logger.info("Gerando Figura 6: Comparação de Effect Sizes...")
    
    # Calcular effect sizes para cada par ruído vs. baseline
    effect_data = []
    for dataset in df['dataset'].unique():
        df_ds = df[df['dataset'] == dataset]
        sem_ruido = df_ds[df_ds['tipo_ruido'] == 'sem_ruido']['acuracia_teste'].values
        
        for tipo in df_ds['tipo_ruido'].unique():
            if tipo != 'sem_ruido':
                com_ruido = df_ds[df_ds['tipo_ruido'] == tipo]['acuracia_teste'].values
                
                if len(sem_ruido) > 0 and len(com_ruido) > 0:
                    cohen_d = TestesEstatisticosAvancados.cohen_d(com_ruido, sem_ruido)
                    glass_d = TestesEstatisticosAvancados.glass_delta(com_ruido, sem_ruido)
                    hedges_g = TestesEstatisticosAvancados.hedges_g(com_ruido, sem_ruido)
                    
                    effect_data.append({
                        'dataset': dataset,
                        'tipo_ruido': tipo,
                        "Cohen's d": cohen_d,
                        "Glass's Δ": glass_d,
                        "Hedges' g": hedges_g
                    })
    
    if len(effect_data) > 0:
        df_effects = pd.DataFrame(effect_data)
        df_effects_melted = df_effects.melt(
            id_vars=['dataset', 'tipo_ruido'],
            value_vars=["Cohen's d", "Glass's Δ", "Hedges' g"],
            var_name='Métrica',
            value_name='Effect Size'
        )
        
        fig6 = px.bar(
            df_effects_melted,
            x='tipo_ruido',
            y='Effect Size',
            color='Métrica',
            facet_col='dataset',
            barmode='group',
            title="Figura 6: Comparação de Effect Sizes (vs. Baseline)",
            height=500
        )
        
        # Adicionar linhas de referência (small/medium/large effects)
        # Usar getattr para evitar warnings de tipo com atributos dinâmicos do Plotly
        annotations = getattr(fig6.layout, 'annotations', None)
        if annotations:
            for annotation in annotations:
                annotation.text = annotation.text.replace('dataset=', '')
        
        if salvar:
            path = os.path.join(pasta_resultados if pasta_resultados else '', 'figura6_effect_sizes.html')
            fig6.write_html(path)
            if pasta_resultados is not None:
                viz_meta['figuras'].append(path)
        
        figuras['figura6'] = fig6
    
    logger.info(f"\n✓ {len(figuras)} figuras geradas!")
    # Persistir metadata
    if pasta_resultados is not None and metadata_path is not None:
        try:
            viz_meta['arquivos_gerados'] = [f for f in os.listdir(pasta_resultados) if os.path.isfile(os.path.join(pasta_resultados, f))]
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(viz_meta, f, indent=2, ensure_ascii=False, default=str)
                # Salvar DataFrame completo das visualizações
                df.to_csv(os.path.join(pasta_resultados, 'visualizacoes_completo.csv'), index=False)
                viz_meta['csv_completo'] = os.path.join(pasta_resultados, 'visualizacoes_completo.csv')
                # Salvar cada visualização individualmente em CSV
                if pasta_individual is not None:
                    for idx, row in df.iterrows():
                        id_vis = f"vis_{idx:05d}"
                        df_row = pd.DataFrame([row])
                        csv_vis_path = os.path.join(pasta_individual, f"{id_vis}.csv")
                        df_row.to_csv(csv_vis_path, index=False)
                    viz_meta['csvs_individuais'] = [os.path.join('visualizacoes_individuais', f) for f in os.listdir(pasta_individual) if f.endswith('.csv')]
        except Exception:
            pass
    
    return figuras


# ============================================================================
# MÓDULO: ANÁLISES ESTATÍSTICAS PROFUNDAS (v7.1)
# ============================================================================

def analise_correlacao_profunda(df: pd.DataFrame, save_path: str = 'figura_correlacao.html'):
    logger.info("\n" + "="*80)
    logger.info(" ANÁLISE DE CORRELAÇÃO")
    logger.info("="*80)
    
    # Selecionar colunas numéricas
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    corr_matrix = df[numeric_cols].corr()
    
    # Criar heatmap interativo
    fig = px.imshow(
        corr_matrix,
        labels=dict(color="Correlação"),
        x=corr_matrix.columns,
        y=corr_matrix.columns,
        color_continuous_scale='RdBu_r',
        zmin=-1, zmax=1,
        title="Matriz de Correlação - Todas as Variáveis",
        height=700, width=900
    )
    
    fig.write_html(save_path)
    logger.info(f"✓ Heatmap salvo: {save_path}")
    
    # Top correlações com acurácia
    if 'acuracia_teste' in corr_matrix.columns:
        logger.info("\nTop correlações com Acurácia de Teste:")
        corr_acc = corr_matrix['acuracia_teste'].abs().sort_values(ascending=False)
        for var, corr in corr_acc.head(8).items():
            if var != 'acuracia_teste':
                # Usar acesso direto ao valor para evitar warnings de tipo
                try:
                    actual_corr = float(corr_matrix.loc['acuracia_teste', var])  # type: ignore[call-overload]
                    logger.info(f"  {var:25s}: {actual_corr:+.4f}")
                except (KeyError, TypeError, ValueError):
                    pass
    
    return corr_matrix


def analise_pca_profunda(df: pd.DataFrame, save_path: Optional[str] = None):
    if not SKLEARN_ADVANCED_AVAILABLE:
        logger.warning("Scikit-learn PCA não disponível")
        return None, None
    
    logger.info("\n" + "="*80)
    logger.info(" ANÁLISE DE COMPONENTES PRINCIPAIS (PCA)")
    logger.info("="*80)
    
    # Preparar dados
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    X = df[numeric_cols].dropna()
    
    # Normalizar
    scaler = SklearnStandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # PCA
    pca = SklearnPCA()
    X_pca = pca.fit_transform(X_scaled)
    
    var_exp = pca.explained_variance_ratio_
    var_cum = np.cumsum(var_exp)
    
    logger.info(f"\nVariância explicada (primeiros 3 componentes): {var_cum[2]:.2%}")
    
    for i in range(min(5, len(var_exp))):
        logger.info(f"  PC{i+1}: {var_exp[i]:.2%} (acumulado: {var_cum[i]:.2%})")
    
    # Visualização: Scree plot + Biplot
    
    fig = make_subplots(
        rows=1, cols=2,
        subplot_titles=('Scree Plot', 'Biplot PC1 vs PC2'),
        specs=[[{'type': 'scatter'}, {'type': 'scatter'}]]
    )
    
    # Scree plot
    fig.add_trace(
        go.Scatter(x=list(range(1, len(var_exp)+1)), y=var_exp,
                   mode='lines+markers', name='Variância'),
        row=1, col=1
    )
    
    # Biplot
    df_plot = df.copy()
    df_plot['PC1'] = X_pca[:, 0]
    df_plot['PC2'] = X_pca[:, 1]
    
    for dataset in df_plot['dataset'].unique():
        df_ds = df_plot[df_plot['dataset'] == dataset]
        fig.add_trace(
            go.Scatter(x=df_ds['PC1'], y=df_ds['PC2'],
                      mode='markers', name=dataset),
            row=1, col=2
        )
    
    fig.update_layout(height=500, width=1200, title_text="Análise PCA")
    if save_path is not None:
        fig.write_html(save_path)
        logger.info(f"✓ PCA salvo: {save_path}")
    
    return pca, X_pca


def analise_clustering_profunda(df: pd.DataFrame, n_clusters: int = 3, save_path: Optional[str] = None):
    """Análise de clustering com K-means."""
    if not SKLEARN_ADVANCED_AVAILABLE:
        logger.warning("Scikit-learn KMeans não disponível")
        return None
    
    logger.info("\n" + "="*80)
    logger.info(f" ANÁLISE DE CLUSTERING (K-means, k={n_clusters})")
    logger.info("="*80)
    
    # Preparar dados
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    X = df[numeric_cols].fillna(0)
    
    # Normalizar
    scaler = SklearnStandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # K-means
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(X_scaled)
    
    # Adicionar ao dataframe
    df_analysis = df.copy()
    df_analysis['cluster'] = clusters
    
    # Analisar clusters
    logger.info("\nCaracterísticas dos Clusters:")
    for i in range(n_clusters):
        df_cluster = df_analysis[df_analysis['cluster'] == i]
        logger.info(f"\n  Cluster {i}: {len(df_cluster)} experimentos")
        
        if 'dataset' in df_cluster.columns:
            logger.info(f"    Datasets: {df_cluster['dataset'].value_counts().to_dict()}")
        
        if 'acuracia_teste' in df_cluster.columns:
            logger.info(f"    Acurácia média: {df_cluster['acuracia_teste'].mean():.3f}")
        
        if 'tempo_segundos' in df_cluster.columns:
            logger.info(f"    Tempo médio: {df_cluster['tempo_segundos'].mean():.1f}s")
    
    # Visualização em PCA space
    pca = SklearnPCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)
    
    df_viz = df_analysis.copy()
    df_viz['PC1'] = X_pca[:, 0]
    df_viz['PC2'] = X_pca[:, 1]
    
    fig = px.scatter(
        df_viz, x='PC1', y='PC2', color='cluster',
        title=f'Clustering K-means (k={n_clusters}) em Espaço PCA',
        labels={'cluster': 'Cluster'},
        height=600, width=800
    )
    
    # Adicionar centroides
    centroides_pca = pca.transform(kmeans.cluster_centers_)
    fig.add_trace(
        go.Scatter(
            x=centroides_pca[:, 0],
            y=centroides_pca[:, 1],
            mode='markers',
            marker=dict(size=15, symbol='x', color='black'),
            name='Centroides'
        )
    )
    
    if save_path is not None:
        fig.write_html(save_path)
        logger.info(f"\n✓ Clustering salvo: {save_path}")
    
    return kmeans


def analise_sensibilidade_profunda(df: pd.DataFrame, save_path: Optional[str] = None):
    """Análise de sensibilidade ao ruído."""
    logger.info("\n" + "="*80)
    logger.info(" ANÁLISE DE SENSIBILIDADE AO RUÍDO")
    logger.info("="*80)
    
    # Calcular sensibilidade (derivada acurácia/ruído)
    if 'nivel_ruido' in df.columns and 'acuracia_teste' in df.columns:
        df_ruido = df[df['nivel_ruido'] > 0].copy()
        
        # Sensibilidade média
        sensibilidade = df_ruido.groupby('nivel_ruido')['acuracia_teste'].mean().diff().mean()
        logger.info(f"\nSensibilidade média (Δacurácia/Δruído): {sensibilidade:+.4f}")
        
        # Visualização: Box plots por dataset
        fig = px.box(
            df_ruido, x='dataset', y='acuracia_teste', color='nivel_ruido',
            title='Sensibilidade da Acurácia ao Nível de Ruído',
            labels={'acuracia_teste': 'Acurácia', 'dataset': 'Dataset'},
            height=600, width=1000
        )
        
        if save_path is not None:
            fig.write_html(save_path)
            logger.info(f"✓ Análise de sensibilidade salva: {save_path}")
        
        return {'sensibilidade_media': sensibilidade}
    
    logger.warning("Colunas necessárias não encontradas")
    return {}


def executar_analises_profundas(df: pd.DataFrame, salvar_figuras: bool = True, pasta_resultados=None):
    """Executa análises profundas do artigo."""
    import os
    
    deep_meta: dict[str, Any] = {
        'tipo': 'analises_profundas',
        'data_execucao': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'arquivos_gerados': [],
        'artefatos': []
    }
    metadata_path: Optional[str] = None
    if pasta_resultados is not None:
        os.makedirs(pasta_resultados, exist_ok=True)
        # Forense: README e metadata
        readme_path = os.path.join(pasta_resultados, 'README_analises_profundas.md')
        metadata_path = os.path.join(pasta_resultados, 'metadata_analises_profundas.json')
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(
                "# Análises Estatísticas Profundas (v7.1)\n\n"
                f"- Data de execução: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                "- Conteúdo: Correlação, PCA, Clustering, Sensibilidade.\n"
            )
        # deep_meta já inicializado
    logger.info("\n" + "="*80)
    logger.info(" EXECUTANDO ANÁLISES ESTATÍSTICAS PROFUNDAS (v7.1)")
    logger.info("="*80)
    
    resultados = {}
    
    # 1. Correlação
    if salvar_figuras:
        path_corr = os.path.join(pasta_resultados if pasta_resultados else '', 'figura_correlacao.html')
        corr_matrix = analise_correlacao_profunda(df, save_path=path_corr)
        if pasta_resultados is not None:
            deep_meta['artefatos'].append(path_corr)
        resultados['correlacao'] = corr_matrix

    # 2. PCA
    if salvar_figuras and SKLEARN_ADVANCED_AVAILABLE:
        path_pca = os.path.join(pasta_resultados if pasta_resultados else '', 'figura_pca.html')
        pca, X_pca = analise_pca_profunda(df, save_path=path_pca)
        if pasta_resultados is not None:
            deep_meta['artefatos'].append(path_pca)
        resultados['pca'] = {'model': pca, 'components': X_pca}

    # 3. Clustering
    if salvar_figuras and SKLEARN_ADVANCED_AVAILABLE:
        path_clust = os.path.join(pasta_resultados if pasta_resultados else '', 'figura_clustering.html')
        kmeans = analise_clustering_profunda(df, n_clusters=3, save_path=path_clust)
        if pasta_resultados is not None:
            deep_meta['artefatos'].append(path_clust)
        resultados['clustering'] = kmeans

    # 4. Sensibilidade
    if salvar_figuras:
        path_sens = os.path.join(pasta_resultados if pasta_resultados else '', 'figura_sensibilidade.html')
        sens = analise_sensibilidade_profunda(df, save_path=path_sens)
        if pasta_resultados is not None:
            deep_meta['artefatos'].append(path_sens)
        resultados['sensibilidade'] = sens
    
    logger.info("\n" + "="*80)
    logger.info(" ANÁLISES PROFUNDAS CONCLUÍDAS!")
    logger.info("="*80)
    
    # Persistir metadata
    if pasta_resultados is not None and metadata_path is not None:
        try:
            deep_meta['arquivos_gerados'] = [f for f in os.listdir(pasta_resultados) if os.path.isfile(os.path.join(pasta_resultados, f))]
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(deep_meta, f, indent=2, ensure_ascii=False, default=str)
        except Exception:
            pass
    return resultados


# ============================================================================
# SCRIPT PRINCIPAL
# ============================================================================

# ===============================
# Utilitário interno de validação
# ===============================
def validar_exportacao_circuito(pasta_resultados: Optional[str] = None) -> str:
    """
    Gera uma imagem PNG de um circuito quântico simples usando o QNode persistido (qnode_)
    para validar a exportação de diagramas via qml.draw_mpl. Salva em pasta_resultados/circuitos
    quando fornecido; caso contrário, salva em ./test_outputs.

    Returns:
        Caminho absoluto do arquivo PNG gerado.
    """
    import numpy as np
    import pennylane as qml
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    import os

    # Diretório de saída
    if pasta_resultados:
        out_dir = os.path.join(pasta_resultados, 'circuitos')
    else:
        out_dir = os.path.join(os.getcwd(), 'test_outputs')
    os.makedirs(out_dir, exist_ok=True)

    # Modelo mínimo
    vqc = ClassificadorVQC(
        n_qubits=4,
        n_camadas=2,
        arquitetura='basic_entangler',  # usar chave válida de ARQUITETURAS
        detectar_barren=False,
        n_epocas=1,
        seed=42,
    )
    vqc._criar_circuito()

    # Entrada dummy
    x = np.zeros(vqc.n_qubits)

    # Desenhar e salvar
    fig, ax = qml.draw_mpl(vqc.qnode_, decimals=2)(vqc.weights_, x, None)
    out_path = os.path.join(out_dir, 'circuito_validacao.png')
    plt.savefig(out_path, dpi=150, bbox_inches='tight', facecolor='white')
    try:
        from matplotlib.figure import Figure as _MplFigure
        import matplotlib.pyplot as _plt
        if isinstance(fig, _MplFigure):
            _plt.close(fig)  # type: ignore[arg-type]
    except Exception:
        pass

    return out_path

def main():
    """Execução principal do framework investigativo."""
    print("="*80)
    print(" FRAMEWORK INVESTIGATIVO COMPLETO - ARTIGO NATURE/QUANTUM")
    print(" Beneficial Quantum Noise in Variational Quantum Classifiers")
    print("="*80)
    
    # Criar pasta de resultados com timestamp (com suporte a Colab/Drive e variável de ambiente)
    pasta_resultados = _preparar_diretorio_resultados(_parse_resultados_base_from_args() or os.environ.get('RESULTS_BASE_DIR'))
    # Forense raiz: README e metadata
    raiz_readme = os.path.join(pasta_resultados, 'README.md')
    raiz_meta_path = os.path.join(pasta_resultados, 'metadata.json')
    with open(raiz_readme, 'w', encoding='utf-8') as f:
        f.write(
            "# Framework Investigativo Completo v7.1\n\n"
            f"- Data de execução: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            "- Este diretório contém todos os resultados do experimento (grid search, análises, figuras).\n"
        )
    raiz_meta = {
        'tipo': 'raiz_experimento',
        'data_execucao': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'arquivos_gerados': []
    }

    # 1. Carregar datasets
    print("\n[1/5] Carregando datasets...")
    datasets = carregar_datasets(seed=42)
    print(f"  ✓ {len(datasets)} datasets carregados")
    for nome, data in datasets.items():
        print(f"    - {nome}: {len(data['y_train'])} treino, {len(data['y_test'])} teste")

    # 2. Executar grid search (suporta modo rápido com env var)
    print("\n[2/5] Executando grid search...")
    n_epocas_padrao = 15
    try:
        modo_rapido = os.environ.get('VQC_QUICK', '0') == '1'
    except Exception:
        modo_rapido = False
    if modo_rapido:
        print("  ⚡ Modo rápido ativado (VQC_QUICK=1): n_epocas=5")
    df_resultados = executar_grid_search(datasets, n_epocas=(5 if modo_rapido else n_epocas_padrao), verbose=True, pasta_resultados=pasta_resultados)
    # Rastreio fino do nível de ruído após grid search
    rastreio_fino_nivel_ruido(df_resultados, datasets, pasta_resultados, n_epocas=(5 if modo_rapido else n_epocas_padrao), verbose=True)

    # Salvar resultados brutos
    csv_path = os.path.join(pasta_resultados, 'resultados_completos_artigo.csv')
    df_resultados.to_csv(csv_path, index=False)
    print(f"\n  ✓ Resultados salvos: {csv_path}")

    # 3. Análises estatísticas
    print("\n[3/6] Executando análises estatísticas...")
    analises = executar_analises_estatisticas(df_resultados, verbose=True, pasta_resultados=pasta_resultados)

    # 4. Gerar visualizações
    print("\n[4/6] Gerando visualizações...")
    gerar_visualizacoes(df_resultados, salvar=True, pasta_resultados=pasta_resultados)

    # 5. Análises profundas (v7.1)
    print("\n[5/6] Executando análises profundas (v7.1)...")
    executar_analises_profundas(df_resultados, salvar_figuras=True, pasta_resultados=pasta_resultados)

    # 6. Resumo final
    print("\n[6/6] Resumo Final")
    print("="*80)
    
    print("\n📊 ESTATÍSTICAS GERAIS:")
    print(f"  Total de experimentos: {len(df_resultados)}")
    print(f"  Datasets testados: {df_resultados['dataset'].nunique()}")
    print(f"  Configurações por dataset: {len(df_resultados) // df_resultados['dataset'].nunique()}")
    
    print("\n🏆 MELHOR CONFIGURAÇÃO GERAL:")
    idx_melhor = df_resultados['acuracia_teste'].idxmax()
    melhor = df_resultados.loc[idx_melhor]
    print(f"  Dataset: {melhor['dataset']}")
    print(f"  Arquitetura: {melhor['arquitetura']}")
    print(f"  Inicialização: {melhor['estrategia_init']}")
    print(f"  Ruído: {melhor['tipo_ruido']} (nível={melhor['nivel_ruido']:.3f})")
    print(f"  Acurácia: {melhor['acuracia_teste']:.4f}")
    
    print("\n🌀 RUÍDOS BENÉFICOS:")
    baseline = df_resultados[df_resultados['tipo_ruido'] == 'sem_ruido']['acuracia_teste'].mean()
    for ruido in ['depolarizante', 'amplitude_damping', 'phase_damping']:
        df_ruido = df_resultados[(df_resultados['tipo_ruido'] == ruido) & (df_resultados['nivel_ruido'] > 0)]
        if len(df_ruido) > 0:
            media = df_ruido['acuracia_teste'].mean()
            delta = media - baseline
            status = "✓ BENÉFICO" if delta > 0 else "✗ Prejudicial"
            print(f"  {ruido:20s}: {media:.4f} (Δ={delta:+.4f}) {status}")
    
    print("\n📏 EFFECT SIZES:")
    if 'effect_sizes' in analises:
        print(f"  Cohen's d: {analises['effect_sizes']['cohen_d']:.4f}")
        print(f"  Glass's Δ: {analises['effect_sizes']['glass_delta']:.4f}")
        print(f"  Hedges' g: {analises['effect_sizes']['hedges_g']:.4f}")
    
    print("\n📁 ARQUIVOS GERADOS:")
    print("  - resultados_completos_artigo.csv")
    print("  - comparacao_baselines.csv (NOVO)")
    print("  - figura2_beneficial_noise.html")
    print("  - figura2b_beneficial_noise_ic95.html (NOVO)")
    print("  - figura3_noise_types.html")
    print("  - figura3b_noise_types_ic95.html (NOVO)")
    print("  - figura4_initialization.html")
    print("  - figura5_architecture_tradeoffs.html")
    print("  - figura6_effect_sizes.html")
    print("  - figura7_overfitting.html")
    
    print("\n🚀 FUNCIONALIDADES AVANÇADAS ATIVADAS:")
    print("  ✓ 4 schedules de ruído (linear, exp, cosine, adaptativo)")
    print("  ✓ Detecção de Barren Plateaus")
    print("  ✓ Monitoramento de emaranhamento")
    print("  ✓ 3 otimizadores (Adam, SGD, QNG)")
    print("  ✓ 3 funções de custo (MSE, Cross-Entropy, Hinge)")
    print("  ✓ Effect sizes: Cohen's d, Glass's Δ, Hedges' g")
    print("  ✓ Post-hoc: Bonferroni, Scheffé")
    
    print("\n✨ NOVAS FUNCIONALIDADES (v7.1):")
    print("  ✓ Autotuning com Optuna (otimização bayesiana)")
    print("  ✓ Modelagem de ruído via Lindblad (5 canais)")
    print("  ✓ PCA (Análise de Componentes Principais)")
    print("  ✓ Clustering K-means")
    print("  ✓ Análise de correlação")
    print("  ✓ Análise de sensibilidade")
    print("  ✓ 9 visualizações interativas totais")
    
    print("\n" + "="*80)
    print(" ✓ FRAMEWORK INVESTIGATIVO COMPLETO v7.1 EXECUTADO COM SUCESSO!")
    print("="*80)
    # Atualizar metadata raiz com a lista final de arquivos
    try:
        raiz_meta['arquivos_gerados'] = [f for f in os.listdir(pasta_resultados) if os.path.isfile(os.path.join(pasta_resultados, f))]
        with open(raiz_meta_path, 'w', encoding='utf-8') as f:
            json.dump(raiz_meta, f, indent=2, ensure_ascii=False, default=str)
    except Exception:
        pass


if __name__ == "__main__":
    # Suporte a execução especializada por variável de ambiente/argumento:
    # - VQC_VALIDATE_CIRCUIT=1: executa a validação de exportação de circuito
    # - VQC_ONLY_VALIDATE=1: executa somente a validação e encerra
    # - Argumentos equivalentes: --validar-circuito, --only-validate
    import sys as _sys
    _args = set(_sys.argv[1:])
    _env_validate = os.environ.get('VQC_VALIDATE_CIRCUIT', '0') == '1'
    _env_only = os.environ.get('VQC_ONLY_VALIDATE', '0') == '1'
    _flag_validate = ('--validar-circuito' in _args)
    _flag_only = ('--only-validate' in _args)

    if _env_validate or _flag_validate or _env_only or _flag_only:
        # Preparar pasta de resultados (coerente com execução principal)
        now = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
        pasta_resultados = f"resultados_{now}"
        os.makedirs(pasta_resultados, exist_ok=True)
        # Validar exportação
        png_path = validar_exportacao_circuito(pasta_resultados)
        print(f"✓ Validação de circuito concluída: {png_path}")
        if _env_only or _flag_only:
            _sys.exit(0)

    # Execução normal
    main()

