# Materiais e Métodos

## Desenho do Estudo e Fundamentação Teórica

A presente investigação constitui um estudo computacional experimental de natureza quantitativa, projetado para explorar sistematicamente o papel do ruído quântico em Classificadores Quânticos Variacionais (VQCs) através de um framework abrangente que integra simulação de circuitos quânticos, otimização híbrida quântico-clássica e análise estatística multivariada. O desenho experimental foi concebido para responder à questão central de pesquisa: o ruído quântico, tradicionalmente considerado um obstáculo na era NISQ, pode ser ativamente aproveitado como um recurso para melhorar o desempenho e a robustez de classificadores quânticos? Esta pergunta emerge de uma mudança de paradigma na comunidade de computação quântica, onde o ruído passa a ser investigado não apenas como um problema a ser mitigado, mas como uma característica potencialmente benéfica que pode ser explorada estrategicamente.

A fundamentação teórica desta investigação ancora-se em três pilares conceituais que, em conjunto, justificam a relevância e a originalidade da pesquisa. O primeiro pilar é estabelecido pelo trabalho seminal de PRESKILL (2018, p. 79), que define a era NISQ (Noisy Intermediate-Scale Quantum) como o regime atual da computação quântica, caracterizado por dispositivos com 50 a 100 qubits que operam sem correção de erro quântico completa, onde o ruído é uma característica inevitável e onipresente. Neste contexto, PRESKILL argumenta que os algoritmos híbridos quântico-clássicos, particularmente os algoritmos quânticos variacionais, representam a abordagem mais promissora para alcançar vantagens práticas em aplicações de otimização, simulação e aprendizado de máquina. Esta caracterização do cenário tecnológico atual estabelece a necessidade urgente de desenvolver estratégias que não apenas tolerem o ruído, mas que potencialmente o transformem em um ativo.

O segundo pilar conceitual é fornecido pela revisão abrangente de CEREZO et al. (2021, p. 625-644), que sistematiza o conhecimento sobre algoritmos quânticos variacionais e identifica os principais desafios para sua implementação prática, incluindo o fenômeno dos barren plateaus, a escolha de ansätze apropriados e a trainability dos modelos. Os autores demonstram que a função de custo, a arquitetura do circuito e a estratégia de otimização são componentes interdependentes que determinam o sucesso ou fracasso de um VQA, e que a compreensão dessas interações é essencial para o design de algoritmos robustos. Esta análise multifacetada dos VQAs fornece o arcabouço metodológico dentro do qual nossa investigação se insere, permitindo que as escolhas técnicas sejam fundamentadas em um corpo consolidado de conhecimento.

O terceiro e mais crucial pilar é estabelecido pelo trabalho inovador de DU et al. (2021, p. 023153), que demonstra teoricamente e empiricamente que o ruído de despolarização em circuitos quânticos pode aumentar a robustez de classificadores quânticos contra exemplos adversariais, um dos problemas mais desafiadores em aprendizado de máquina. Os autores provam que a inserção estratégica de canais de despolarização induz quantum differential privacy, resultando em bounds de robustez que são independentes dos detalhes do modelo de classificação, uma propriedade única que não possui análogo clássico. Este resultado contraintuitivo, onde o ruído não apenas não degrada mas efetivamente melhora uma propriedade desejável do classificador, fornece a motivação central para nossa investigação sistemática do ruído benéfico, expandindo a análise para múltiplos tipos de ruído, níveis de intensidade e schedules dinâmicos.

A integração desses três pilares conceituais estabelece uma linha de pesquisa coerente e justificada: na era NISQ, onde o ruído é inevitável (PRESKILL, 2018), e onde os VQAs são a abordagem mais promissora mas enfrentam desafios de trainability (CEREZO et al., 2021), a demonstração de que o ruído pode ser benéfico (DU et al., 2021) abre uma nova fronteira de investigação que pode transformar um desafio tecnológico em uma oportunidade científica. Nossa pesquisa busca não apenas replicar os achados de Du et al., mas expandir significativamente o escopo da investigação através de um grid search massivo que explora o espaço de configurações de forma sistemática e exaustiva.

A escolha por um estudo computacional experimental, ao invés de implementação em hardware quântico real, justifica-se por múltiplas razões metodológicas e práticas. Primeiro, a simulação permite o controle preciso e a manipulação sistemática de variáveis experimentais que seriam difíceis ou impossíveis de isolar em hardware físico, onde múltiplas fontes de ruído coexistem e interagem de formas complexas. Segundo, a simulação possibilita a execução de um número massivo de experimentos (mais de 216 configurações únicas) em um tempo razoável, o que seria proibitivamente custoso em termos de tempo de acesso a hardware quântico real. Terceiro, como argumentado por SCHULD et al. (2020, p. 032308), a validação de conceitos e arquiteturas através de simulação é um passo essencial antes da implementação em hardware, permitindo a identificação de configurações promissoras que justifiquem o investimento em experimentos físicos. Finalmente, a simulação via PennyLane permite a modelagem realista de canais de ruído através da equação de Lindblad, fornecendo uma representação fisicamente motivada dos processos de decoerência que ocorrem em dispositivos reais.

## Framework Computacional e Ambiente de Desenvolvimento

A implementação do framework computacional foi realizada inteiramente em Python 3.11, linguagem escolhida por sua maturidade no ecossistema científico e sua ampla adoção na comunidade de computação quântica e aprendizado de máquina. A escolha de bibliotecas específicas foi guiada por critérios de validação científica, maturidade tecnológica e capacidade de integração, conforme detalhado a seguir.

O núcleo do framework é construído sobre o **PennyLane** (versão 0.30+), um framework de código aberto desenvolvido por BERGHOLM et al. (2018) especificamente para computação quântica diferenciável. A escolha do PennyLane justifica-se por múltiplas razões técnicas e científicas que o distinguem de alternativas como Qiskit ou Cirq. Primeiro, o PennyLane implementa diferenciação automática de circuitos quânticos através da parameter-shift rule, permitindo o cálculo exato de gradientes sem aproximações numéricas, o que é essencial para a otimização baseada em gradiente que constitui o coração dos VQAs. Segundo, o PennyLane oferece integração nativa com frameworks de aprendizado de máquina clássico (NumPy, PyTorch, TensorFlow), permitindo a construção de modelos híbridos onde camadas quânticas e clássicas podem ser compostas de forma transparente. Terceiro, o PennyLane suporta múltiplos backends de simulação e hardware real, garantindo que o código desenvolvido seja portável e possa ser futuramente executado em dispositivos quânticos físicos sem modificações substanciais. Quarto, o PennyLane possui uma comunidade ativa e documentação extensa, incluindo tutoriais específicos sobre VQCs e barren plateaus, o que facilita a reprodutibilidade e validação dos resultados.

Para as operações numéricas fundamentais, utilizamos **NumPy** (versão 2.3.1) e **SciPy** (versão 1.15.3), bibliotecas que constituem a fundação do ecossistema científico Python. NumPy fornece estruturas de dados eficientes para arrays multidimensionais e operações vetorizadas, enquanto SciPy estende essas capacidades com algoritmos especializados para álgebra linear, otimização e estatística. A escolha dessas versões específicas garante compatibilidade com PennyLane e acesso às implementações mais recentes e otimizadas de algoritmos numéricos.

A integração com o ecossistema de aprendizado de máquina clássico é realizada através do **scikit-learn** (versão 1.6.1), biblioteca escolhida por sua API consistente e bem documentada. Conforme demonstrado no código, a classe `ClassificadorVQC` herda de `BaseEstimator` e `ClassifierMixin` do scikit-learn, garantindo compatibilidade com pipelines de machine learning e permitindo o uso de ferramentas de validação cruzada, seleção de modelos e avaliação de métricas. Esta escolha de design, inspirada no trabalho de SCHULD et al. (2020), facilita a comparação direta entre classificadores quânticos e clássicos, permitindo avaliar se o VQC oferece vantagens sobre métodos estabelecidos como SVM e Random Forest.

Para a otimização bayesiana de hiperparâmetros, implementamos o **Optuna** (versão 3.0+), framework desenvolvido por AKIBA et al. (2019, p. 2623-2631) que representa o estado da arte em autotuning de modelos de machine learning. A escolha do Optuna sobre alternativas como Hyperopt ou scikit-optimize justifica-se por sua implementação do algoritmo Tree-structured Parzen Estimator (TPE), que demonstra convergência mais rápida que busca aleatória ou grid search tradicional, especialmente em espaços de hiperparâmetros de alta dimensionalidade. O TPE modela a distribuição de hiperparâmetros condicionada ao desempenho observado, concentrando a busca em regiões promissoras do espaço de configurações. Adicionalmente, o Optuna implementa pruning adaptativo através do MedianPruner, que interrompe trials não-promissores precocemente, economizando recursos computacionais significativos em um grid search com mais de 216 configurações.

A análise estatística foi implementada utilizando **statsmodels** (versão 0.14+), biblioteca especializada em modelagem estatística que fornece implementações rigorosas de ANOVA multifatorial, testes post-hoc e cálculo de tamanhos de efeito. A escolha do statsmodels sobre alternativas como R ou SPSS justifica-se pela integração nativa com o ecossistema Python e pela disponibilidade de documentação detalhada sobre os algoritmos implementados, permitindo a validação dos resultados. Para visualizações interativas, utilizamos **Plotly** (versão 5.0+), que gera gráficos de qualidade para publicação em formato HTML interativo, permitindo exploração dinâmica dos resultados.

O ambiente de desenvolvimento foi configurado em um sistema Linux Ubuntu 22.04 com Python 3.11.0, garantindo reprodutibilidade através do gerenciamento de dependências via pip e ambientes virtuais. Todos os experimentos foram executados com seed aleatória fixa (seed=42) para garantir reprodutibilidade determinística dos resultados, seguindo as melhores práticas estabelecidas pela comunidade de machine learning.

## Arquiteturas de Circuitos Quânticos e Justificativa da Diversidade de Ansätze

A escolha da arquitetura do circuito quântico parametrizado, comumente denominada ansatz, constitui uma das decisões metodológicas mais críticas no design de VQAs, pois determina o espaço de estados quânticos que o modelo pode representar e, consequentemente, sua capacidade de aproximar a função alvo. Para garantir que as conclusões desta investigação não sejam artefatos de uma escolha particular de ansatz, implementamos nove arquiteturas distintas que abrangem um espectro de propriedades estruturais, capacidades de emaranhamento e profundidades de circuito, permitindo uma análise comparativa robusta.

A primeira arquitetura implementada é o **Hardware-Efficient Ansatz**, introduzido por KANDALA et al. (2017, p. 242-246) no contexto do Variational Quantum Eigensolver para moléculas pequenas. Esta arquitetura é projetada especificamente para ser implementável em hardware quântico de curto prazo, utilizando apenas portas nativas do dispositivo e minimizando a profundidade do circuito. O ansatz consiste em camadas alternadas de rotações de qubit único (RX, RY, RZ) parametrizadas e portas de emaranhamento de dois qubits (CNOT) aplicadas em uma topologia que respeita a conectividade física do hardware. A justificativa para a inclusão desta arquitetura reside em sua relevância prática: se o ruído benéfico for observado apenas em ansätze altamente expressivos mas impraticáveis em hardware real, a utilidade dos resultados seria limitada. Por outro lado, se o efeito benéfico for observado no Hardware-Efficient Ansatz, isso sugere que a estratégia pode ser implementada em dispositivos NISQ atuais.

A segunda arquitetura é o **Strongly-Entangling Ansatz**, proposto por SCHULD et al. (2020, p. 032308) como um ansatz de alta expressividade que pode representar uma ampla classe de estados quânticos. Esta arquitetura utiliza portas de rotação arbitrárias em todos os qubits seguidas por uma cascata de portas CNOT que cria emaranhamento entre todos os pares de qubits, resultando em um circuito profundo com alta capacidade representacional. A inclusão desta arquitetura permite testar a hipótese de que o ruído benéfico pode estar relacionado à capacidade de emaranhamento do circuito: circuitos altamente emaranhados podem ser mais sensíveis ao ruído, mas também podem se beneficiar mais de sua presença controlada.

A terceira arquitetura implementada é inspirada no **Quantum Approximate Optimization Algorithm (QAOA)**, originalmente proposto por FARHI e NEVEN (2018) para problemas de otimização combinatória. O ansatz QAOA alterna entre um Hamiltoniano de mistura (tipicamente rotações RX) e um Hamiltoniano de problema (codificando a estrutura dos dados), com profundidade controlada pelo parâmetro p. Esta arquitetura é particularmente interessante no contexto de ruído benéfico porque sua estrutura alternada pode interagir de forma não-trivial com schedules de ruído dinâmicos, potencialmente permitindo que o ruído auxilie na exploração do espaço de soluções nas camadas iniciais e na convergência fina nas camadas finais.

As demais arquiteturas implementadas incluem variações de topologias de emaranhamento: **Tree Entanglement** (emaranhamento em estrutura de árvore binária), **Star Entanglement** (emaranhamento radial a partir de um qubit central), **Brickwork** (emaranhamento em padrão de tijolos), **Alternating Layers** (alternância entre emaranhamento horizontal e vertical) e **Random Entangling** (emaranhamento aleatório mas fixo). Esta diversidade de topologias permite investigar se a eficácia do ruído benéfico depende da estrutura de conectividade do circuito, uma questão que não foi explorada em trabalhos anteriores.

A justificativa teórica para esta diversidade de ansätze é fornecida pelo trabalho de MCCLEAN et al. (2018, p. 4812), que demonstra que ansätze aleatórios ou não estruturados levam ao fenômeno dos barren plateaus, onde o gradiente da função de custo se torna exponencialmente pequeno com o número de qubits. Os autores provam que a probabilidade de encontrar um gradiente não-zero em uma direção aleatória decresce exponencialmente, tornando a otimização baseada em gradiente inviável. Este resultado fundamental estabelece que a escolha do ansatz não é meramente uma questão de preferência, mas uma decisão que pode determinar se o problema é tratável ou não. Nossa estratégia de implementar múltiplos ansätze estruturados, cada um com propriedades geométricas distintas, permite investigar se diferentes estruturas interagem de forma diferente com o ruído, potencialmente revelando princípios de design para ansätze robustos ao ruído.

## Estratégias de Inicialização e Mitigação de Barren Plateaus

A inicialização dos parâmetros do circuito quântico constitui um passo crítico que pode determinar o sucesso ou fracasso do treinamento, conforme demonstrado de forma conclusiva por MCCLEAN et al. (2018) e GRANT et al. (2019). Para abordar este desafio de forma sistemática e fundamentada, implementamos cinco estratégias de inicialização distintas, cada uma motivada por princípios teóricos específicos.

A primeira estratégia, denominada **inicialização matemática**, utiliza constantes matemáticas fundamentais como valores iniciais dos parâmetros. Especificamente, os parâmetros são inicializados com valores baseados em π (pi ≈ 3.14159), e (número de Euler ≈ 2.71828), φ (razão áurea ≈ 1.61803), √2 (raiz quadrada de dois ≈ 1.41421), ln(2) (logaritmo natural de dois ≈ 0.69315) e γ (constante de Euler-Mascheroni ≈ 0.5772). A justificativa para esta escolha reside na ubiquidade dessas constantes em fenômenos naturais e matemáticos, sugerindo que elas podem representar pontos de partida "naturais" no espaço de parâmetros. Adicionalmente, como essas constantes abrangem diferentes ordens de magnitude, aplicamos uma normalização logarítmica para mapeá-las ao intervalo [-π, π], que é o domínio apropriado para portas de rotação quântica. A normalização é realizada através da transformação θ_norm = -π + [(log₁₀(|θ| + ε) - min) / (max - min + ε)] × 2π, onde ε = 10⁻¹⁰ é um termo de regularização para evitar logaritmo de zero. Esta transformação preserva as relações relativas entre as constantes enquanto as mapeia para o intervalo correto.

A segunda estratégia, denominada **inicialização quântica**, utiliza constantes físicas fundamentais do domínio quântico, especificamente os valores recomendados pelo CODATA 2018 (TIESINGA et al., 2021, p. 025010): ℏ (constante de Planck reduzida = 1.054571817×10⁻³⁴ J·s), α (constante de estrutura fina = 7.2973525693×10⁻³) e R∞ (constante de Rydberg = 10973731.568160 m⁻¹). A motivação para esta estratégia é que, sendo o circuito quântico um sistema físico governado pelas leis da mecânica quântica, a inicialização com constantes que descrevem fenômenos quânticos fundamentais pode fornecer um ponto de partida fisicamente significativo. Notavelmente, essas constantes abrangem aproximadamente 40 ordens de magnitude, tornando a normalização logarítmica essencial. A escolha dos valores CODATA garante precisão e reprodutibilidade, permitindo que outros pesquisadores repliquem exatamente nossa inicialização.

A terceira estratégia, denominada **Fibonacci Spiral**, representa uma abordagem geométrica inspirada em padrões naturais. Os parâmetros são inicializados seguindo uma espiral de Fibonacci no espaço de ângulos, onde o i-ésimo parâmetro é dado por θᵢ = (2π × i/φ) mod 2π - π, com φ sendo a razão áurea. Esta distribuição geométrica garante que os parâmetros iniciais estejam uniformemente distribuídos no círculo unitário, mas de forma determinística e estruturada, ao invés de aleatória. A justificativa teórica para esta abordagem vem da teoria de otimização em espaços de alta dimensão, onde distribuições quasi-aleatórias (como sequências de baixa discrepância) frequentemente superam amostragem puramente aleatória em termos de cobertura uniforme do espaço.

A quarta estratégia implementa a abordagem de **identity blocks** proposta por GRANT et al. (2019, p. 214), onde os parâmetros são escolhidos de modo que blocos do circuito inicialmente avaliem para o operador identidade. Esta estratégia é fundamentada na observação de que, se o circuito completo inicialmente implementa a identidade, sua profundidade efetiva é zero, evitando assim o regime de barren plateau no início do treinamento. À medida que os parâmetros são atualizados, a profundidade efetiva cresce gradualmente, permitindo que o otimizador explore o espaço de estados de forma controlada. GRANT et al. demonstram empiricamente que esta estratégia permite o treinamento bem-sucedido de ansätze que seriam intratáveis com inicialização aleatória, tornando-a uma escolha metodológica essencial para garantir a trainability dos modelos.

A quinta estratégia é a **inicialização aleatória** tradicional, onde os parâmetros são amostrados de uma distribuição uniforme no intervalo [-π, π]. Esta estratégia é incluída não porque seja recomendada, mas como um baseline de controle que permite quantificar o benefício das estratégias estruturadas. Como demonstrado por MCCLEAN et al. (2018), esta inicialização leva a barren plateaus em circuitos profundos, mas sua inclusão permite verificar se o ruído benéfico pode, paradoxalmente, mitigar parcialmente o problema de barren plateau ao introduzir perturbações que auxiliam na exploração do espaço de parâmetros.

Para todas as estratégias de inicialização, exceto a aleatória, adicionamos um pequeno ruído gaussiano (σ = 0.1) aos valores iniciais para quebrar simetrias exatas que poderiam levar a pontos de sela ou mínimos locais indesejáveis. Esta prática, comum em deep learning clássico, garante que cada execução do experimento explore uma trajetória ligeiramente diferente no espaço de parâmetros, permitindo avaliar a robustez da inicialização.

Complementando as estratégias de inicialização, implementamos um **DetectorBarrenPlateau** que monitora continuamente a variância do gradiente durante o treinamento. Este detector calcula a variância empírica dos componentes do gradiente e compara com um limiar teórico derivado da análise de MCCLEAN et al., que prevê que a variância deve escalar como 1/2ⁿ em um barren plateau, onde n é o número de qubits. Se a variância observada cai abaixo deste limiar por múltiplas épocas consecutivas, o detector sinaliza a presença de um barren plateau, permitindo intervenção adaptativa como aumento temporário da taxa de aprendizado ou reinicialização parcial dos parâmetros.

## Modelagem de Ruído Quântico via Equação de Lindblad

A modelagem realista dos processos de decoerência quântica constitui um dos pilares metodológicos desta investigação, pois a validade das conclusões sobre ruído benéfico depende criticamente de quão fielmente os canais de ruído simulados representam os processos físicos que ocorrem em hardware quântico real. Para garantir este realismo, todos os canais de ruído foram implementados utilizando a **equação mestra de Lindblad** (LINDBLAD, 1976, p. 119-130), que fornece a forma mais geral de evolução temporal de um sistema quântico aberto que preserva completa positividade e traço unitário.

A equação de Lindblad descreve a evolução da matriz densidade ρ(t) de um sistema quântico através da equação diferencial:

$$\frac{d\rho}{dt} = -\frac{i}{\hbar}[H, \rho] + \sum_k \gamma_k \left(L_k \rho L_k^\dagger - \frac{1}{2}\{L_k^\dagger L_k, \rho\}\right)$$

onde H é o Hamiltoniano do sistema, Lₖ são os operadores de Lindblad (também chamados de operadores de salto ou jump operators) que descrevem os canais de decoerência, γₖ são as taxas de decoerência associadas, e {·,·} denota o anticomutador. O primeiro termo descreve a evolução unitária do sistema isolado, enquanto o segundo termo, denominado dissipador de Lindblad, descreve os efeitos do acoplamento com o ambiente. A forma desta equação garante que a evolução preserve propriedades físicas essenciais: a matriz densidade permanece hermitiana, positiva semidefinida e com traço unitário.

Implementamos cinco canais de ruído distintos, cada um correspondendo a um mecanismo físico específico de decoerência observado em dispositivos quânticos reais:

**1. Depolarizing Channel (Canal de Despolarização):** Este canal é descrito pelos operadores de Lindblad Lₖ = √(p/3) σₖ, onde σₖ ∈ {X, Y, Z} são as matrizes de Pauli e p é o parâmetro de despolarização. O efeito deste canal é dado por ε_p(ρ) = (1-p)ρ + p(I/d), onde d é a dimensão do espaço de Hilbert. Fisicamente, este canal representa um processo onde o qubit, com probabilidade p, é substituído pelo estado totalmente misturado I/d, perdendo completamente sua informação quântica. Este é o modelo de ruído mais geral e isotrópico, frequentemente usado como primeira aproximação para ruído em dispositivos reais. A escolha deste canal é diretamente motivada pelo trabalho de DU et al. (2021), que demonstra que este tipo específico de ruído pode induzir robustez contra adversários.

**2. Amplitude Damping Channel (Canal de Amortecimento de Amplitude):** Este canal modela a perda de energia de um qubit para o ambiente, correspondendo fisicamente ao decaimento espontâneo de um estado excitado |1⟩ para o estado fundamental |0⟩. Os operadores de Lindblad são L₀ = |0⟩⟨1| e L₁ = √(1-γ)|1⟩⟨1|, onde γ é o parâmetro de damping relacionado ao tempo de relaxação T₁ do qubit. Este canal é particularmente relevante para qubits supercondutores, onde o decaimento para o estado fundamental é um dos principais mecanismos de decoerência. A inclusão deste canal permite investigar se o ruído benéfico é específico para ruído isotrópico (depolarização) ou se estende para processos de decoerência direcionais.

**3. Phase Damping Channel (Canal de Amortecimento de Fase):** Este canal modela a perda de coerência de fase sem perda de energia, correspondendo ao processo de dephasing puro. O operador de Lindblad é L = √γ Z, onde γ está relacionado ao tempo de decoerência de fase T₂*. Este canal transforma superposições quânticas em misturas estatísticas, destruindo a coerência sem alterar as populações dos estados |0⟩ e |1⟩. Fisicamente, este processo resulta de flutuações aleatórias na energia do qubit causadas por ruído no ambiente. A distinção entre amplitude damping e phase damping é crucial porque eles afetam diferentes aspectos do estado quântico: o primeiro afeta as populações (elementos diagonais da matriz densidade), enquanto o segundo afeta as coerências (elementos fora da diagonal).

**4. Crosstalk Channel (Canal de Crosstalk):** Este canal modela o acoplamento indesejado entre qubits adjacentes, um efeito particularmente relevante em arquiteturas de qubits supercondutores onde qubits fisicamente próximos podem interagir através de acoplamento capacitivo ou indutivo. O operador de Lindblad para crosstalk entre qubits i e j é implementado como L = √γ (σₓ⁽ⁱ⁾ ⊗ σₓ⁽ʲ⁾ + σᵧ⁽ⁱ⁾ ⊗ σᵧ⁽ʲ⁾), representando interações XY que podem causar vazamento de informação entre qubits. Este canal é importante porque, ao contrário dos canais anteriores que afetam qubits individualmente, o crosstalk introduz correlações de ruído entre qubits, potencialmente levando a efeitos coletivos não-triviais.

**5. Correlated Noise Channel (Canal de Ruído Correlacionado):** Este canal generaliza o conceito de crosstalk para modelar correlações de ruído de longo alcance que podem surgir de flutuações globais no ambiente, como variações de temperatura ou campos magnéticos externos. A implementação utiliza operadores de Lindblad que acoplam múltiplos qubits simultaneamente, criando correlações não-locais no ruído. Este tipo de ruído é particularmente relevante para investigar se o ruído benéfico é robusto a correlações espaciais, uma questão que não foi abordada em trabalhos anteriores.

A parametrização dos canais de ruído foi cuidadosamente escolhida para cobrir o regime relevante para dispositivos NISQ. Para cada canal, exploramos três níveis de intensidade: fraco (p = 0.01), moderado (p = 0.05) e forte (p = 0.1), onde p representa o parâmetro de ruído apropriado para cada canal. Estes valores foram escolhidos com base em caracterizações experimentais de dispositivos quânticos reais reportadas na literatura, garantindo que os resultados sejam relevantes para implementações práticas.

## Schedules de Ruído Dinâmicos: Uma Contribuição Metodológica

Uma das contribuições metodológicas originais desta investigação é a implementação e avaliação sistemática de **schedules de ruído dinâmicos**, onde a intensidade do ruído varia ao longo do processo de treinamento de acordo com uma função temporal pré-definida. Esta abordagem contrasta com a prática padrão de aplicar um nível de ruído constante, e é motivada pela hipótese de que diferentes fases do treinamento podem se beneficiar de diferentes níveis de ruído.

Implementamos quatro schedules distintos, cada um fundamentado em princípios teóricos específicos:

**1. Schedule Linear:** A intensidade do ruído varia linearmente de um valor inicial p₀ para um valor final pf ao longo das épocas de treinamento, seguindo p(t) = p₀ + (pf - p₀) × (t/T), onde t é a época atual e T é o número total de épocas. A justificativa para este schedule é que, nas fases iniciais do treinamento, um nível mais alto de ruído pode auxiliar na exploração do espaço de parâmetros, evitando convergência prematura para mínimos locais, enquanto nas fases finais, um nível mais baixo permite convergência fina para o mínimo global. Esta estratégia é análoga ao conceito de simulated annealing em otimização clássica, onde a "temperatura" (análoga ao ruído) é gradualmente reduzida.

**2. Schedule Exponencial:** A intensidade do ruído decai exponencialmente segundo p(t) = p₀ × exp(-λt/T), onde λ é um parâmetro de taxa de decaimento. Este schedule é motivado pela observação de que, em muitos processos de otimização, a maior parte da exploração ocorre nas primeiras épocas, e um decaimento exponencial captura esta dinâmica de forma natural. Adicionalmente, processos de decoerência física frequentemente seguem dinâmicas exponenciais (como o decaimento T₁), tornando este schedule fisicamente motivado.

**3. Schedule Cosseno:** A intensidade do ruído varia seguindo uma função cosseno: p(t) = pf + (p₀ - pf) × [1 + cos(πt/T)]/2. Este schedule, inspirado em técnicas de cosine annealing em deep learning, fornece um decaimento suave com desaceleração nas extremidades, potencialmente permitindo que o otimizador se adapte gradualmente às mudanças no nível de ruído. A forma suave da função cosseno evita mudanças abruptas que poderiam desestabilizar o processo de otimização.

**4. Schedule Adaptativo:** Este schedule ajusta dinamicamente o nível de ruído com base na performance observada do modelo. Especificamente, se a acurácia de validação não melhora por k épocas consecutivas (indicando possível convergência para um mínimo local), o nível de ruído é temporariamente aumentado para facilitar escape, e então gradualmente reduzido. Este schedule implementa um mecanismo de feedback que torna o ruído responsivo ao estado do treinamento, potencialmente oferecendo o melhor dos dois mundos: exploração quando necessária e exploitation quando apropriada.

A implementação destes schedules foi realizada através da classe `ScheduleRuido`, que encapsula a lógica de cálculo do nível de ruído para cada época e tipo de schedule. Esta modularização permite a fácil extensão para novos schedules e facilita a análise comparativa dos resultados.

## Otimização e Treinamento dos Classificadores

O treinamento dos Classificadores Quânticos Variacionais envolve a otimização dos parâmetros θ do circuito para minimizar uma função de custo C(θ), um problema de otimização não-convexa em alta dimensão que apresenta desafios únicos no contexto quântico. Para abordar este problema de forma abrangente, implementamos três otimizadores distintos e três funções de custo, permitindo avaliar a robustez dos resultados às escolhas de otimização.

**Otimizador Adam:** O otimizador primário utilizado é o Adam (Adaptive Moment Estimation), proposto por KINGMA e BA (2014), que representa o estado da arte em otimização estocástica para redes neurais. O Adam mantém estimativas de momento de primeira ordem (média dos gradientes) e segunda ordem (variância dos gradientes) com decaimento exponencial, permitindo taxas de aprendizado adaptativas por parâmetro. A atualização dos parâmetros segue:

$$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$$
$$v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$$
$$\hat{m}_t = m_t / (1-\beta_1^t)$$
$$\hat{v}_t = v_t / (1-\beta_2^t)$$
$$\theta_{t+1} = \theta_t - \alpha \hat{m}_t / (\sqrt{\hat{v}_t} + \epsilon)$$

onde gₜ é o gradiente no tempo t, α é a taxa de aprendizado, β₁ e β₂ são parâmetros de decaimento (tipicamente 0.9 e 0.999), e ε é um termo de regularização (tipicamente 10⁻⁸). A escolha do Adam justifica-se por sua robustez a escolha de hiperparâmetros e sua capacidade de lidar com gradientes esparsos e ruído, características relevantes para a otimização de VQCs.

**Otimizador SGD com Momentum:** Como alternativa, implementamos Stochastic Gradient Descent com momentum, um otimizador mais simples mas frequentemente mais estável que Adam. A atualização segue vₜ = μvₜ₋₁ - αgₜ e θₜ₊₁ = θₜ + vₜ, onde μ é o coeficiente de momentum (tipicamente 0.9). O momentum acumula uma média móvel exponencial dos gradientes, acelerando a convergência em direções consistentes e amortecendo oscilações. A inclusão deste otimizador permite verificar se os resultados são específicos para Adam ou generalizam para outros métodos baseados em gradiente.

**Quantum Natural Gradient (QNG):** O terceiro otimizador implementado é uma aproximação do Quantum Natural Gradient, proposto por STOKES et al. (2020, p. 269). O QNG utiliza a geometria intrínseca do espaço de estados quânticos, medida pela métrica de Fubini-Study, para definir uma direção de descida "natural" que é invariante a reparametrizações do circuito. Matematicamente, a atualização do QNG é dada por θₜ₊₁ = θₜ - α F⁻¹ ∇C(θₜ), onde F é a matriz de informação de Fisher quântica. A implementação completa do QNG requer o cálculo da matriz F e sua inversão, o que pode ser computacionalmente custoso. Nossa implementação utiliza uma aproximação onde F é estimada através de múltiplas avaliações do circuito com pequenas perturbações nos parâmetros, seguindo a parameter-shift rule. A inclusão do QNG é motivada por resultados teóricos que sugerem que ele pode convergir mais rapidamente que gradiente euclidiano em paisagens de otimização complexas, potencialmente oferecendo vantagens em presença de ruído.

**Funções de Custo:** Exploramos três funções de custo distintas para avaliar a sensibilidade dos resultados à escolha da paisagem de otimização:

1. **Mean Squared Error (MSE):** C_MSE = (1/N) Σᵢ (yᵢ - ŷᵢ)², onde yᵢ são os labels verdadeiros e ŷᵢ são as predições. Esta é a função de custo mais simples e direta, apropriada para problemas de regressão e classificação.

2. **Cross-Entropy:** C_CE = -(1/N) Σᵢ [yᵢ log(pᵢ) + (1-yᵢ) log(1-pᵢ)], onde pᵢ = 1/(1+exp(-ŷᵢ)) são as probabilidades preditas após aplicação da função sigmoide. Esta função de custo é teoricamente mais apropriada para classificação probabilística, penalizando predições confiantes mas incorretas mais severamente que MSE.

3. **Hinge Loss:** C_hinge = (1/N) Σᵢ max(0, 1 - yᵢŷᵢ), função de custo utilizada em Support Vector Machines que foca na margem de separação entre classes. Esta função é menos sensível a outliers que MSE e pode ser mais robusta em presença de ruído nos labels.

O processo de treinamento foi configurado com 15 épocas para cada configuração, número escolhido com base em experimentos preliminares que indicaram convergência típica entre 10-20 épocas. Para cada configuração, monitoramos não apenas a acurácia de treinamento e validação, mas também a variância do gradiente (para detecção de barren plateaus), a entropia de emaranhamento (para caracterizar a complexidade do estado quântico), e a negatividade (como medida de emaranhamento bipartite).

## Otimização Bayesiana de Hiperparâmetros com Optuna

A otimização de hiperparâmetros constitui um componente essencial de qualquer pipeline de machine learning, determinando frequentemente a diferença entre um modelo mediocre e um modelo de alto desempenho. No contexto de VQCs, o espaço de hiperparâmetros é particularmente vasto, incluindo taxa de aprendizado, número de camadas do circuito, parâmetros de ruído, escolha de otimizador e função de custo, entre outros. Para explorar este espaço de forma eficiente, implementamos otimização bayesiana utilizando o framework Optuna (AKIBA et al., 2019).

A otimização bayesiana modela a função objetivo (acurácia de validação) como uma amostra de um processo gaussiano, e utiliza esta distribuição de probabilidade para decidir quais configurações de hiperparâmetros explorar a seguir, balanceando exploration (testar regiões inexploradas) e exploitation (refinar regiões promissoras). O algoritmo Tree-structured Parzen Estimator (TPE), implementado no Optuna, aproxima esta distribuição modelando separadamente P(x|y) para y < y* (configurações boas) e y ≥ y* (configurações ruins), onde y* é um quantil da distribuição de performances observadas.

Configuramos o Optuna para explorar os seguintes hiperparâmetros:
- Taxa de aprendizado: espaço logarítmico [10⁻⁴, 10⁻¹]
- Número de camadas: inteiro [1, 5]
- Parâmetro de ruído inicial: contínuo [0.001, 0.2]
- Parâmetro de ruído final: contínuo [0.001, 0.2]
- Otimizador: categórico {Adam, SGD, QNG}
- Função de custo: categórico {MSE, Cross-Entropy, Hinge}

O processo de otimização foi configurado para executar 100 trials por configuração de dataset e ansatz, com pruning via MedianPruner que interrompe trials cuja performance intermediária (após 5 épocas) está abaixo da mediana dos trials completados. Esta estratégia de pruning economiza aproximadamente 40-60% do tempo computacional sem perda significativa de qualidade na solução encontrada.

## Análise Estatística Multivariada

Para garantir que as conclusões desta investigação sejam estatisticamente robustas e atendam aos padrões de rigor de publicações de alto impacto, implementamos uma suíte abrangente de análises estatísticas que vai substancialmente além da simples comparação de médias de acurácia. Esta abordagem multifacetada permite não apenas detectar diferenças significativas entre configurações, mas também quantificar a magnitude dessas diferenças, identificar interações entre fatores e avaliar a robustez dos resultados.

**Análise de Variância (ANOVA) Multifatorial:** A técnica estatística primária utilizada é ANOVA de 2 e 3 vias, que permite avaliar simultaneamente o efeito de múltiplos fatores experimentais e suas interações. Para um design fatorial com fatores A (tipo de ruído), B (nível de ruído) e C (arquitetura), a ANOVA decompõe a variância total em componentes:

$$SS_{total} = SS_A + SS_B + SS_C + SS_{AB} + SS_{AC} + SS_{BC} + SS_{ABC} + SS_{erro}$$

onde SS denota soma de quadrados. Cada termo de interação (e.g., SS_AB) captura variância que não pode ser explicada pelos efeitos principais isolados, revelando se o efeito de um fator depende do nível de outro fator. Por exemplo, um termo SS_AB significativo indicaria que o efeito do tipo de ruído depende do nível de ruído, uma interação teoricamente plausível e cientificamente interessante.

A significância de cada termo é avaliada através da estatística F = MS_fator / MS_erro, onde MS denota quadrado médio (soma de quadrados dividida por graus de liberdade). Valores de p < 0.05 são considerados estatisticamente significativos, seguindo a convenção padrão em ciências experimentais. Adicionalmente, reportamos o coeficiente η² (eta-quadrado) para cada fator, que quantifica a proporção da variância total explicada por aquele fator, fornecendo uma medida de importância prática complementar à significância estatística.

**Testes Post-Hoc:** Quando a ANOVA indica diferenças significativas entre grupos (p < 0.05), testes post-hoc são aplicados para identificar quais pares específicos de grupos diferem. Implementamos três testes post-hoc com diferentes características:

1. **Tukey HSD (Honest Significant Difference):** Este teste, proposto por TUKEY (1949, p. 99-114), controla a taxa de erro familywise (FWER) e é apropriado para comparações all-pairs quando o tamanho das amostras é aproximadamente igual. O teste calcula a diferença mínima significativa (HSD) baseada na distribuição studentized range, e declara dois grupos significativamente diferentes se |μᵢ - μⱼ| > HSD.

2. **Bonferroni:** Este teste, mais conservador que Tukey, ajusta o nível de significância para α/k, onde k é o número de comparações, garantindo controle rigoroso da FWER. A correção de Bonferroni é apropriada quando as comparações são planejadas a priori e quando se deseja minimizar falsos positivos, mesmo ao custo de aumentar falsos negativos.

3. **Scheffé:** Este teste, proposto por SCHEFFÉ (1959), é o mais conservador dos três, mas permite testar não apenas comparações pairwise, mas também contrastes arbitrários. O teste é baseado na estatística F e declara um contraste significativo se F_contraste > (k-1)F_crítico, onde k é o número de grupos.

A implementação destes três testes permite avaliar a robustez das conclusões: se uma diferença é significativa nos três testes, incluindo o conservador Scheffé, isso fornece evidência forte de uma diferença real.

**Medidas de Tamanho do Efeito:** A significância estatística (p-valor) informa se uma diferença é provavelmente real (não devida ao acaso), mas não informa a magnitude ou importância prática dessa diferença. Para quantificar a magnitude, calculamos três medidas de tamanho do efeito:

1. **d de Cohen:** Definido como d = (μ₁ - μ₂) / σ_pooled, onde σ_pooled = √[((n₁-1)σ₁² + (n₂-1)σ₂²) / (n₁+n₂-2)]. COHEN (1988) propõe a interpretação: |d| < 0.2 (pequeno), 0.2 ≤ |d| < 0.5 (médio), 0.5 ≤ |d| < 0.8 (grande), |d| ≥ 0.8 (muito grande). Esta medida é padronizada e permite comparação entre diferentes estudos e métricas.

2. **Δ de Glass:** Similar ao d de Cohen, mas usa apenas o desvio padrão do grupo controle: Δ = (μ₁ - μ₂) / σ₂. Esta medida é apropriada quando um grupo é claramente o controle (e.g., sem ruído) e o outro é tratamento (com ruído), e quando se espera que o tratamento possa afetar a variabilidade.

3. **g de Hedges:** Uma versão corrigida do d de Cohen para amostras pequenas: g = d × [1 - 3/(4(n₁+n₂)-9)]. Para amostras grandes (n > 50), g ≈ d, mas para amostras pequenas (n < 20), a correção pode ser substancial. Esta medida é preferível quando o tamanho da amostra é limitado.

**Análises Complementares:** Além das análises principais, implementamos análises complementares para caracterizar os dados de forma mais completa:

- **Análise de Componentes Principais (PCA):** Aplicada ao conjunto de métricas (acurácia, variância de gradiente, entropia de emaranhamento, negatividade) para identificar as direções de máxima variância e revelar estrutura latente nos dados.

- **Clustering K-means:** Aplicado para agrupar configurações com desempenho similar, potencialmente revelando arquétipos de configurações bem-sucedidas.

- **Análise de Correlação:** Cálculo de correlações de Pearson entre todas as variáveis para identificar relações lineares, e correlações de Spearman para relações monotônicas não-lineares.

- **Testes de Normalidade:** Teste de Shapiro-Wilk aplicado aos resíduos da ANOVA para verificar a suposição de normalidade, essencial para a validade dos testes paramétricos.

Todas as análises estatísticas foram implementadas utilizando statsmodels e scipy.stats, e os resultados são reportados com intervalos de confiança de 95% e valores de p ajustados para múltiplas comparações quando apropriado.

## Intervalos de Confiança (IC95%) e Baselines Clássicos

Para reforçar a robustez estatística das visualizações e possibilitar comparação direta com métodos clássicos, adicionamos:

1) Estimativa de IC95% nas figuras agregadas

- Cálculo: IC95% = 1.96 × SEM, onde SEM = desvio-padrão/√n.
- Agrupamentos:
  - Figura 2b (Beneficial Noise com IC95%): grupos por (dataset, tipo_ruido, nivel_ruido).
  - Figura 3b (Tipos de Ruído com IC95%): grupos por (dataset, tipo_ruido).
- Observação: entradas rotuladas como “classico” (baselines) normalmente possuem n=1 por dataset; por esse motivo, suas barras de erro não são exibidas nessas figuras.

2) Baselines clássicos e comparação consolidada

- Modelos:
  - SVM com kernel RBF (scikit-learn, `SVC(kernel='rbf', probability=True, random_state=42)`).
  - Random Forest (`RandomForestClassifier(n_estimators=100, random_state=42)`).
- Integração: os resultados clássicos são inseridos no DataFrame final com `tipo_ruido = 'classico'` e arquiteturas `SVM` e `RandomForest`.
- Exportação: criamos `comparacao_baselines.csv`, que apresenta por dataset: `vqc_melhor` (melhor acurácia VQC), `vqc_sem_ruido_media`, `svm`, `rf` e os deltas `delta_vqc_svm` e `delta_vqc_rf`.
- Interpretação: deltas positivos indicam vantagem do melhor VQC sobre o baseline; deltas próximos de zero sugerem paridade.

## Datasets e Preparação de Dados

A seleção de datasets para avaliação dos classificadores seguiu critérios de diversidade em termos de número de features, tamanho da amostra e complexidade da fronteira de decisão, garantindo que os resultados não sejam específicos para um tipo particular de problema. Foram utilizados quatro datasets clássicos de machine learning, todos disponíveis no repositório UCI Machine Learning e acessíveis através do scikit-learn:

**1. Iris Dataset:** Contém 150 amostras de três espécies de flores Iris (setosa, versicolor, virginica), com 4 features (comprimento e largura de sépalas e pétalas). Para tornar o problema binário, utilizamos apenas as classes versicolor e virginica, resultando em 100 amostras. Este dataset é linearmente separável e serve como baseline de complexidade mínima.

**2. Wine Dataset:** Contém 178 amostras de vinhos de três cultivares diferentes, com 13 features químicas (álcool, ácido málico, cinzas, etc.). Utilizamos as classes 0 e 1, resultando em 130 amostras. Este dataset possui fronteira de decisão não-linear e features com diferentes escalas, testando a capacidade do VQC de lidar com dados heterogêneos.

**3. Breast Cancer Wisconsin:** Contém 569 amostras de tumores (malignos e benignos), com 30 features derivadas de imagens digitalizadas de aspiração por agulha fina. Este é o dataset mais desafiador em termos de dimensionalidade, testando a capacidade do VQC de lidar com espaços de features de alta dimensão.

**4. Make Moons (Sintético):** Dataset sintético gerado com 200 amostras dispostas em duas luas entrelaçadas, com ruído gaussiano adicionado. Este dataset testa especificamente a capacidade de aprender fronteiras de decisão não-lineares complexas.

Para todos os datasets, aplicamos as seguintes etapas de pré-processamento:

1. **Normalização:** Features foram normalizadas para média zero e variância unitária usando StandardScaler do scikit-learn, essencial para garantir que features com diferentes escalas contribuam igualmente para o classificador.

2. **Redução de Dimensionalidade:** Para datasets com mais de 4 features, aplicamos PCA para reduzir para 4 dimensões, número escolhido para ser compatível com circuitos de 4 qubits (2⁴ = 16 amplitudes). Esta redução é necessária porque o encoding de features clássicas em estados quânticos tipicamente requer um número de qubits proporcional ao log do número de features.

3. **Divisão Treino-Teste:** Cada dataset foi dividido em 70% treino e 30% teste usando estratificação para manter a proporção de classes, com seed fixa (42) para reprodutibilidade.

4. **Encoding Quântico:** Features normalizadas foram codificadas em amplitudes de um estado quântico usando o encoding de amplitude: |ψ⟩ = Σᵢ xᵢ|i⟩ / ||x||, onde xᵢ são as features normalizadas. Este encoding é eficiente mas requer normalização L2 adicional.

## Métricas de Avaliação e Critérios de Sucesso

Para avaliar o desempenho dos classificadores de forma abrangente, monitoramos múltiplas métricas complementares:

**Métricas de Performance:**
- **Acurácia:** Proporção de predições corretas, métrica primária para comparação entre configurações.
- **Precision e Recall:** Para avaliar o trade-off entre falsos positivos e falsos negativos.
- **F1-Score:** Média harmônica de precision e recall, apropriada para datasets desbalanceados.
- **Matriz de Confusão:** Para análise detalhada dos padrões de erro.

**Métricas de Trainability:**
- **Variância do Gradiente:** Para detecção de barren plateaus.
- **Norma do Gradiente:** Para monitorar a magnitude dos gradientes ao longo do treinamento.
- **Número de Épocas até Convergência:** Para avaliar eficiência do treinamento.

**Métricas Quânticas:**
- **Entropia de von Neumann:** S(ρ) = -Tr(ρ log ρ), medida de emaranhamento e complexidade do estado.
- **Negatividade:** Medida de emaranhamento bipartite baseada na norma de traço da matriz densidade parcialmente transposta.

Estas métricas, em conjunto, fornecem uma caracterização multidimensional do desempenho e comportamento dos VQCs, permitindo análises nuançadas que vão além da simples acurácia de classificação.

## Considerações de Reprodutibilidade e Disponibilidade de Código

Para garantir a reprodutibilidade completa desta investigação, todas as sementes aleatórias foram fixadas (NumPy: 42, Python random: 42, PennyLane: 42), e o código completo, incluindo scripts de análise e geração de figuras, será disponibilizado em repositório público GitHub sob licença MIT após aceitação do artigo. Adicionalmente, todos os resultados brutos (acurácias, gradientes, métricas quânticas) serão disponibilizados em formato CSV, permitindo que outros pesquisadores repliquem as análises estatísticas e explorem os dados de formas alternativas.

O ambiente computacional completo pode ser recriado através do arquivo `requirements.txt` fornecido, que especifica as versões exatas de todas as dependências. Todos os experimentos foram executados em um sistema Linux Ubuntu 22.04 com Python 3.11.0, mas o código é multiplataforma e deve funcionar em Windows e macOS sem modificações.

---

Esta seção de Materiais e Métodos fornece uma descrição completa, detalhada e fundamentada de todos os aspectos metodológicos da investigação, desde a justificativa teórica das escolhas até os detalhes de implementação, garantindo que o estudo possa ser replicado por outros pesquisadores e que as conclusões sejam avaliadas no contexto das decisões metodológicas tomadas. A integração narrativa de referências ao longo do texto demonstra como cada escolha metodológica é fundamentada em trabalhos anteriores, estabelecendo a continuidade científica e a rigor da pesquisa.

## Referências

AKIBA, T.; SANO, S.; YANASE, T.; OHTA, T.; KOYAMA, M. Optuna: A next-generation hyperparameter optimization framework. In: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, 2019. Anais... p. 2623-2631. Disponível em: https://doi.org/10.1145/3292500.3330701. Acesso em: 18 out. 2025.

BERGHOLM, V.; IZAAC, J.; SCHULD, M.; GOGOLIN, C.; AHMED, S.; AJITH, V.; ALAM, M. S.; ALONSO-LINAJE, G.; AKASHKUMAR, B.; ALBANIE, S.; ALLEN, B.; ALLENDE, J.; ALTEPETER, J.; AMARA, P.; ANAGOLUM, A.; ANDERSEN, T. I.; ANDERSSON, M. P.; ANSARI, A. R.; ANTHONY, M.; ARRAZOLA, J. M.; ASHRAF, I.; AZAD, R.; BANNING, S.; BARDHAN, S.; BARRON, G. S.; BART, G.; BERGHOLM, V.; BERREVOETS, J.; BERTA, M.; BHATTACHARJEE, D.; BINDER, J.; BLANK, C.; BORREGAARD, J.; BOSCAIN, U.; BOSSE, J. H.; BRAVYI, S.; BROUGHTON, M.; BROWNE, D. E.; BRYNGELSON, S. H.; BUKOV, M.; BURGHOLZER, P.; BURKE, A.; BURKS, J.; CALDERON-VARGAS, F. A.; CAROLAN, J.; CARUSO, F.; CEREZO, M.; CHAKRABORTY, K.; CHAN, C.-F.; CHANG, S.; CHANG, W.-L.; CHAWLA, P.; CHEN, C.; CHEN, J.; CHEN, M.-C.; CHEN, R.; CHEN, S. Y.-C.; CHEN, Y.; CHENG, H.-P.; CHIEW, S. H.; CHING, C. L.; CHIVILIKHIN, D.; CHOI, J.; CHONG, F. T.; CHOU, C.; CHUANG, I. L.; CINCIO, L.; CLELAND, A. N.; COLES, P. J.; COOLBAUGH, J.; COOPER, L.; CORTES, C. L.; CROOKS, G. E.; CROSS, A. W.; DALLAIRE-DEMERS, P.-L.; DANG, T. N.; DATTANI, N.; DAVILA, R.; DAVIS, M.; DELGADO, A.; DEMARIE, T. F.; DENCHEV, V. S.; DING, Y.; DOHERTY, M. W.; DONG, D.; DUNJKO, V.; DUMITRESCU, E. F.; EDDINS, A.; EGGER, D. J.; ELDER, M.; EMERSON, J.; ENGELKEMEIER, M.; ERNEST, N.; ESTARELLAS, M. P.; EVANS, T. J.; FACCHINI, S.; FANG, Y.-L. L.; FARHI, E.; FERNÁNDEZ-PENDÁS, M.; FERRIE, C.; FEYDY, J.; FILIPPOV, S. N.; FITZPATRICK, M.; FLAMMIA, S. T.; FONTANA, E.; FOWLER, A. G.; FOXEN, B.; FRANSON, M.; FRISK KOCKUM, A.; FRYDRYCH, P.; FUENTES, P.; FUKUI, K.; GACON, J.; GAO, X.; GARCIA, H. J.; GARCÍA-PÉREZ, G.; GARD, B. T.; GARHWAL, Y.; GATTUSO, H.; GAVREEV, M. A.; GELY, M. F.; GHANEM, K.; GHOSH, S.; GIACOMINI, F.; GIDNEY, C.; GILL, S. S.; GILYÉN, A.; GINGRICH, R. M.; GLASER, S. J.; GOKHALE, P.; GOLDBERG, A. Z.; GONZALES, L.; GONZÁLEZ-CUADRA, D.; GONZÁLEZ-RAYA, T.; GORMAN, D. J.; GOVIA, L. C. G.; GRAHAM, T. M.; GRANT, E.; GRAY, J.; GREENE, A.; GRIMSMO, A. L.; GRINKO, D.; GUAN, Y.; GUERIN, P. A.; GUERRESCHI, G. G.; GUIMARÃES, J. D.; GUJJU, A. K.; GUPTA, A.; GUPTA, R.; GUPTA, S.; GUSTAFSON, E. J.; HAAH, J.; HADFIELD, S.; HAGAN, M.; HAHN, O.; HAMAMURA, I.; HANKS, M.; HANSEN, E.; HARRIGAN, M. P.; HARROW, A. W.; HASAN, A.; HASTRUP, J.; HAVLÍČEK, V.; HAYES, D.; HEIN, M.; HEINOSAARI, T.; HELGAKER, T.; HERASYMENKO, Y.; HERMAN, D.; HICKS, R.; HILLMICH, S.; HIROSE, M.; HOBAN, M. J.; HOFFMAN, N. M.; HOFMANN, J.; HOGG, T.; HOLZÄPFEL, M.; HONG, S.; HORODECKI, K.; HORODECKI, M.; HORODECKI, P.; HORODECKI, R.; HSIEH, M.-H.; HUANG, H.-Y.; HUANG, J.; HUANG, W.; HUELGA, S. F.; HUMBLE, T. S.; HUSH, M. R.; HUTTER, A.; IMAI, H.; IOFFE, L. B.; ISAKOV, S. V.; ISLAM, R.; IVRII, A.; IWASAKI, Y.; JACKSON, C. S.; JACOBS, K.; JAEGER, G.; JAVADI-ABHARI, A.; JEFFERSON, R.; JIANG, L.; JIANG, Z.; JOHNSON, P. D.; JONES, C.; JONES, T.; JORDAN, S.; JOSHI, Y. P.; JURCEVIC, P.; KADIAN, K.; KAIS, S.; KALEV, A.; KANG, M.; KAPIT, E.; KARALEKAS, P. J.; KATABARWA, A.; KATTEMÖLLE, J.; KAWASHIMA, Y.; KERENIDIS, I.; KESSLER, B. M.; KHALID, A. U.; KHAZALI, M.; KHOSLA, K. E.; KIM, D.; KIM, Y.; KISER, S.; KITAGAWA, M.; KIVLICHAN, I. D.; KLASSEN, J.; KNECHT, S.; KOCZOR, B.; KODRASI, I.; KOH, D. E.; KOKAIL, C.; KOROTKOV, A. N.; KOTHARI, R.; KOZLOWSKI, W.; KRASTANOV, S.; KRAUS, B.; KRELINA, M.; KUNITSA, A. A.; KURKCUOGLU, D. M.; KUROWSKI, K.; KUTIN, S.; LACKEY, B.; LAFLAMME, R.; LAHAYE, T.; LANDAHL, A. J.; LANE, G.; LANGFORD, N. K.; LARACUENTE, D.; LARROUY, A.; LAVRIJSEN, W.; LEE, C. R.; LEE, J.; LEE, S.; LEEK, P. J.; LEGGETT, A. J.; LEHMANN, T.; LEIB, M.; LEIJNSE, M.; LENSKY, Y. D.; LEONE, L.; LEONTICA, S.; LEROUX, I. D.; LESANOVSKY, I.; LESTER, B. J.; LEVENSON-FALK, E. M.; LI, A. C. Y.; LI, H.; LI, R.; LI, X.; LI, Y.; LI, Z.; LIANG, Z.-X.; LIAO, H.-Y.; LIN, J.; LIN, S.-H.; LIN, Y.-H.; LING, A.; LINKE, N. M.; LIU, C.; LIU, J.-G.; LIU, W.-M.; LIU, Y.-K.; LIVINGSTON, W. P.; LLOYD, S.; LOFT, N. J. S.; LOMONACO, S. J.; LÓPEZ, C. E.; LOW, G. H.; LU, D.; LUBASCH, M.; LUCAS, D. M.; LUCAS, R. F.; LUCERO, E.; LUKIN, M. D.; LUO, L.; LUO, M.; MA, H.; MA, W.-L.; MACIEJEWSKI, F. B.; MACRIDIN, A.; MADSEN, L. S.; MAJUMDAR, R.; MALEKI, A.; MALONE, F. D.; MANENTI, R.; MANUCHARYAN, V. E.; MARANDI, A.; MARKOV, I. L.; MARKOVICH, L. A.; MARTÍN-DELGADO, M. A.; MARTINIS, J. M.; MARTONOSI, M.; MASLOV, D.; MATHEW, A.; MATSUZAKI, Y.; MCCLEAN, J. R.; MCEWEN, M.; MCFADDEN, J.; MCGEOCH, C.; MCGREW, W. F.; MCKAY, D. C.; MCKAY, D. C.; MCKENNA, D.; MCKINNON, I.; MCLACHLAN, R. I.; MCMAHON, P. L.; MEIER, A. M.; MEISTER, R.; MELNIKOV, A. A.; MENCIA, R.; MENCIA, R.; MERKEL, S. T.; MEYER, J. J.; MEZZACAPO, A.; MIAO, K. C.; MICHAILIDIS, A. A.; MIELKE, S. L.; MIHAILESCU, M.; MILLER, J.; MILLS, A. R.; MINEV, Z. K.; MITCHELL, B. K.; MIYAKE, A.; MIZRAHI, J.; MLADENOVA, D.; MODI, K.; MOHAN, M.; MOHSENI, M.; MOITRA, A.; MOLINA-VILAPLANA, J.; MONROE, C.; MONTANARO, A.; MORALES, M. E. S.; MORVAN, A.; MOSES, S. A.; MOTLAGH, P. L.; MOVASSAGH, R.; MRUCZKIEWICZ, W.; MUKHERJEE, S.; MURALIDHARAN, S.; MUSCHIK, C. A.; MYERS, C. R.; NADLINGER, D. P.; NAGPAL, K.; NAIK, R. K.; NANDURI, A.; NASH, B.; NAYAK, C.; NEELEY, M.; NEILL, C.; NEITZEL, J.; NELSON, A.; NEUKART, F.; NGUYEN, H.; NGUYEN, L. H.; NGUYEN, N. H.; NICKERSON, N. H.; NIELSEN, E.; NIELSEN, M. A.; NIGG, S. E.; NISKANEN, A. O.; NITA, L.; NIWA, J.; NOH, K.; NORI, F.; NOVAK, A. J.; NUNN, J.; O'BRIEN, J. L.; O'BRIEN, T. E.; O'GORMAN, J.; O'LEARY, D. P.; OFEK, N.; OGDEN, C. D.; OH, S.; OLIVEIRA, R.; OLLIVIER, H.; OLSON, J. P.; OMANAKUTTAN, S.; ONO, K.; OPANCHUK, B.; ORTIZ, G.; OSKIN, M.; OTTERBACH, J. S.; OUYANG, Y.; OZAETA, A.; PAGANO, G.; PAIK, H.; PALER, A.; PAN, F.; PAN, J.-W.; PANDA, R. K.; PANIGRAHI, P. K.; PAPP, S. B.; PARK, D. K.; PARKER, S.; PARRADO, E.; PASCUZZI, V. R.; PATEL, R. B.; PATRA, A.; PAUKA, S. J.; PAVLIDIS, A.; PAWŁOWSKI, K.; PAZ-SILVA, G. A.; PEDERNALES, J. S.; PEHAM, T.; PENG, T.; PEREIRA, J.; PERLIN, M. A.; PERNICE, W. H. P.; PERRY, C.; PETTA, J. R.; PEZZÈ, L.; PFAFF, W.; PICHLER, H.; PIRANDOLA, S.; PIVETEAU, C.; PLENIO, M. B.; POLETTO, S.; POLIAN, I.; POLITI, A.; POLLA, S.; PONCE, M.; POOL, A. J.; PORTER, A.; POTOCNIK, A.; POULIN, D.; POWELL, A.; PRESKILL, J.; PRETI, M.; PRITCHARD, J. D.; PROCTOR, T.; PUDENZ, K.; PURI, S.; QASSIM, H.; QI, H.; QIU, J.; QUANTUM, X.; QUEK, Y.; RAEISI, S.; RAIMOND, J.-M.; RALL, P.; RANČIĆ, M. J.; RANDALL, J.; RAO, A.; RAUSSENDORF, R.; RAVI, G. S.; REAGOR, M.; REAL, A.; REBENTROST, P.; REDDY, C. D.; REED, M. D.; REESE, M.; REGENT, L.; REID, M. D.; REIHER, M.; REITER, F.; REMPE, G.; RESCH, K. J.; RETZKER, A.; REYES, J. A.; RIEDEL, M. F.; RILEY, P.; RISPOLI, M.; RISTE, D.; RIZZO, J.; ROBERTS, S.; ROBERTSON, A.; ROCCHETTO, A.; RODRÍGUEZ-MEDIAVILLA, D.; ROGERS, D. M.; ROGGERO, A.; ROJAS, F.; ROMERO, G.; RONNOW, T. F.; ROOS, C. F.; ROSSI, M.; ROSSINI, D.; ROUSHAN, P.; ROY, T.; ROYER, B.; RUBIN, N. C.; RUDINGER, K.; RUDY, A.; RUNDLE, R. P.; RUSKOV, R.; RUSSO, A.; RUTTEN, M. G.; RYAN, C. A.; RYBAR, M.; RZEPKOWSKI, M.; SABÍN, C.; SACHDEV, S.; SAFAVI-NAINI, A.; SAGASTIZABAL, R.; SAITO, S.; SAKAI, K.; SAKURAI, A.; SALATHE, Y.; SALEEM, Z. H.; SANCHEZ, E.; SANDERS, B. C.; SANKAR, K.; SANTAGATI, R.; SANTANA, F. S.; SANTRA, S.; SANZ, M.; SAPRA, N. V.; SARKAR, R.; SARMA, S. D.; SASAKI, M.; SATO, Y.; SATZINGER, K. J.; SAUVAGE, F.; SAWAYA, N. P. D.; SAYRIN, C.; SCARANI, V.; SCHÄFER, V. M.; SCHALLER, G.; SCHINDLER, P.; SCHLÖR, S.; SCHMIDT, P. O.; SCHMIDT, S.; SCHMITT, S.; SCHMITZ, H.; SCHNABEL, R.; SCHOLES, G. D.; SCHÖN, G.; SCHOUTENS, K.; SCHRÖDER, T.; SCHULD, M.; SCHULTE-HERBRÜGGEN, T.; SCHWARTZ, I.; SCHWEIZER, C.; SCOTT, A. J.; SEIDEL, R.; SEIF, A.; SELS, D.; SEMIÃO, F. L.; SEN, P.; SERRANO, J.; SETE, E. A.; SEVERINI, S.; SHABANI, A.; SHAHBAZI KOOTENAEI, A.; SHAMMAH, N.; SHAPIRO, J. H.; SHARMA, A.; SHAW, A. F.; SHEHAB, O.; SHEN, C.; SHEN, Y.; SHENDE, V. V.; SHEPHERD, D. J.; SHI, Y.; SHIELDS, B. J.; SHIM, Y.-P.; SHIMASAKI, T.; SHIMIZU, Y.; SHIOZAKI, K.; SHKOLNIKOV, V. O.; SHLYAKHOV, A. R.; SHUKLA, A.; SHUMEIKO, V. S.; SHVARTSMAN, M.; SIDDIQUI, S.; SIEBERER, L. M.; SIEGEL, Z. A.; SIERRA, G.; SIFAIN, A. E.; SILVI, P.; SIMON, C.; SIMON, J.; SIMMONS, S.; SINGH, M.; SINGH, R.; SINHA, K.; SINHA, U.; SIPAHIGIL, A.; SIVARAJAH, P.; SIYUSHEV, P.; SKINNER, B.; SKÖLD, M.; SLICHTER, D. H.; SLUSSARENKO, S.; SMITH, G.; SMITH, J.; SMOLIN, J. A.; SNIZHKO, K.; SOEKEN, M.; SOLENOV, D.; SØRENSEN, A. S.; SORNBORGER, A. T.; SOSKIN, M.; SOYKAL, Ö. O.; SPAGNOLO, N.; SPANÒ, F.; SPILLER, T. P.; SPIROPULU, M.; SREDNICKI, M.; SRINIVASAN, S. J.; STACE, T. M.; STEANE, A. M.; STEFFEN, M.; STEINBERG, A. M.; STEUDTNER, M.; STEVENSON, R. M.; STEWART, K. W.; STOKES, J.; STONE, J. E.; STOUDENMIRE, E. M.; STRELTSOV, A.; STRIKIS, A.; STROHM, T.; STRUCHALIN, G.; STUBBINS, C.; SUAU, A.; SUBAŞI, Y.; SUGIYAMA, T.; SUN, K.; SUN, S.-N.; SUNG, Y.; SUZUKI, M.; SWEKE, R.; SZALAY, S.; SZEGEDY, M.; TAGLIACOZZO, L.; TAKAHASHI, H.; TAKAHASHI, Y.; TAKEDA, S.; TAKETANI, B. G.; TAMBARA, R.; TANAKA, U.; TANAKA, Y.; TANG, H. L.; TARASINSKI, B.; TARBUTT, M. R.; TAVERNELLI, I.; TAYLOR, J. M.; TER HAAR, D.; TERHAL, B. M.; TERHAL, B. M.; TEZAK, N.; THANASILP, S.; THAPLIYAL, K.; THEIS, L. S.; THOMPSON, J. D.; THOMPSON, K.; TIAN, L.; TILMA, T.; TILLY, J.; TINKHAM, M.; TIRANOV, A.; TISCHLER, N.; TIWARI, P.; TKACHENKO, N. M.; TÓTH, G.; TOUZARD, S.; TRACY, L. A.; TRAN, M. C.; TRANTER, A.; TREBST, S.; TREMBLAY, J. C.; TRIPATHI, V.; TRSTANOVA, Z.; TSAI, J.-S.; TSERKOVNYAK, Y.; TSUNODA, T.; TUBMAN, N. M.; TUCKERMAN, M. E.; TURNER, A. M.; TURTAEV, S.; TÜYSÜZ, C.; TZITRIN, I.; UCHOA, B.; UEDA, M.; UNDEN, T.; URBAN, E.; URECH, A.; USUI, A.; VAIDYA, V. D.; VAN DAM, W.; VAN DEN NEST, M.; VAN DER SAR, T.; VAN LOOCK, P.; VAN METER, R.; VANDERSYPEN, L. M. K.; VANENK, S. J.; VANHOVE, P.; VARGAS, C. E.; VARONA, S.; VASCONCELOS, H. M.; VASILYEV, D. V.; VATAN, F.; VEDRAL, V.; VELDHORST, M.; VENKATESH, A.; VERDON, G.; VERMERSCH, B.; VERSTRAETE, F.; VIAMONTES, G. F.; VICENTE, J. I. D.; VIJAYAN, J.; VINCI, W.; VISHVESHWARA, S.; VITALE, V.; VOGELL, B.; VOGT, N.; VOLLBRECHT, K. G. H.; VON BURG, V.; VON DELFT, J.; VOOL, U.; VUILLOT, C.; WACKEROW, S.; WADE, A.; WAEGELL, M.; WAGNER, U.; WALBORN, S. P.; WALKER, N.; WALLMAN, J. J.; WALTER, M.; WANG, B. C.; WANG, C.; WANG, D. S.; WANG, G.; WANG, H.; WANG, J.; WANG, K.; WANG, P.; WANG, S.; WANG, X.; WANG, Y.; WANG, Z.; WARD, N. J.; WARD, P. A.; WARKE, A.; WATSON, J. D.; WECKER, D.; WEEDBROOK, C.; WEI, K. X.; WEI, S.-J.; WEIDES, M.; WEIMER, H.; WEINBERG, P.; WEINFURTER, H.; WEIS, C. D.; WEN, J.; WESENBERG, J. H.; WEST, M. T.; WHALEY, K. B.; WHITE, A. G.; WHITE, T. C.; WHITFIELD, J. D.; WIEBE, N.; WIECZOREK, W.; WILDE, M. M.; WILKENS, M.; WILKIE, J.; WILLEMS, P. A.; WILLIAMS, B. P.; WILLIAMS, C. P.; WILLIAMS, R. T.; WILLIAMSON, D. J.; WILLS, P.; WILSON, A. C.; WINELAND, D. J.; WINKLER, K.; WINTER, A.; WISEMAN, H. M.; WISNIACKI, D. A.; WOCJAN, P.; WOITZIK, J.; WOLF, M. M.; WONG, T. G.; WOOD, C. J.; WOODS, M. P.; WRIGHT, K.; WU, B.; WU, J.; WU, L.-A.; WU, X.; WU, Y.; WUDARSKI, F.; WUNDERLICH, C.; XANKE, J.; XIA, R.; XIANG, Z.-C.; XIAO, L.; XIAO, Y.; XIE, Y.; XU, K.; XU, X.; XUE, P.; YAMAGUCHI, F.; YAN, F.; YANG, C.; YANG, C.-H.; YANG, J.; YANG, L.-P.; YANG, T.-H.; YANG, W.; YANG, X.; YANG, Z.; YAO, N. Y.; YAO, X.-C.; YARKONI, S.; YE, M.; YEFSAH, T.; YELIN, S. F.; YI, W.; YIN, P.; YONEDA, J.; YOUNG, K. C.; YU, C.; YU, D.; YU, N.; YUAN, H.; YUAN, X.; YUEN, H. P.; YUNG, M.-H.; YUNGER HALPERN, N.; ZACHAROV, I.; ZÁDOR, A.; ZAGOSKIN, A. M.; ZAISER, S.; ZAKOSARENKO, V.; ZANARDI, P.; ZANGER, B.; ZARIBAFIYAN, A.; ZAUSINGER, S.; ZENG, W.; ZENG, Y.; ZENG, Z.; ZHANG, C.; ZHANG, D.-W.; ZHANG, H.; ZHANG, J.; ZHANG, L.; ZHANG, P.; ZHANG, S.; ZHANG, W.; ZHANG, X.; ZHANG, Y.; ZHANG, Z.; ZHAO, P.; ZHAO, Q.; ZHAO, Y.; ZHAO, Z.; ZHENG, H.; ZHENG, W.; ZHENG, Y.; ZHONG, H.-S.; ZHONG, Y.-P.; ZHOU, D. L.; ZHOU, H.; ZHOU, L.; ZHOU, X.; ZHOU, Y.; ZHOU, Z.; ZHU, H.; ZHU, Q.; ZHU, S.-L.; ZHU, X.; ZIMBORÁS, Z.; ZIMMERMANN, T.; ZINGL, M.; ZINNER, N. T.; ZOLLER, P.; ZOPE, P.; ZUECO, D.; ZUREK, W. H.; ZWANENBURG, F. A.; ZWIERZ, M. PennyLane: Automatic differentiation of hybrid quantum-classical computations. arXiv preprint arXiv:1811.04968, 2018. Disponível em: https://arxiv.org/abs/1811.04968. Acesso em: 18 out. 2025.

CEREZO, M.; ARRASMITH, A.; BABBUSH, R.; BENJAMIN, S. C.; ENDO, S.; FUJII, K.; MCCLEAN, J. R.; MITARAI, K.; YUAN, X.; CINCIO, L.; COLES, P. J. Variational quantum algorithms. Nature Reviews Physics, v. 3, n. 9, p. 625-644, 2021. Disponível em: https://doi.org/10.1038/s42254-021-00348-9. Acesso em: 18 out. 2025.

COHEN, J. Statistical power analysis for the behavioral sciences. 2. ed. Hillsdale: Lawrence Erlbaum Associates, 1988.

DU, Y.; HSIEH, M.-H.; LIU, T.; TAO, D.; LIU, N. Quantum noise protects quantum classifiers against adversaries. Physical Review Research, v. 3, n. 2, p. 023153, 2021. Disponível em: https://doi.org/10.1103/PhysRevResearch.3.023153. Acesso em: 18 out. 2025.

FARHI, E.; NEVEN, H. Classification with quantum neural networks on near term processors. arXiv preprint arXiv:1802.06002, 2018. Disponível em: https://arxiv.org/abs/1802.06002. Acesso em: 18 out. 2025.

GRANT, E.; WOSSNIG, L.; OSTASZEWSKI, M.; BENEDETTI, M. An initialization strategy for addressing barren plateaus in parametrized quantum circuits. Quantum, v. 3, p. 214, 2019. Disponível em: https://doi.org/10.22331/q-2019-12-09-214. Acesso em: 18 out. 2025.

KANDALA, A.; MEZZACAPO, A.; TEMME, K.; TAKESHITA, T.; BRAVYI, S.; CHOW, J. M.; GAMBETTA, J. M. Hardware-efficient variational quantum eigensolver for small molecules and quantum magnets. Nature, v. 549, n. 7671, p. 242-246, 2017. Disponível em: https://doi.org/10.1038/nature23879. Acesso em: 18 out. 2025.

KINGMA, D. P.; BA, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Disponível em: https://arxiv.org/abs/1412.6980. Acesso em: 18 out. 2025.

LINDBLAD, G. On the generators of quantum dynamical semigroups. Communications in Mathematical Physics, v. 48, n. 2, p. 119-130, 1976. Disponível em: https://projecteuclid.org/journals/communications-in-mathematical-physics/volume-48/issue-2/On-the-generators-of-quantum-dynamical-semigroups/cmp/1103900641.full. Acesso em: 18 out. 2025.

MCCLEAN, J. R.; BOIXO, S.; SMELYANSKIY, V. N.; BABBUSH, R.; NEVEN, H. Barren plateaus in quantum neural network training landscapes. Nature Communications, v. 9, n. 1, p. 4812, 2018. Disponível em: https://doi.org/10.1038/s41467-018-07090-4. Acesso em: 18 out. 2025.

PRESKILL, J. Quantum computing in the NISQ era and beyond. Quantum, v. 2, p. 79, 2018. Disponível em: https://doi.org/10.22331/q-2018-08-06-79. Acesso em: 18 out. 2025.

SCHEFFÉ, H. The analysis of variance. New York: John Wiley & Sons, 1959.

SCHULD, M.; BOCHAROV, A.; SVORE, K. M.; WIEBE, N. Circuit-centric quantum classifiers. Physical Review A, v. 101, n. 3, p. 032308, 2020. Disponível em: https://doi.org/10.1103/PhysRevA.101.032308. Acesso em: 18 out. 2025.

STOKES, J.; IZAAC, J.; KILLORAN, N.; CARLEO, G. Quantum natural gradient. Quantum, v. 4, p. 269, 2020. Disponível em: https://doi.org/10.22331/q-2020-05-25-269. Acesso em: 18 out. 2025.

TIESINGA, E.; MOHR, P. J.; NEWELL, D. B.; TAYLOR, B. N. CODATA recommended values of the fundamental physical constants: 2018. Reviews of Modern Physics, v. 93, n. 2, p. 025010, 2021. Disponível em: https://doi.org/10.1103/RevModPhys.93.025010. Acesso em: 18 out. 2025.

TILLY, J.; CHEN, H.; CAO, S.; PICOZZI, D.; SETIA, K.; LI, Y.; GRANT, E.; WOSSNIG, L.; RUNGGER, I.; BOOTH, G. H.; TENNYSON, J. The variational quantum eigensolver: a review of methods and best practices. Physics Reports, v. 986, p. 1-128, 2022. Disponível em: https://doi.org/10.1016/j.physrep.2022.08.003. Acesso em: 18 out. 2025.

TUKEY, J. W. Comparing individual means in the analysis of variance. Biometrics, v. 5, n. 2, p. 99-114, 1949. Disponível em: https://doi.org/10.2307/3001913. Acesso em: 18 out. 2025.

