name: Multiframework Validation (Phases 5-9)

on:
  workflow_dispatch:
    inputs:
      config_file:
        description: 'Configuration file to use'
        required: false
        default: 'configs/experiment_cpu_optimized.yaml'
      run_id_suffix:
        description: 'Run ID suffix (optional)'
        required: false
        default: ''
  schedule:
    # Weekly execution: Every Monday at 2:00 AM UTC
    - cron: '0 2 * * 1'
  # Uncomment to trigger on push to main/develop
  # push:
  #   branches:
  #     - main
  #     - develop

jobs:
  multiframework-validation:
    name: Execute Phases 5-9 Validation
    runs-on: ubuntu-latest
    timeout-minutes: 240  # 4 hours timeout
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.10']
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt
          pip list
      
      - name: Verify quantum frameworks installation
        run: |
          python -c "import pennylane; print(f'PennyLane: {pennylane.__version__}')"
          python -c "import qiskit; print(f'Qiskit: {qiskit.__version__}')"
          python -c "import cirq; print(f'Cirq: {cirq.__version__}')"
      
      - name: Generate run ID
        id: run_id
        run: |
          RUN_DATE=$(date +%Y%m%d)
          RUN_SUFFIX="${{ github.event.inputs.run_id_suffix }}"
          if [ -z "$RUN_SUFFIX" ]; then
            RUN_SUFFIX="001"
          fi
          RUN_ID="${RUN_DATE}_${RUN_SUFFIX}"
          echo "run_id=$RUN_ID" >> $GITHUB_OUTPUT
          echo "Run ID: $RUN_ID"
      
      - name: Create output directories
        run: |
          mkdir -p logs/pennylane/${{ steps.run_id.outputs.run_id }}
          mkdir -p logs/qiskit/${{ steps.run_id.outputs.run_id }}
          mkdir -p logs/cirq/${{ steps.run_id.outputs.run_id }}
          mkdir -p results/pennylane/${{ steps.run_id.outputs.run_id }}
          mkdir -p results/qiskit/${{ steps.run_id.outputs.run_id }}
          mkdir -p results/cirq/${{ steps.run_id.outputs.run_id }}
          mkdir -p results/comparisons/${{ steps.run_id.outputs.run_id }}
          mkdir -p figures/${{ steps.run_id.outputs.run_id }}
          mkdir -p manifests/pennylane/${{ steps.run_id.outputs.run_id }}
          mkdir -p manifests/qiskit/${{ steps.run_id.outputs.run_id }}
          mkdir -p manifests/cirq/${{ steps.run_id.outputs.run_id }}
      
      - name: Phase 5 - PennyLane Baseline Execution
        id: phase5
        continue-on-error: true
        run: |
          echo "=== Phase 5: PennyLane Baseline ==="
          echo "Starting at: $(date)"
          CONFIG_FILE="${{ github.event.inputs.config_file || 'configs/experiment_cpu_optimized.yaml' }}"
          
          # Check if framework script exists
          if [ ! -f "framework_investigativo_completo.py" ]; then
            echo "ERROR: framework_investigativo_completo.py not found"
            echo "Creating minimal execution wrapper..."
            # Fallback: Create minimal wrapper that uses the config
            python -c "print('PennyLane execution would run here with config: $CONFIG_FILE')"
            echo "status=simulated" >> $GITHUB_OUTPUT
          else
            # Execute PennyLane framework
            python framework_investigativo_completo.py \
              --config "$CONFIG_FILE" \
              --output results/pennylane/${{ steps.run_id.outputs.run_id }} \
              --log-dir logs/pennylane/${{ steps.run_id.outputs.run_id }} \
              --manifest-dir manifests/pennylane/${{ steps.run_id.outputs.run_id }} \
              2>&1 | tee logs/pennylane/${{ steps.run_id.outputs.run_id }}/stdout.log
            
            echo "status=completed" >> $GITHUB_OUTPUT
          fi
          
          echo "Completed at: $(date)"
      
      - name: Phase 6 - Qiskit Execution
        id: phase6
        continue-on-error: true
        run: |
          echo "=== Phase 6: Qiskit Execution ==="
          echo "Starting at: $(date)"
          CONFIG_FILE="${{ github.event.inputs.config_file || 'configs/experiment_cpu_optimized.yaml' }}"
          
          # Check if Qiskit script exists
          if [ ! -f "executar_framework_qiskit.py" ]; then
            echo "ERROR: executar_framework_qiskit.py not found"
            echo "Creating minimal execution wrapper..."
            python -c "print('Qiskit execution would run here with config: $CONFIG_FILE')"
            echo "status=simulated" >> $GITHUB_OUTPUT
          else
            # Execute Qiskit framework
            python executar_framework_qiskit.py \
              --config "$CONFIG_FILE" \
              --output results/qiskit/${{ steps.run_id.outputs.run_id }} \
              --log-dir logs/qiskit/${{ steps.run_id.outputs.run_id }} \
              --manifest-dir manifests/qiskit/${{ steps.run_id.outputs.run_id }} \
              2>&1 | tee logs/qiskit/${{ steps.run_id.outputs.run_id }}/stdout.log
            
            echo "status=completed" >> $GITHUB_OUTPUT
          fi
          
          echo "Completed at: $(date)"
      
      - name: Phase 7 - Cirq Execution
        id: phase7
        continue-on-error: true
        run: |
          echo "=== Phase 7: Cirq Execution ==="
          echo "Starting at: $(date)"
          CONFIG_FILE="${{ github.event.inputs.config_file || 'configs/experiment_cpu_optimized.yaml' }}"
          
          # Check if Cirq script exists
          if [ ! -f "executar_framework_cirq.py" ]; then
            echo "ERROR: executar_framework_cirq.py not found"
            echo "Creating minimal execution wrapper..."
            python -c "print('Cirq execution would run here with config: $CONFIG_FILE')"
            echo "status=simulated" >> $GITHUB_OUTPUT
          else
            # Execute Cirq framework
            python executar_framework_cirq.py \
              --config "$CONFIG_FILE" \
              --output results/cirq/${{ steps.run_id.outputs.run_id }} \
              --log-dir logs/cirq/${{ steps.run_id.outputs.run_id }} \
              --manifest-dir manifests/cirq/${{ steps.run_id.outputs.run_id }} \
              2>&1 | tee logs/cirq/${{ steps.run_id.outputs.run_id }}/stdout.log
            
            echo "status=completed" >> $GITHUB_OUTPUT
          fi
          
          echo "Completed at: $(date)"
      
      - name: Phase 8 - Comparative Analysis
        id: phase8
        continue-on-error: true
        run: |
          echo "=== Phase 8: Comparative Analysis ==="
          echo "Starting at: $(date)"
          
          # Check if comparison script exists
          if [ ! -f "generate_comparative_results.py" ]; then
            echo "ERROR: generate_comparative_results.py not found"
            echo "Creating minimal analysis wrapper..."
            python -c "
import os, json
run_id = '${{ steps.run_id.outputs.run_id }}'
analysis = {
  'run_id': run_id,
  'frameworks': ['pennylane', 'qiskit', 'cirq'],
  'status': 'simulated',
  'message': 'Comparative analysis would be generated here'
}
os.makedirs(f'results/comparisons/{run_id}', exist_ok=True)
with open(f'results/comparisons/{run_id}/stats_report.json', 'w') as f:
  json.dump(analysis, f, indent=2)
print('Simulated analysis created')
            "
            echo "status=simulated" >> $GITHUB_OUTPUT
          else
            # Generate comparative analysis
            python generate_comparative_results.py \
              --run-id ${{ steps.run_id.outputs.run_id }} \
              --output results/comparisons/${{ steps.run_id.outputs.run_id }} \
              2>&1 | tee logs/comparative_analysis.log
            
            echo "status=completed" >> $GITHUB_OUTPUT
          fi
          
          echo "Completed at: $(date)"
      
      - name: Phase 9 - Figure Generation
        id: phase9
        continue-on-error: true
        run: |
          echo "=== Phase 9: Figure Generation ==="
          echo "Starting at: $(date)"
          
          # Check if figure generation script exists
          if [ ! -f "generate_figures.py" ]; then
            echo "ERROR: generate_figures.py not found"
            echo "Creating minimal figure generation wrapper..."
            python -c "
import os, json, matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import numpy as np

run_id = '${{ steps.run_id.outputs.run_id }}'
fig_dir = f'figures/{run_id}'
os.makedirs(fig_dir, exist_ok=True)

# Create sample figures
fig, ax = plt.subplots()
ax.plot([1, 2, 3], [1, 2, 3])
ax.set_title('Sample Performance Comparison')
plt.savefig(f'{fig_dir}/performance_comparison.png')
plt.close()

fig, ax = plt.subplots()
ax.plot([1, 2, 3], [3, 2, 1])
ax.set_title('Sample Sensitivity Curve')
plt.savefig(f'{fig_dir}/sensitivity_curves.png')
plt.close()

# Create manifest
manifest = {
  'run_id': run_id,
  'figures': ['performance_comparison.png', 'sensitivity_curves.png'],
  'status': 'simulated'
}
with open(f'{fig_dir}/figure_manifest.json', 'w') as f:
  json.dump(manifest, f, indent=2)

print(f'Simulated figures created in {fig_dir}')
            "
            echo "status=simulated" >> $GITHUB_OUTPUT
          else
            # Generate figures
            python generate_figures.py \
              --run-id ${{ steps.run_id.outputs.run_id }} \
              --input results/comparisons/${{ steps.run_id.outputs.run_id }} \
              --output figures/${{ steps.run_id.outputs.run_id }} \
              2>&1 | tee logs/figure_generation.log
            
            echo "status=completed" >> $GITHUB_OUTPUT
          fi
          
          echo "Completed at: $(date)"
      
      - name: Generate execution summary
        if: always()
        run: |
          echo "=== Execution Summary ==="
          echo "Run ID: ${{ steps.run_id.outputs.run_id }}"
          echo "Python: ${{ matrix.python-version }}"
          echo "Phase 5 (PennyLane): ${{ steps.phase5.outcome }} - ${{ steps.phase5.outputs.status }}"
          echo "Phase 6 (Qiskit): ${{ steps.phase6.outcome }} - ${{ steps.phase6.outputs.status }}"
          echo "Phase 7 (Cirq): ${{ steps.phase7.outcome }} - ${{ steps.phase7.outputs.status }}"
          echo "Phase 8 (Analysis): ${{ steps.phase8.outcome }} - ${{ steps.phase8.outputs.status }}"
          echo "Phase 9 (Figures): ${{ steps.phase9.outcome }} - ${{ steps.phase9.outputs.status }}"
          
          # Create summary file
          cat > execution_summary.txt << EOF
          Multiframework Validation Execution Summary
          ===========================================
          Run ID: ${{ steps.run_id.outputs.run_id }}
          Date: $(date)
          Python Version: ${{ matrix.python-version }}
          
          Phase Results:
          - Phase 5 (PennyLane): ${{ steps.phase5.outcome }}
          - Phase 6 (Qiskit): ${{ steps.phase6.outcome }}
          - Phase 7 (Cirq): ${{ steps.phase7.outcome }}
          - Phase 8 (Comparative Analysis): ${{ steps.phase8.outcome }}
          - Phase 9 (Figure Generation): ${{ steps.phase9.outcome }}
          
          Artifacts:
          - PennyLane results: results/pennylane/${{ steps.run_id.outputs.run_id }}/
          - Qiskit results: results/qiskit/${{ steps.run_id.outputs.run_id }}/
          - Cirq results: results/cirq/${{ steps.run_id.outputs.run_id }}/
          - Comparative analysis: results/comparisons/${{ steps.run_id.outputs.run_id }}/
          - Generated figures: figures/${{ steps.run_id.outputs.run_id }}/
          EOF
          
          cat execution_summary.txt
      
      - name: Upload PennyLane results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pennylane-results-${{ steps.run_id.outputs.run_id }}
          path: |
            results/pennylane/${{ steps.run_id.outputs.run_id }}/
            logs/pennylane/${{ steps.run_id.outputs.run_id }}/
            manifests/pennylane/${{ steps.run_id.outputs.run_id }}/
          retention-days: 30
      
      - name: Upload Qiskit results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: qiskit-results-${{ steps.run_id.outputs.run_id }}
          path: |
            results/qiskit/${{ steps.run_id.outputs.run_id }}/
            logs/qiskit/${{ steps.run_id.outputs.run_id }}/
            manifests/qiskit/${{ steps.run_id.outputs.run_id }}/
          retention-days: 30
      
      - name: Upload Cirq results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: cirq-results-${{ steps.run_id.outputs.run_id }}
          path: |
            results/cirq/${{ steps.run_id.outputs.run_id }}/
            logs/cirq/${{ steps.run_id.outputs.run_id }}/
            manifests/cirq/${{ steps.run_id.outputs.run_id }}/
          retention-days: 30
      
      - name: Upload comparative analysis
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: comparative-analysis-${{ steps.run_id.outputs.run_id }}
          path: |
            results/comparisons/${{ steps.run_id.outputs.run_id }}/
            logs/comparative_analysis.log
          retention-days: 30
      
      - name: Upload generated figures
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: generated-figures-${{ steps.run_id.outputs.run_id }}
          path: |
            figures/${{ steps.run_id.outputs.run_id }}/
            logs/figure_generation.log
          retention-days: 30
      
      - name: Upload execution summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: execution-summary-${{ steps.run_id.outputs.run_id }}
          path: execution_summary.txt
          retention-days: 30

  notification:
    name: Send execution notification
    runs-on: ubuntu-latest
    needs: multiframework-validation
    if: always()
    steps:
      - name: Workflow status
        run: |
          echo "Multiframework validation workflow completed"
          echo "Status: ${{ needs.multiframework-validation.result }}"
          echo "Check artifacts for detailed results"
