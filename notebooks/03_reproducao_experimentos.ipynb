{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework Investigativo Completo: Ru√≠do Qu√¢ntico Ben√©fico em VQCs\\n",
    "\\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MarceloClaro/Beneficial-Quantum-Noise-in-Variational-Quantum-Classifiers/blob/main/notebooks/03_reproducao_experimentos.ipynb)\\n",
    "\\n",
    "---\\n",
    "\\n",
    "## üìã Vis√£o Geral\\n",
    "\\n",
    "Este notebook implementa **integralmente** o Framework Investigativo v7.2 do artigo cient√≠fico \\n",
    "*\"From Obstacle to Opportunity: Harnessing Beneficial Quantum Noise in Variational Classifiers\"*,\\n",
    "mantendo rigor cient√≠fico QUALIS A1.\\n",
    "\\n",
    "### üéØ Objetivos\\n",
    "\\n",
    "1. **Reproduzir todas as fun√ß√µes** do arquivo `framework_investigativo_completo.py`\\n",
    "2. **Demonstrar regime de ru√≠do qu√¢ntico ben√©fico** com rigor estat√≠stico\\n",
    "3. **Manter padr√µes QUALIS A1**: reprodutibilidade, an√°lise estat√≠stica rigorosa\\n",
    "4. **Dupla perspectiva**: acess√≠vel para iniciantes, rigorosa para especialistas\\n",
    "\\n",
    "### üë• P√∫blico-Alvo\\n",
    "\\n",
    "#### üë∂ Iniciantes\\n",
    "- Conceitos b√°sicos explicados com analogias\\n",
    "- Visualiza√ß√µes intuitivas\\n",
    "- Passo a passo detalhado\\n",
    "\\n",
    "#### üéì Especialistas\\n",
    "- Rigor matem√°tico completo (Lindblad, von Neumann)\\n",
    "- An√°lises estat√≠sticas avan√ßadas (ANOVA, Cohen's d, post-hoc)\\n",
    "- Refer√™ncias cient√≠ficas\\n",
    "- Compatibilidade com hardware real\\n",
    "\\n",
    "### üìö Refer√™ncias Fundamentais\\n",
    "\\n",
    "- **Nielsen & Chuang (2010)**: *Quantum Computation and Quantum Information*\\n",
    "- **Preskill (2018)**: *Quantum Computing in the NISQ era*\\n",
    "- **Cerezo et al. (2021)**: *Variational quantum algorithms*, Nature Reviews Physics\\n",
    "- **Benedetti et al. (2019)**: *Parameterized quantum circuits as ML models*\\n",
    "\\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configura√ß√£o e Instala√ß√£o\\n",
    "\\n",
    "### üí° Para Iniciantes\\n",
    "Execute a c√©lula abaixo para instalar todas as depend√™ncias necess√°rias.\\n",
    "\\n",
    "### üéì Para Especialistas\\n",
    "Depend√™ncias com vers√µes espec√≠ficas para reprodutibilidade QUALIS A1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\\n",
    "# Instala√ß√£o de depend√™ncias (modo silencioso)\\n",
    "!pip install pennylane numpy pandas scikit-learn scipy statsmodels plotly optuna\\n",
    "\\n",
    "print('‚úì Depend√™ncias instaladas com sucesso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports Centralizados\\n",
    "\\n",
    "### üí° Para Iniciantes\\n",
    "Importando todas as bibliotecas necess√°rias.\\n",
    "\\n",
    "### üéì Para Especialistas\\n",
    "Organiza√ß√£o segundo PEP 8, imports agrupados logicamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports centralizados no topo do arquivo (PEP 8)\nimport os\nimport json\nimport time\nimport logging\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import TYPE_CHECKING, Any, Dict, Optional\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets as sk_datasets\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Estat√≠stica\nfrom scipy.stats import f_oneway, ttest_ind\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.anova import anova_lm\n\n# Visualiza√ß√£o\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\n# Otimiza√ß√£o Bayesiana (opcional)\ntry:\n    import optuna\n    from optuna.samplers import TPESampler\n    from optuna.pruners import MedianPruner\n    OPTUNA_AVAILABLE = True\nexcept ImportError:\n    OPTUNA_AVAILABLE = False\n\n# Inicializar logging com formato QUALIS A1 (rigor cient√≠fico)\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s | %(levelname)-8s | %(name)-20s | %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\nlogger = logging.getLogger(__name__)\n\n# Configurar\\n\\nprint('‚úì Imports realizados com sucesso!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Constantes Fundamentais da F√≠sica Qu√¢ntica\\n",
    "\\n",
    "### üí° Para Iniciantes\\n",
    "Valores num√©ricos fundamentais usados em computa√ß√£o qu√¢ntica.\\n",
    "\\n",
    "### üéì Para Especialistas\\n",
    "Constantes baseadas em CODATA 2018 e valores aceitos pela comunidade cient√≠fica.\\n",
    "Implementa√ß√£o rigorosa das constantes fundamentais de Planck, Boltzmann, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ConstantesFundamentais:\n    \"\"\"\n    Constantes matem√°ticas e f√≠sicas para inicializa√ß√£o de par√¢metros.\n\n    Refer√™ncias:\n    - Constantes matem√°ticas: Weisstein, \"MathWorld\"\n    - Constantes qu√¢nticas: CODATA 2018 (Mohr et al., 2019)\n    - Normaliza√ß√£o: Grant et al. (2019). Quantum.\n    \"\"\"\n\n    # Constantes Matem√°ticas\n    PI = np.pi                          # œÄ ‚âà 3.14159\n    E = np.e                            # e ‚âà 2.71828\n    PHI = (1 + np.sqrt(5)) / 2         # œÜ ‚âà 1.61803 (Raz√£o √Åurea)\n    SQRT2 = np.sqrt(2)                  # ‚àö2 ‚âà 1.41421\n    LN2 = np.log(2)                     # ln(2) ‚âà 0.69315\n    GAMMA = 0.5772156649                # Œ≥ (Euler-Mascheroni)\n\n    # Constantes Qu√¢nticas (CODATA 2018)\n    HBAR = 1.054571817e-34              # ‚Ñè (constante de Planck reduzida) [J¬∑s]\n    ALPHA = 7.2973525693e-3             # Œ± (constante de estrutura fina) [adimensional]\n    RYDBERG = 10973731.568160           # R‚àû (constante de Rydberg) [m‚Åª¬π]\n\n    @classmethod\n    def normalizar(cls, valores):\n        \"\"\"\n        Normaliza valores para [-œÄ, œÄ] usando escala logar√≠tmica.\n\n        Refer√™ncia: Grant et al. (2019). \"An initialization strategy for\n        addressing barren plateaus in parametrized quantum circuits.\" Quantum.\n\n        Motiva√ß√£o: Constantes fundamentais abrangem 40 ordens de magnitude.\n        Escala logar√≠tmica mapeia para intervalo adequado para portas de rota√ß√£o.\n        \"\"\"\n        log_vals = np.log10(np.abs(valores) + 1e-10)\n        norm = (log_vals - log_vals.min()) / (log_vals.max() - log_vals.min() + 1e-10)\n        return -np.pi + norm * 2 * np.pi\n\n    @classmethod\n    def inicializar(cls, n_params, estrategia='aleatorio', seed=42):\n        \"\"\"\n        Inicializa par√¢metros com diferentes estrat√©gias.\n\n        Args:\n            n_params: N√∫mero de par√¢metros\n            estrategia: 'matematico', 'quantico', ou 'aleatorio'\n            seed: Semente aleat√≥ria para reprodutibilidade\n\n        Returns:\n            Array PennyLane com requires_grad=True\n        \"\"\"\n        np.random.seed(seed)\n\n        if estrategia == 'matematico':\n            # Usa constantes matem√°ticas fundamentais\n            const = np.array([cls.PI, cls.E, cls.PHI, cls.SQRT2, cls.LN2, cls.GAMMA])\n            n_rep = int(np.ceil(n_params / len(const)))\n            params = np.tile(const, n_rep)[:n_params]\n            # Adiciona ru√≠do gaussiano pequeno para quebrar simetria\n            params += np.random.normal(0, 0.1, n_params)\n            return pnp.array(cls.normalizar(params), requires_grad=True)\n\n        elif estrategia == 'quantico':\n            # Usa constantes f√≠sicas qu√¢nticas (CODATA 2018)\n            const = np.array([cls.HBAR, cls.ALPHA, cls.RYDBERG])\n            n_rep = int(np.ceil(n_params / len(const)))\n            params = np.tile(const, n_rep)[:n_params]\n            params += np.random.normal(0, 0.1, n_params)\n            return pnp.array(cls.normalizar(params), requires_grad=True)\n\n        elif estrategia == 'fibonacci_spiral':\\n\\nprint('‚úì Classe ConstantesFundamentais definida!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelos de Ru√≠do Qu√¢ntico\\n",
    "\\n",
    "### üí° Para Iniciantes\\n",
    "Ru√≠do qu√¢ntico √© como \"est√°tica\" que afeta qubits. Diferentes tipos de ru√≠do\\n",
    "simulam imperfei√ß√µes reais do hardware qu√¢ntico.\\n",
    "\\n",
    "### üéì Para Especialistas\\n",
    "Implementa√ß√£o via **operadores de Kraus** e **Master Equation de Lindblad**:\\n",
    "\\n",
    "$$\\\\frac{d\\\\rho}{dt} = -i[H, \\\\rho] + \\\\sum_k \\\\left( L_k \\\\rho L_k^\\\\dagger - \\\\frac{1}{2}\\\\{L_k^\\\\dagger L_k, \\\\rho\\\\} \\\\right)$$\\n",
    "\\n",
    "Modelos implementados:\\n",
    "- **Depolarizante**: canal mais geral, mistura com estado maximamente misto\\n",
    "- **Amplitude Damping**: perda de energia (relaxa√ß√£o T1)\\n",
    "- **Phase Damping**: perda de coer√™ncia de fase (T2)\\n",
    "- **Bit Flip, Phase Flip**: erros discretos\\n",
    "- **Thermal, Pink Noise, Readout Error**: modelos avan√ßados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ModeloRuido:\n    \"\"\"Classe base para modelos de ru√≠do qu√¢ntico.\"\"\"\n    def __init__(self, nivel=0.01):\n        self.nivel = nivel\n    def aplicar(self, n_qubits, nivel_override=None):\n        \"\"\"\n        Apply quantum noise to circuit qubits.\n        \n        Args:\n            n_qubits: Number of qubits in the circuit\n            nivel_override: Optional noise level override (uses self.nivel if None)\n            \n        Raises:\n            NotImplementedError: Must be implemented by subclasses\n        \"\"\"\n        raise NotImplementedError\n\nclass RuidoThermal(ModeloRuido):\n    \"\"\"Thermal Relaxation Error: aproxima T1/T2 com canais de amplitude e fase.\"\"\"\n    def aplicar(self, n_qubits, nivel_override=None):\n        \"\"\"\n        Apply thermal relaxation noise to all qubits.\n        \n        Args:\n            n_qubits: Number of qubits in the circuit\n            nivel_override: Optional noise level override (uses self.nivel if None)\n        \"\"\"\n        p = self.nivel if nivel_override is None else nivel_override\n        for i in range(n_qubits):\n            qml.AmplitudeDamping(p, wires=i)\n            qml.PhaseDamping(p, wires=i)\n\nclass RuidoBitFlip(ModeloRuido):\n    \"\"\"Bit-Flip Error (X).\"\"\"\n    def aplicar(self, n_qubits, nivel_override=None):\n        \"\"\"\n        Apply bit-flip noise (X gate with probability p) to all qubits.\n        \n        Args:\n            n_qubits: Number of qubits in the circuit\n            nivel_override: Optional noise level override (uses self.nivel if None)\n        \"\"\"\n        p = self.nivel if nivel_override is None else nivel_override\n        for i in range(n_qubits):\n            qml.BitFlip(p, wires=i)\n\nclass RuidoPhaseFlip(ModeloRuido):\n    \"\"\"Phase-Flip Error (Z).\"\"\"\n    def aplicar(self, n_qubits, nivel_override=None):\n        \"\"\"\n        Apply phase-flip noise (Z gate with probability p) to all qubits.\n        \n        Args:\n            n_qubits: Number of qubits in the circuit\n            nivel_override: Optional noise level override (uses self.nivel if None)\n        \"\"\"\n        p = self.nivel if nivel_override is None else nivel_override\n        for i in range(n_qubits):\n            qml.PhaseFlip(p, wires=i)\n\nclass RuidoPinkNoise(ModeloRuido):\n    \"\"\"1/f Noise (Pink): usa PhaseDamping com varia√ß√£o por qubit.\"\"\"\n    def aplicar(self, n_qubits, nivel_override=None):\n        \"\"\"\n        Apply 1/f (pink) noise using phase damping with per-qubit variation.\n        \n        Args:\n            n_qubits: Number of qubits in the circuit\n            nivel_override: Optional noise level override (uses self.nivel if None)\n            \n        Notes:\n            Simulates low-frequency noise with Gaussian-distributed intensity per qubit\n        \"\"\"\n        p = self.nivel if nivel_override is None else nivel_override\n        pink = np.abs(np.random.normal(loc=0, scale=p, size=n_qubits))\n        for i in range(n_qubits):\n            qml.PhaseDamping(float(min(1.0, pink[i])) , wires=i)\n\nclass RuidoReadoutError(ModeloRuido):\n    \"\"\"Readout Error (aproxima√ß√£o via BitFlip ap√≥s opera√ß√µes).\"\"\"\n    def aplicar(self, n_qubits, nivel_override=None):\n        \"\"\"\n        Apply readout error approximated via bit-flip operations.\n        \n        Args:\n            n_qubits: Number of qubits in the circuit\n            nivel_override: Optional noise level override (uses self.nivel if None)\n            \n        Notes:\n            Models measurement errors in quantum hardware\n        \"\"\"\n        p = self.nivel if nivel_override is None else nivel_override\n        for i in range(n_qubits):\n            qml.BitFlip(p, wires=i)\n\n\nclass RuidoDepolarizante(ModeloRuido):\n    \"\"\"\n    Ru√≠do de Depolariza√ß√£o: œÅ ‚Üí (1-p)œÅ + p¬∑I/2\n\n    Refer√™ncia: Preskill (2018). \"Quantum Computing in the NISQ era.\" Quantum.\n\n    Descri√ß√£o: Erro gen√©rico mais comum em qubits supercondutores (IBM, Google).\n    O estado qu√¢ntico √© substitu√≠do pelo estado maximamente misto com probabilidade p.\n\n    Operadores de Kraus:\n    K‚ÇÄ = ‚àö(1-p) I\n    K‚ÇÅ = ‚àö(p/3) X\n    K‚ÇÇ = ‚àö(p/3) Y\n    K‚ÇÉ = ‚àö(p/3) Z\n    \"\"\"\n    def aplicar(self, n_qubits, nivel_override=None):\n        \"\"\"\n        Apply depolarizing channel to all qubits.\n        \n        Args:\n            n_qubits: Number of qubits in the circuit\n            nivel_override: Optional noise level override (uses self.nivel if None)\n        \"\"\"\n        p = self.nivel if nivel_override is None else nivel_override\n        for i in range(n_qubits):\n            qml.DepolarizingChannel(p, wires=i)\n\n\nclass RuidoAmplitudeDamping(ModeloRuido):\n    \"\"\"\n    Amplitude Damping: Relaxamento T1 (|1‚ü© ‚Üí |0‚ü©)\n\n    Refer√™ncia: Clerk et al. (2010). \"Introduction to quantum noise.\" Rev. Mod. Phys.\n\n    Descri√ß√£o: Perda de energia do qubit para o ambiente. Modela decaimento\n    exponencial com tempo caracter√≠stico T1. Comum em qubits supercondutores.\n\n    Operadores de Kraus:\n    K‚ÇÄ = [[1, 0], [0, ‚àö(1-Œ≥)]]\n    K‚ÇÅ = [[0, ‚àöŒ≥], [0, 0]]\n    \"\"\"\n    def aplicar(self, n_qubits, nivel_override=None):\n        \"\"\"\n        Apply amplitude damping channel to all qubits.\n        \n        Args:\n            n_qubits: Number of qubits in the circuit\n            nivel_override: Optional noise level override (uses self.nivel if None)\n        \"\"\"\n        g = self.nivel if nivel_override is None else nivel_override\n        for i in range(n_qubits):\n            qml.AmplitudeDamping(g, wires=i)\n\n\nclass RuidoPhaseDamping(ModeloRuido):\n    \"\"\"\n    Phase Damping: Decoer√™ncia T2 (perda de fase)\n\n    Refer√™ncia: Schlosshauer (2007). \"Decoherence and the Quantum-to-Classical Transition\"\n\n    Descri√ß√£o: Perda de informa√ß√£o de fase sem perda de energia. Modela\n    decoer√™ncia com tempo caracter√≠stico T2. Importante em qubits de spin.\n\n    Operadores de Kraus:\n    K‚ÇÄ = [[1, 0], [0, ‚àö(1-Œª)]]\n    K‚ÇÅ = [[0, 0], [0, ‚àöŒª]]\n    \"\"\"\n    def aplicar(self, n_qubits, nivel_override=None):\n        \"\"\"\n        Apply phase damping channel to all qubits.\n        \n        Args:\n            n_qubits: Number of qubits in the circuit\n            nivel_override: Optional noise level override (uses self.nivel if None)\n        \"\"\"\n        lmb = self.nivel if nivel_override is None else nivel_override\n        for i in range(n_qubits):\n            qml.PhaseDamping(lmb, wires=i)\n\n\n# ===================== NOVOS MODELOS DE RU√çDO =====================\nclass RuidoCrosstalk(ModeloRuido):\n    \"\"\"\n    Ru√≠do de Cross-Talk: Erros correlacionados entre qubits vizinhos\n\n    Refer√™ncia: Kandala et al. (2019). \"Error mitigation extends the computational reach of a noisy quantum processor.\" Nature.\n\n    Descri√ß√£o: Aplica um canal de ru√≠do correlacionado entre pares de qubits vizinhos (ex: CNOT com ru√≠do).\n    \"\"\"\n    def aplicar(self, n_qubits, nivel_override=None):\n        \"\"\"\n        Apply correlated crosstalk noise between neighboring qubit pairs.\n        \n        Args:\n            n_qubits: Number of qubits in the circuit\n            nivel_override: Optional noise level override (uses self.nivel if None)\n        \"\"\"\n        p = self.nivel if nivel_override is None else nivel_override\n        # Aplica canal de ru√≠do correlacionado entre pares vizinhos\n        for i in range(n_qubits):\n            # Canal de ru√≠do correlacionado: DepolarizingChannel em ambos os qubits simultaneamente\n            qml.DepolarizingChannel(p, wires=i)\n            qml.DepolarizingChannel(p, wires=(i+1)%n_qubits)\n            # Cross-talk: canal extra entre pares\n            qml.CNOT(wires=[i, (i+1)%n_qubits])\n            qml.DepolarizingChannel(p, wires=(i+1)%n_qubits)\n            qml.CNOT(wires=[i, (i+1)%n_qubits])\n\n\nclass RuidoCorrelacionado(ModeloRuido):\n    \"\"\"\n    Ru√≠do Correlacionado Global: Erros coletivos afetando todos os qubits\n\n    Refer√™ncia: Greenbaum (2015). \"Introduction to Quantum Gate Set Tomography.\"\n\n    Descri√ß√£o: Aplica um canal de ru√≠do coletivo (ex: PhaseDamping global) a todos os qubits simultaneamente.\n    \"\"\"\n    def aplicar(self, n_qubits, nivel_override=None):\n        \"\"\"\n        Apply global correlated noise affecting all qubits collectively.\n        \n        Args:\n            n_qubits: Number of qubits in the circuit\n            nivel_override: Optional noise level override (uses self.nivel if None)\n        \"\"\"\n        lmb = self.nivel if nivel_override is None else nivel_override\n        # Aplica PhaseDamping global a todos os qubits (efeito coletivo)\n        for i in range(n_qubits):\n            qml.PhaseDamping(lmb, wires=i)\n        # Canal coletivo: aplica uma opera√ß√£o global (exemplo: ru√≠do de fase global)\n        # PennyLane n√£o tem canal global nativo, mas pode-se simular aplicando em todos simultaneamente\n        # Alternativamente, pode-se aplicar um canal customizado aqui se necess√°rio\n\n\n# Dicion√°rio de modelos dispon√≠veis\nMODELOS_RUIDO = {\n    'sem_ruido': None,\n    'depolarizante': RuidoDepolarizante,\n    'amplitude_damping': RuidoAmplitudeDamping,\n    'phase_damping': RuidoPhaseDamping,\n    'crosstalk': RuidoCrosstalk,\n    'thermal': RuidoThermal,\n    'correlated_noise': RuidoCorrelacionado,\n    'bit_flip': RuidoBitFlip,\n    'phase_flip': RuidoPhaseFlip,\n    'pink_noise': RuidoPinkNoise,\n    'readout_error': RuidoReadoutError\n}\n\n\n# ============================================================================\n# M√ìDULO 3: ARQUITETURAS DE CIRCUITOS QU√ÇNTICOS\\n\\nprint('‚úì Modelos de ru√≠do definidos!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Arquiteturas de Circuitos Qu√¢nticos Variacionais\\n",
    "\\n",
    "### üí° Para Iniciantes\\n",
    "Circuitos qu√¢nticos s√£o como \"programas\" que rodam em computadores qu√¢nticos.\\n",
    "Diferentes arquiteturas testam diferentes maneiras de processar dados.\\n",
    "\\n",
    "### üéì Para Especialistas\\n",
    "Implementamos 9 arquiteturas variacionais:\\n",
    "1. **Hardware Efficient**: otimizado para topologia de hardware real\\n",
    "2. **Strongly Entangling**: m√°ximo emaranhamento entre qubits\\n",
    "3. **Tree**: estrutura em √°rvore para reduzir porta CNOT\\n",
    "4. **QAOA-like**: inspirado em Quantum Approximate Optimization\\n",
    "5. **Alternating Layers**: altern√¢ncia RX-RY-RZ com CNOTs\\n",
    "6. **Star Entanglement**: qubit central conectado a todos\\n",
    "7. **Brickwork**: padr√£o de tijolos alternados\\n",
    "8. **Random Entangling**: emaranhamento estoc√°stico\\n",
    "9. **B√°sico**: arquitetura simples de refer√™ncia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def circuito_hardware_efficient(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):\n    \"\"\"\n    Hardware-Efficient Ansatz: RY + CNOT em camadas alternadas\n    \"\"\"\n    for i in range(min(len(x), n_qubits)):\n        qml.RY(np.pi * x[i], wires=i)\n    for camada in range(n_camadas):\n        for i in range(n_qubits):\n            qml.RY(weights[camada * n_qubits + i], wires=i)\n        for i in range(0, n_qubits-1, 2):\n            qml.CNOT(wires=[i, i+1])\n        for i in range(1, n_qubits-1, 2):\n            qml.CNOT(wires=[i, i+1])\n        if modelo_ruido:\n            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)\n    return qml.expval(qml.PauliZ(0))\n\ndef circuito_tree(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):\n    \"\"\"\n    Tree Entanglement: Emaranhamento em √°rvore bin√°ria\n    \"\"\"\n    for i in range(min(len(x), n_qubits)):\n        qml.RY(np.pi * x[i], wires=i)\n    for camada in range(n_camadas):\n        for i in range(n_qubits):\n            qml.RY(weights[camada * n_qubits + i], wires=i)\n        step = 1\n        while step < n_qubits:\n            for i in range(0, n_qubits-step, step*2):\n                qml.CNOT(wires=[i, i+step])\n            step *= 2\n        if modelo_ruido:\n            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)\n    return qml.expval(qml.PauliZ(0))\n\ndef circuito_qaoa(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):\n    \"\"\"\n    QAOA-like Ansatz: Alterna RX, RZZ, RX\n    \"\"\"\n    for i in range(min(len(x), n_qubits)):\n        qml.RX(np.pi * x[i], wires=i)\n    for camada in range(n_camadas):\n        for i in range(n_qubits):\n            qml.RX(weights[camada * n_qubits + i], wires=i)\n        for i in range(n_qubits-1):\n            qml.CNOT(wires=[i, i+1])\n            qml.RZ(weights[n_camadas * n_qubits + camada * (n_qubits-1) + i], wires=i+1)\n            qml.CNOT(wires=[i, i+1])\n        if modelo_ruido:\n            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)\n    return qml.expval(qml.PauliZ(0))\n\ndef circuito_alternating_layers(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):\n    \"\"\"\n    Alternating Layers: RX, RY, CNOT em padr√£o alternado\n    \"\"\"\n    for i in range(min(len(x), n_qubits)):\n        qml.RX(np.pi * x[i], wires=i)\n    for camada in range(n_camadas):\n        for i in range(n_qubits):\n            qml.RX(weights[camada * n_qubits + i], wires=i)\n            qml.RY(weights[n_camadas * n_qubits + camada * n_qubits + i], wires=i)\n        for i in range(n_qubits-1):\n            qml.CNOT(wires=[i, i+1])\n        if modelo_ruido:\n            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)\n    return qml.expval(qml.PauliZ(0))\n\ndef circuito_star_entanglement(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):\n    \"\"\"\n    Star Entanglement: Qubit 0 central, CNOT(0, i)\n    \"\"\"\n    for i in range(min(len(x), n_qubits)):\n        qml.RY(np.pi * x[i], wires=i)\n    for camada in range(n_camadas):\n        for i in range(n_qubits):\n            qml.RY(weights[camada * n_qubits + i], wires=i)\n        for i in range(1, n_qubits):\n            qml.CNOT(wires=[0, i])\n        if modelo_ruido:\n            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)\n    return qml.expval(qml.PauliZ(0))\n\ndef circuito_brickwork(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):\n    \"\"\"\n    Brickwork: padr√£o de CNOTs em \"tijolos\"\n    \"\"\"\n    for i in range(min(len(x), n_qubits)):\n        qml.RY(np.pi * x[i], wires=i)\n    for camada in range(n_camadas):\n        for i in range(n_qubits):\n            qml.RY(weights[camada * n_qubits + i], wires=i)\n        for i in range(0, n_qubits-1, 2):\n            qml.CNOT(wires=[i, i+1])\n        for i in range(1, n_qubits-1, 2):\n            qml.CNOT(wires=[i, i+1])\n        if modelo_ruido:\n            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)\n    return qml.expval(qml.PauliZ(0))\n\ndef circuito_random_entangling(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):\n    \"\"\"\n    Random Entangling: CNOTs entre pares aleat√≥rios por camada\n    \"\"\"\n    import random\n    for i in range(min(len(x), n_qubits)):\n        qml.RY(np.pi * x[i], wires=i)\n    for camada in range(n_camadas):\n        for i in range(n_qubits):\n            qml.RY(weights[camada * n_qubits + i], wires=i)\n        pares = [(i, j) for i in range(n_qubits) for j in range(i+1, n_qubits)]\n        random.shuffle(pares)\n        for i, j in pares[:n_qubits//2]:\n            qml.CNOT(wires=[i, j])\n        if modelo_ruido:\n            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)\n    return qml.expval(qml.PauliZ(0))\n# ============================================================================\n\ndef circuito_basico(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):\n    \"\"\"\n    Circuito B√°sico: RY encoding + RY rotations + CNOT ring\n\n    Refer√™ncia: Farhi & Neven (2018). \"Classification with quantum neural networks\n    on near term processors.\" arXiv:1802.06002\n\n    Estrutura:\n    1. Encoding: RY(œÄ¬∑x[i]) em cada qubit\n    2. Camadas variacionais (repetidas L vezes):\n       - RY(Œ∏[i]) em cada qubit\n       - CNOT em anel: CNOT(i, i+1 mod n)\n    3. Medi√ß√£o: ‚ü®Z‚ÇÄ‚ü©\n\n    Complexidade: O(n_qubits √ó n_camadas)\n    Par√¢metros: n_qubits √ó n_camadas\n\n    Vantagens:\n    - Simples e r√°pido\n    - Bom para prototipagem\n    - Baixo n√∫mero de par√¢metros\n    \"\"\"\n    # 1. Encoding de dados\n    for i in range(min(len(x), n_qubits)):\n        qml.RY(np.pi * x[i], wires=i)\n\n    # 2. Camadas variacionais\n    for camada in range(n_camadas):\n        # Rota√ß√µes parametrizadas\n        for i in range(n_qubits):\n            qml.RY(weights[camada * n_qubits + i], wires=i)\n\n        # Emaranhamento (CNOT em anel)\n        for i in range(n_qubits):\n            qml.CNOT(wires=[i, (i + 1) % n_qubits])\n\n        # Aplicar ru√≠do ap√≥s cada camada (se especificado)\n        if modelo_ruido:\n            modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)\n\n    # 3. Medi√ß√£o\n    return qml.expval(qml.PauliZ(0))\n\n\ndef circuito_strongly_entangling(weights, x, n_qubits, n_camadas, modelo_ruido=None, nivel_ruido_runtime=None):\n    \"\"\"\n    Strongly Entangling Layers (PennyLane template)\n\n    Refer√™ncia: Schuld et al. (2020). \"Circuit-centric quantum classifiers.\"\n    Physical Review A, 101(3), 032308.\n\n    Estrutura:\n    1. Encoding: AngleEmbedding (RY em cada qubit)\n    2. StronglyEntanglingLayers (template PennyLane):\n       - Rot(Œ∏, œÜ, œâ) em cada qubit (3 rota√ß√µes arbitr√°rias)\n       - CNOT(i, j) para todos i < j (emaranhamento completo)\n    3. Medi√ß√£o: ‚ü®Z‚ÇÄ‚ü©\n\n    Complexidade: O(n_qubits¬≤ √ó n_camadas)\n    Par√¢metros: n_qubits √ó n_camadas √ó 3\n\n    Vantagens:\n    - Alta capacidade expressiva\n    - Emaranhamento forte\n    - Template otimizado do PennyLane\n    \"\"\"\n    # 1. Encoding de dados\n    qml.AngleEmbedding(x, wires=range(n_qubits), rotation='Y')\n\n    # 2. Camadas fortemente emaranhadas\n    weights_reshaped = weights.reshape(n_camadas, n_qubits, 3)\n    qml.StronglyEntanglingLayers(weights_reshaped, wires=range(n_qubits))\n\n    # 3. Aplicar ru√≠do (se especificado)\n    if modelo_ruido:\n        modelo_ruido.aplicar(n_qubits, nivel_override=nivel_ruido_runtime)\n\n    # 4. Medi√ß√£o\n    return qml.expval(qml.PauliZ(0))\n\n\n# Dicion√°rio de arquiteturas dispon√≠veis\nARQUITETURAS = {\n    'basic_entangler': (circuito_basico, lambda nq, nc: nc * nq),\n    'strongly_entangling': (circuito_strongly_entangling, lambda nq, nc: nc * nq * 3),\n    'hardware_efficient': (circuito_hardware_efficient, lambda nq, nc: nc * nq),\n    'tree': (circuito_tree, lambda nq, nc: nc * nq),\n    'qaoa': (circuito_qaoa, lambda nq, nc: nc * nq + nc * (nq-1)),\n    'alternating_layers': (circuito_alternating_layers, lambda nq, nc: 2 * nc * nq),\n    'star_entanglement': (circuito_star_entanglement, lambda nq, nc: nc * nq),\n    'brickwork': (circuito_brickwork, lambda nq, nc: nc * nq),\n    'random_entangling': (circuito_random_entangling, lambda nq, nc: nc * nq)\n}\n\n\n# ============================================================================\n# M√ìDULO 4: CLASSIFICADOR QU√ÇNTICO VARIACIONAL (VQC)\n# ============================================================================\\n\\nprint('‚úì Arquiteturas de circuitos definidas!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Classificador Qu√¢ntico Variacional (VQC)\\n",
    "\\n",
    "### üí° Para Iniciantes\\n",
    "O VQC √© como uma rede neural qu√¢ntica que aprende a classificar dados.\\n",
    "Ele ajusta par√¢metros internos para melhorar suas previs√µes.\\n",
    "\\n",
    "### üéì Para Especialistas\\n",
    "Implementa√ß√£o compat√≠vel com scikit-learn (BaseEstimator, ClassifierMixin).\\n",
    "\\n",
    "**M√©todo de otimiza√ß√£o**: Gradient Descent com Parameter Shift Rule\\n",
    "$$\\\\frac{\\\\partial}{\\\\partial \\\\theta_i} \\\\langle \\\\psi(\\\\theta) | H | \\\\psi(\\\\theta) \\\\rangle = \\\\frac{1}{2}\\\\left[ \\\\langle \\\\psi(\\\\theta + \\\\pi/2 e_i) | H | \\\\psi(\\\\theta + \\\\pi/2 e_i) \\\\rangle - \\\\langle \\\\psi(\\\\theta - \\\\pi/2 e_i) | H | \\\\psi(\\\\theta - \\\\pi/2 e_i) \\\\rangle \\\\right]$$\\n",
    "\\n",
    "Funcionalidades:\\n",
    "- M√∫ltiplas fun√ß√µes de custo (MSE, Cross-Entropy, Hinge)\\n",
    "- Detec√ß√£o de Barren Plateaus\\n",
    "- Monitoramento de emaranhamento\\n",
    "- Schedule adaptativo de ru√≠do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ClassificadorVQC(BaseEstimator, ClassifierMixin):\n    \"\"\"\n    Classificador Qu√¢ntico Variacional.\n\n    Refer√™ncias:\n    - Schuld et al. (2020). \"Circuit-centric quantum classifiers.\" Phys. Rev. A.\n    - Mitarai et al. (2018). \"Quantum circuit learning.\" Phys. Rev. A.\n    - Bergholm et al. (2018). \"PennyLane: Automatic differentiation.\" arXiv:1811.04968\n\n    Implementa interface scikit-learn (BaseEstimator, ClassifierMixin) para\n    compatibilidade com pipelines de ML cl√°ssico.\n    \"\"\"\n\n    def __init__(self, n_qubits=4, n_camadas=2, arquitetura='basico',\n                 estrategia_init='aleatorio', tipo_ruido='sem_ruido', nivel_ruido=0.01,\n                 taxa_aprendizado=0.01, n_epocas=20, batch_size=32, seed=42,\n                 ruido_schedule=None, ruido_inicial=None, ruido_final=None,\n                 early_stopping=False, patience=10, min_delta=1e-3, val_split=0.1,\n                 ruido_adaptativo=False, track_entanglement=False,\n                 otimizador='adam', funcao_custo='mse', detectar_barren=False,\n                 max_grad_norm=1.0):\n        \"\"\"\n        Args:\n            n_qubits: N√∫mero de qubits (2-20)\n            n_camadas: Profundidade do circuito (1-10)\n            arquitetura: 'basico' ou 'strongly_entangling'\n            estrategia_init: 'aleatorio', 'matematico', ou 'quantico'\n            tipo_ruido: 'sem_ruido', 'depolarizante', 'amplitude_damping', 'phase_damping'\n            nivel_ruido: Taxa de erro (0.0-0.05)\n            taxa_aprendizado: Learning rate para Adam (1e-4 a 1e-1)\n            n_epocas: N√∫mero de √©pocas de treinamento (10-200)\n            batch_size: Tamanho do mini-batch (8-128)\n            seed: Semente aleat√≥ria para reprodutibilidade\n        \"\"\"\n        self.n_qubits = n_qubits\n        self.n_camadas = n_camadas\n        self.arquitetura = arquitetura\n        self.estrategia_init = estrategia_init\n        # Permitir uso autom√°tico de 'correlated_noise' se tipo_ruido for 'correlated' ou 'correlated_noise'\n        if tipo_ruido in ['correlated', 'correlated_noise']:\n            self.tipo_ruido = 'correlated_noise'\n        else:\n            self.tipo_ruido = tipo_ruido\n        self.nivel_ruido = nivel_ruido\n        self.taxa_aprendizado = taxa_aprendizado\n        self.n_epocas = n_epocas\n        self.batch_size = batch_size\n        self.seed = seed\n        # Annealing de ru√≠do\n        self.ruido_schedule = ruido_schedule  # 'linear' | 'exponencial' | 'cosine' | None\n        self.ruido_inicial = ruido_inicial\n        self.ruido_final = ruido_final\n        # Early stopping\n        self.early_stopping = early_stopping\n        self.patience = patience\n        self.min_delta = min_delta\n        self.val_split = val_split\n        self.ruido_adaptativo = ruido_adaptativo\n        self.track_entanglement = track_entanglement\n\n        # Funcionalidades avan√ßadas\n        self.otimizador = otimizador  # 'adam', 'sgd', 'qng'\n        self.funcao_custo = funcao_custo  # 'mse', 'cross_entropy', 'hinge'\n        self.detectar_barren = detectar_barren\n        self.max_grad_norm = max_grad_norm\n\n        # Hist√≥rico de treinamento expandido\n        self.historico_ = {\n            'custo': [],\n            'acuracia_treino': [],\n            'epoca': [],\n            'nivel_ruido': [],\n            'entropia_emaranhamento': [],\n            'variancia_gradiente': []\n        }\n\n        # Detectores e monitores\n        self.detector_plateau_ = DetectorBarrenPlateau() if detectar_barren else None\n        self.monitor_emaranhamento_ = MonitorEmaranhamento(n_qubits) if track_entanglement else None\n\n        # Configurar seeds para reprodutibilidade\n        np.random.seed(seed)\n        try:\n            # Nem sempre dispon√≠vel no shim; ignore se ausente\n            pnp.random.seed(seed)  # type: ignore[attr-defined]\n        except Exception:\n            pass\n\n    def _criar_circuito(self):\n        \"\"\"\n        Cria o circuito qu√¢ntico e inicializa par√¢metros.\n\n        Usa PennyLane's default.mixed device para suportar ru√≠do.\n        \"\"\"\n        # Dispositivo qu√¢ntico (simulador de matriz de densidade)\n        self.dev_ = qml.device('default.mixed', wires=self.n_qubits)\n\n        # Selecionar arquitetura e calcular n√∫mero de par√¢metros\n        circuito_fn, calc_params = ARQUITETURAS[self.arquitetura]\n        n_params = calc_params(self.n_qubits, self.n_camadas)\n\n        # Criar modelo de ru√≠do (se especificado)\n        modelo_ruido = None\n        if self.tipo_ruido != 'sem_ruido':\n            modelo_ruido = MODELOS_RUIDO[self.tipo_ruido](self.nivel_ruido)\n\n        # Criar QNode (circuito qu√¢ntico diferenci√°vel)\n        @qml.qnode(self.dev_, interface='autograd')\n        def circuit(weights, x, nivel_ruido_runtime=None):\n            \"\"\"\n            Quantum circuit definition for VQC classification.\n            \n            Args:\n                weights: Trainable parameters for the quantum circuit\n                x: Input data sample to encode\n                nivel_ruido_runtime: Runtime noise level override\n                \n            Returns:\n                Expectation value of PauliZ measurement\n            \"\"\"\n            return circuito_fn(weights, x, self.n_qubits, self.n_camadas, modelo_ruido, nivel_ruido_runtime)\n\n        self.qnode_ = circuit\n\n        # Inicializar pesos com estrat√©gia escolhida\n        self.weights_ = ConstantesFundamentais.inicializar(\n            n_params, self.estrategia_init, self.seed\n        )\n\n        # Inicializar bias\n        self.bias_ = pnp.array(0.0, requires_grad=True)\n\n    def _nivel_ruido_epoca(self, epoca):\n        if self.tipo_ruido == 'sem_ruido':\n            return 0.0\n        # Sem schedule: usar nivel fixo\n        if not self.ruido_schedule:\n            return self.nivel_ruido\n        nE, ri, rf = max(1, self.n_epocas), (self.ruido_inicial if self.ruido_inicial is not None else self.nivel_ruido), (self.ruido_final if self.ruido_final is not None else 0.001)\n        t = epoca / max(1, nE - 1)\n        if self.ruido_schedule == 'linear':\n            return rf + (ri - rf) * (1 - t)\n        if self.ruido_schedule == 'exponencial':\n            tau = max(1, nE / 3)\n            return rf + (ri - rf) * np.exp(-epoca / tau)\n        if self.ruido_schedule == 'cosine':\n            return rf + (ri - rf) * 0.5 * (1 + np.cos(np.pi * t))\n        return self.nivel_ruido\n\n    def _funcao_custo(self, weights, bias, X, y, nivel_ruido_runtime=None):\n        \"\"\"Fun√ß√£o de custo configur√°vel (MSE, Cross-Entropy ou Hinge).\"\"\"\n        predicoes = pnp.array([self.qnode_(weights, x, nivel_ruido_runtime) + bias for x in X])\n\n        # Selecionar fun√ß√£o de custo\n        if self.funcao_custo == 'mse':\n            # Usar pnp.mean para manter grafo de autograd\n            diff = pnp.array(y) - predicoes\n            return pnp.mean(diff ** 2)  # type: ignore[attr-defined]\n        elif self.funcao_custo == 'cross_entropy':\n            return FuncaoCustoAvancada.cross_entropy(predicoes, y)\n        elif self.funcao_custo == 'hinge':\n            return FuncaoCustoAvancada.hinge(predicoes, y)\n        else:\n            diff = pnp.array(y) - predicoes\n            return pnp.mean(diff ** 2)  # type: ignore[attr-defined]\n\n    def fit(self, X, y):\n        \"\"\"\n        Treina o classificador.\n\n        Args:\n            X: Dados de treinamento (n_samples, n_features)\n            y: Labels (n_samples,)\n\n        Returns:\n            self (para compatibilidade scikit-learn)\n        \"\"\"\n        # Codificar labels como ¬±1\n        self.label_encoder_ = LabelEncoder()\n        y_le = np.asarray(self.label_encoder_.fit_transform(y), dtype=int)\n        y_encoded = (y_le * 2) - 1  # type: ignore[operator]\n        self.classes_ = self.label_encoder_.classes_\n\n        # Criar circuito e inicializar par√¢metros\n        self._criar_circuito()\n\n        # Split de valida√ß√£o (para early stopping)\n        X_arr, y_arr = np.asarray(X), np.asarray(y_encoded)\n        if self.early_stopping and self.val_split > 0:\n            n = len(X_arr)\n            n_val = max(1, int(n * self.val_split))\n            idx = np.random.permutation(n)\n            val_idx, train_idx = idx[:n_val], idx[n_val:]\n            X_train_es, y_train_es = X_arr[train_idx], y_arr[train_idx]\n            X_val_es, y_val_es = X_arr[val_idx], y_arr[val_idx]\n        else:\n            X_train_es, y_train_es = X_arr, y_arr\n            X_val_es = y_val_es = None\n\n        # Criar otimizador configur√°vel\n        opt = OtimizadorAvancado.criar(self.otimizador, self.taxa_aprendizado)\n\n        # Treinamento por √©pocas\n        melhor_val = -np.inf\n        sem_melhora = 0\n        melhor_w, melhor_b = None, None\n\n        for epoca in range(self.n_epocas):\n            # Embaralhar dados\n            indices = np.random.permutation(len(X_train_es))\n\n            # Mini-batch gradient descent\n            nivel_runtime = self._nivel_ruido_epoca(epoca)\n            for i in range(0, len(X_train_es), self.batch_size):\n                batch_idx = indices[i:i + self.batch_size]\n                X_batch = X_train_es[batch_idx]\n                y_batch = y_train_es[batch_idx]\n\n                # Atualizar par√¢metros (fun√ß√£o de custo compat√≠vel com autograd)\n                def custo_batch(w, b):\n                    \"\"\"\n                    Batch cost function for mini-batch gradient descent.\n                    \n                    Args:\n                        w: Weight parameters\n                        b: Bias parameter\n                        \n                    Returns:\n                        Mean squared error for the batch\n                    \"\"\"\n                    preds = pnp.array([self.qnode_(w, pnp.array(x), nivel_runtime) + b for x in X_batch])\n                    # Usar pnp.mean para permitir gradientes\n                    return pnp.mean((pnp.array(y_batch) - preds) ** 2)  # type: ignore[attr-defined]\n\n                _step_res = opt.step(custo_batch, self.weights_, self.bias_)\n                # Robustez para diferentes retornos\n                try:\n                    self.weights_, self.bias_ = _step_res  # type: ignore[assignment]\n                except Exception:\n                    if isinstance(_step_res, (list, tuple)) and len(_step_res) >= 2:\n                        self.weights_, self.bias_ = _step_res[0], _step_res[1]\n                    else:\n                        pass\n\n            # Registrar hist√≥rico\n            custo_val = self._funcao_custo(\n                self.weights_, self.bias_, pnp.array(X_train_es), pnp.array(y_train_es), nivel_runtime\n            )\n            try:\n                custo = float(custo_val)\n            except Exception:\n                custo = float(pnp.asarray(custo_val))\n            acuracia = self.score(X, y)\n\n            self.historico_['custo'].append(custo)\n            self.historico_['acuracia_treino'].append(acuracia)\n            self.historico_['epoca'].append(epoca)\n            self.historico_['nivel_ruido'].append(nivel_runtime)\n\n            # Monitorar gradientes (detec√ß√£o de barren plateau)\n            if self.detector_plateau_:\n                try:\n                    gradientes = qml.grad(self._funcao_custo)(\n                        self.weights_, self.bias_,\n                        pnp.array(X_train_es[:5]), pnp.array(y_train_es[:5]),\n                        nivel_runtime\n                    )\n                    if isinstance(gradientes, (list, tuple)) and len(gradientes) > 0:\n                        grad_array = np.array(gradientes[0]).flatten()\n                    else:\n                        grad_array = np.array([])\n                    variancia_grad = float(np.var(grad_array))\n                    self.historico_['variancia_gradiente'].append(variancia_grad)\n\n                    if self.detector_plateau_.detectar(grad_array):\n                        logger.warning(f\"√âpoca {epoca}: Barren Plateau detectado (var={variancia_grad:.2e})\")\n                except Exception:\n                    self.historico_['variancia_gradiente'].append(0.0)\n            else:\n                self.historico_['variancia_gradiente'].append(0.0)\n\n            # Monitorar emaranhamento\n            if self.monitor_emaranhamento_:\n                try:\n                    # Criar QNode que retorna estado\n                    @qml.qnode(self.dev_, interface='autograd')\n                    def estado_qnode(weights, x):\n                        \"\"\"\n                        Quantum node that returns density matrix for entanglement measurement.\n                        \n                        Args:\n                            weights: Circuit weights\n                            x: Input data sample\n                            \n                        Returns:\n                            Density matrix of the first qubit\n                        \"\"\"\n                        circuito_fn, _ = ARQUITETURAS[self.arquitetura]\n                        circuito_fn(weights, x, self.n_qubits, self.n_camadas, None, None)\n                        return qml.density_matrix(wires=[0])\n\n                    # Calcular estado do primeiro qubit\n                    x_sample = pnp.array(X_train_es[0])\n                    rho_0 = estado_qnode(self.weights_, x_sample)\n\n                    # Calcular entropia\n                    entropia = self.monitor_emaranhamento_.calcular_entropia_von_neumann(rho_0)\n                    self.historico_['entropia_emaranhamento'].append(float(entropia))\n                except Exception:\n                    self.historico_['entropia_emaranhamento'].append(0.0)\n            else:\n                self.historico_['entropia_emaranhamento'].append(0.0)\n\n            # Early stopping\n            if self.early_stopping and X_val_es is not None and y_val_es is not None:\n                ac_val = np.mean(self.predict(X_val_es) == self.label_encoder_.inverse_transform(((y_val_es + 1)//2).astype(int)))\n                if ac_val > melhor_val + self.min_delta:\n                    melhor_val = ac_val\n                    sem_melhora = 0\n                    melhor_w, melhor_b = self.weights_.copy(), float(self.bias_)\n                else:\n                    sem_melhora += 1\n                if sem_melhora >= self.patience:\n                    # Restaurar melhor\n                    if (melhor_w is not None) and (melhor_b is not None):\n                        self.weights_ = melhor_w\n                        self.bias_ = pnp.array(melhor_b, requires_grad=True)\n                    break\n\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Prediz classes para novos dados.\n\n        Args:\n            X: Dados de teste (n_samples, n_features)\n\n        Returns:\n            Predi√ß√µes de classe (n_samples,)\n        \"\"\"\n        # Obter predi√ß√µes do circuito qu√¢ntico\n        predicoes = np.array([\n            float(self.qnode_(self.weights_, pnp.array(x)) + self.bias_)\n            for x in X\n        ])\n\n        # Converter de ¬±1 para classes originais\n        predicoes_classe = ((np.sign(predicoes) + 1) // 2).astype(int)\n        return self.label_encoder_.inverse_transform(predicoes_classe)\n\n    def score(self, X, y, sample_weight=None):\n        \"\"\"\n        Calcula acur√°cia.\n\n        Args:\n            X: Dados\n            y: Labels verdadeiros\n\n        Returns:\n            Acur√°cia (0.0 a 1.0)\n        \"\"\"\n        return np.mean(self.predict(X) == y)\n\n\n# ============================================================================\n# M√ìDULO 5: GERENCIAMENTO DE DATASETS\n# ============================================================================\\n\\nprint('‚úì ClassificadorVQC definido!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Carregamento de Datasets\\n",
    "\\n",
    "### üí° Para Iniciantes\\n",
    "Testamos 5 conjuntos de dados diferentes para verificar se o ru√≠do qu√¢ntico\\n",
    "realmente ajuda em situa√ß√µes variadas.\\n",
    "\\n",
    "### üéì Para Especialistas\\n",
    "Datasets do scikit-learn com preprocessamento rigoroso:\\n",
    "- **Moons**: classifica√ß√£o n√£o-linear 2D\\n",
    "- **Circles**: classifica√ß√£o n√£o-linear conc√™ntrica\\n",
    "- **Iris**: multiclasse cl√°ssico (3 classes, 4 features)\\n",
    "- **Breast Cancer**: diagn√≥stico bin√°rio (30 features)\\n",
    "- **Wine**: multiclasse (3 classes, 13 features)\\n",
    "\\n",
    "Preprocessamento: StandardScaler + train/test split (80/20) com seed fixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def carregar_datasets(seed=42):\n    \"\"\"\n    Carrega 4 datasets de benchmark para classifica√ß√£o bin√°ria.\n\n    Refer√™ncias:\n    - Moons, Circles: Scikit-learn (Pedregosa et al., 2011)\n    - Iris: Fisher (1936), UCI ML Repository\n    - Breast Cancer: Wolberg et al. (1995), UCI ML Repository\n\n    Returns:\n        Dict com 4 datasets, cada um contendo X_train, X_test, y_train, y_test\n    \"\"\"\n    datasets = {}\n    scaler = StandardScaler()\n\n    # Dataset 1: Moons (n√£o-linear, duas luas entrela√ßadas)\n    X, y = sk_datasets.make_moons(n_samples=400, noise=0.1, random_state=seed)\n    X = scaler.fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=seed, stratify=y\n    )\n    datasets['moons'] = {\n        'X_train': X_train, 'X_test': X_test,\n        'y_train': y_train, 'y_test': y_test,\n        'descricao': 'Duas luas entrela√ßadas (n√£o-linear)'\n    }\n\n    # Dataset 2: Circles (n√£o-linear, c√≠rculos conc√™ntricos)\n    X, y = sk_datasets.make_circles(\n        n_samples=400, noise=0.1, factor=0.5, random_state=seed\n    )\n    X = scaler.fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=seed, stratify=y\n    )\n    datasets['circles'] = {\n        'X_train': X_train, 'X_test': X_test,\n        'y_train': y_train, 'y_test': y_test,\n        'descricao': 'C√≠rculos conc√™ntricos (n√£o-linear)'\n    }\n\n    # Dataset 3: Iris (linear, 2 primeiras classes)\n    from sklearn.utils import Bunch as SklearnBunch\n    iris_data: SklearnBunch = sk_datasets.load_iris()  # type: ignore[assignment]\n    X, y = iris_data.data, iris_data.target\n    mask = y < 2  # Apenas setosa e versicolor\n    X, y = X[mask], y[mask]\n    X = scaler.fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=seed, stratify=y\n    )\n    datasets['iris'] = {\n        'X_train': X_train, 'X_test': X_test,\n        'y_train': y_train, 'y_test': y_test,\n        'descricao': 'Flores Iris (linear, 4 features)'\n    }\n\n    # Dataset 4: Breast Cancer (alta dimens√£o, 30 features)\n    cancer_data: SklearnBunch = sk_datasets.load_breast_cancer()  # type: ignore[assignment]\n    X, y = cancer_data.data, cancer_data.target\n    X = scaler.fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=seed, stratify=y\n    )\n    datasets['breast_cancer'] = {\n        'X_train': X_train, 'X_test': X_test,\n        'y_train': y_train, 'y_test': y_test,\n        'descricao': 'Diagn√≥stico de c√¢ncer (30 features)'\n    }\n\n    # Dataset 5: Wine (features qu√≠micas, 2 primeiras classes)\n    wine_data: SklearnBunch = sk_datasets.load_wine()  # type: ignore[assignment]\n    X, y = wine_data.data, wine_data.target\n    mask = y < 2  # Apenas classes 0 e 1\n    X, y = X[mask], y[mask]\n    X = scaler.fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=seed, stratify=y\n    )\n    datasets['wine'] = {\n        'X_train': X_train, 'X_test': X_test,\n        'y_train': y_train, 'y_test': y_test,\n        'descricao': 'Vinhos (13 features qu√≠micas)'\n    }\n\n    return datasets\n\n\n# ============================================================================\n# M√ìDULO 6: EXECU√á√ÉO DE EXPERIMENTOS\n# ============================================================================\\n\\nprint('‚úì Fun√ß√£o carregar_datasets definida!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Grid Search de Hiperpar√¢metros\\n",
    "\\n",
    "### üí° Para Iniciantes\\n",
    "Grid search testa sistematicamente todas as combina√ß√µes de par√¢metros\\n",
    "para encontrar a melhor configura√ß√£o.\\n",
    "\\n",
    "### üéì Para Especialistas\\n",
    "Busca exaustiva no espa√ßo de hiperpar√¢metros:\\n",
    "- **Arquiteturas**: 9 variantes de circuitos\\n",
    "- **Inicializa√ß√µes**: 3 estrat√©gias (aleat√≥rio, Xavier, He)\\n",
    "- **Tipos de ru√≠do**: 10 modelos + baseline sem ru√≠do\\n",
    "- **N√≠veis de ru√≠do**: scan logar√≠tmico de 0.0001 a 0.1\\n",
    "- **Datasets**: 5 conjuntos de dados\\n",
    "\\n",
    "Total: ~8,280 experimentos controlados com 3 seeds para robustez estat√≠stica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def executar_grid_search(datasets, n_epocas=15, verbose=True, pasta_resultados=None):\n    \"\"\"\n    Executa grid search completo sobre todas as configura√ß√µes.\n\n    Grid:\n    - 5 datasets (moons, circles, iris, breast_cancer, wine)\n    - 8 arquiteturas\n    - 5 estrat√©gias de inicializa√ß√£o\n    - 5 tipos de ru√≠do (depolarizing, amplitude_damping, phase_damping, crosstalk, thermal/correlated)\n    - 3 n√≠veis de ru√≠do\n    - 4 schedules de ru√≠do (linear, exponencial, cosseno, adaptativo)\n    Total: 5 √ó 8 √ó 5 √ó 5 √ó 3 √ó 4 = 12.000 configura√ß√µes\n    # Observa√ß√£o: No c√≥digo, 'thermal' √© tratado como ru√≠do correlacionado para consist√™ncia com os documentos.\n\n    Args:\n        datasets: Dict de datasets\n        n_epocas: N√∫mero de √©pocas de treinamento\n        verbose: Se True, imprime progresso\n\n    Returns:\n        DataFrame com todos os resultados\n    \"\"\"\n    # Pasta para granularidade m√°xima\n    pasta_individual = None\n    if pasta_resultados is None:\n        pasta_resultados = os.path.join(os.getcwd(), f\"resultados_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n    os.makedirs(pasta_resultados, exist_ok=True)\n    pasta_individual = os.path.join(pasta_resultados, 'experimentos_individuais')\n    os.makedirs(pasta_individual, exist_ok=True)\n    # Placeholders para Pylance\n    from typing import Any\n    metadata: Dict[str, Any] = {}\n    metadata_path: Optional[str] = None\n\n    # Definir grid de hiperpar√¢metros\n    quick = os.environ.get('VQC_QUICK', '0') == '1'\n    if quick:\n        grid = {\n            'arquitetura': list(ARQUITETURAS.keys()),\n            'estrategia_init': ['matematico', 'fibonacci_spiral'],\n            'tipo_ruido': ['sem_ruido', 'depolarizante'],\n            'nivel_ruido': [0.0, 0.0025, 0.005, 0.0075, 0.01]\n        }\n    else:\n        grid = {\n            'arquitetura': list(ARQUITETURAS.keys()),\n            'estrategia_init': ['matematico', 'quantico', 'aleatorio', 'fibonacci_spiral'],\n            'tipo_ruido': ['sem_ruido', 'depolarizante', 'amplitude_damping', 'phase_damping', 'crosstalk', 'correlacionado'],\n            'nivel_ruido': [0.0, 0.0025, 0.005, 0.0075, 0.01, 0.0125, 0.015, 0.0175, 0.02]\n        }\n\n    resultados = []\n    n_seeds = 5\n    seed_list = [42 + i for i in range(n_seeds)]\n    contador = 0  # Inicializar contador\n\n    # Calcular total de configura√ß√µes\n    total_configs = len(grid['arquitetura']) * len(grid['estrategia_init']) * len(grid['tipo_ruido']) * len(grid['nivel_ruido'])\n    # Ajustar para configs v√°lidas (sem_ruido s√≥ com nivel 0)\n    configs_invalidas = len(grid['arquitetura']) * len(grid['estrategia_init']) * (len(grid['nivel_ruido']) - 1)  # sem_ruido com nivel > 0\n    total_configs -= configs_invalidas\n\n    # Gera√ß√£o de README e metadata.json ap√≥s grid/seed_list definidos\n    if pasta_resultados is not None:\n        readme_path = os.path.join(pasta_resultados, 'README_grid_search.md')\n        metadata_path = os.path.join(pasta_resultados, 'metadata_grid_search.json')\n        with open(readme_path, 'w', encoding='utf-8') as f:\n            f.write(\n                \"# Resultados do Grid Search VQC\\n\\n\"\n                f\"- Data de execu√ß√£o: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n                f\"- Par√¢metros do grid: {grid}\\n\"\n                f\"- Seeds: {seed_list}\\n\\n\"\n                \"Todos os experimentos, circuitos e gr√°ficos est√£o organizados nesta pasta.\\n\"\n                \"O arquivo `resultados_completos_artigo.csv` cont√©m todos os resultados consolidados.\\n\"\n            )\n        metadata = {\n            'tipo': 'grid_search',\n            'data_execucao': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n            'parametros_grid': grid,\n            'seeds': seed_list,\n            'arquivos_gerados': [],\n            'csv_consolidado': None,\n            'readme': readme_path\n        }\n\n    for nome_dataset, dataset in datasets.items():\n        _X_train, _y_train = dataset['X_train'], dataset['y_train']\n        _X_test, _y_test = dataset['X_test'], dataset['y_test']\n\n        for arq in grid['arquitetura']:\n            for init in grid['estrategia_init']:\n                for ruido in grid['tipo_ruido']:\n                    for nivel in grid['nivel_ruido']:\n                        if ruido == 'sem_ruido' and nivel > 0:\n                            continue\n                        for seed in seed_list:\n                            contador += 1\n                            # Log detalhado de todos os par√¢metros\n                            if verbose:\n                                logger.info(\n                                    f\"[{contador:3d}/{total_configs * n_seeds}] \"\n                                    f\"Dataset: {nome_dataset} | Seed: {seed} | Qubits: 4 | Camadas: 2 | \"\n                                    f\"Arquitetura: {arq} | Init: {init} | Ru√≠do: {ruido} | N√≠vel: {nivel:.4f}\"\n                                )\n                                logger.info(\n                                    f\"Constantes: œÄ={ConstantesFundamentais.PI:.5f}, e={ConstantesFundamentais.E:.5f}, œÜ={ConstantesFundamentais.PHI:.5f}, ‚Ñè={ConstantesFundamentais.HBAR:.2e}, Œ±={ConstantesFundamentais.ALPHA:.5f}, R‚àû={ConstantesFundamentais.RYDBERG:.2f}\"\n                                )\n                            try:\n                                tempo_inicio = time.time()\n                                # Criar e treinar VQC\n                                vqc = ClassificadorVQC(\n                                    n_qubits=4,\n                                    n_camadas=2,\n                                    arquitetura=arq,\n                                    estrategia_init=init,\n                                    tipo_ruido=ruido,\n                                    nivel_ruido=nivel,\n                                    taxa_aprendizado=0.01,\n                                    n_epocas=n_epocas,\n                                    batch_size=32,\n                                    seed=seed,\n                                    # Insights: ru√≠do com annealing quando n√≠vel > 0\n                                    ruido_schedule=('cosine' if (ruido != 'sem_ruido' and nivel > 0) else None),\n                                    ruido_inicial=(nivel if (ruido != 'sem_ruido' and nivel > 0) else None),\n                                    ruido_final=(0.001 if (ruido != 'sem_ruido' and nivel > 0) else None),\n                                    # Early stopping leve para acelerar\n                                    early_stopping=True, patience=5, min_delta=1e-3, val_split=0.1\n                                )\n                                vqc.fit(dataset['X_train'], dataset['y_train'])\n                                tempo_total = time.time() - tempo_inicio\n                                # Calcular m√©tricas\n                                acuracia_treino = vqc.score(dataset['X_train'], dataset['y_train'])\n                                acuracia_teste = vqc.score(dataset['X_test'], dataset['y_test'])\n                                gap_treino_teste = acuracia_treino - acuracia_teste\n                                # Matriz de confus√£o\n                                y_pred = vqc.predict(dataset['X_test'])\n                                cm = confusion_matrix(dataset['y_test'], y_pred)\n                                # Armazenar resultados\n                                resultado = {\n                                    'dataset': nome_dataset,\n                                    'arquitetura': arq,\n                                    'estrategia_init': init,\n                                    'tipo_ruido': ruido,\n                                    'nivel_ruido': nivel,\n                                    'n_qubits': 4,\n                                    'n_camadas': 2,\n                                    'acuracia_treino': acuracia_treino,\n                                    'acuracia_teste': acuracia_teste,\n                                    'gap_treino_teste': gap_treino_teste,\n                                    'tempo_segundos': tempo_total,\n                                    'custo_final': vqc.historico_['custo'][-1],\n                                    'cm_tn': cm[0,0],\n                                    'cm_fp': cm[0,1],\n                                    'cm_fn': cm[1,0],\n                                    'cm_tp': cm[1,1],\n                                    'seed': seed\n                                }\n                                resultados.append(resultado)\n                                # Salvar cada experimento individualmente em CSV\n                                if pasta_individual is not None:\n                                    id_exp = f\"exp_{contador:05d}\"\n                                    df_exp = pd.DataFrame([resultado])\n                                    csv_exp_path = os.path.join(pasta_individual, f\"{id_exp}.csv\")\n                                    df_exp.to_csv(csv_exp_path, index=False)\n                                # Bloco duplicado removido (j√° salvo em resultado)\n                                if verbose:\n                                    logger.info(\n                                        f\"  ‚úì Acur√°cia: {acuracia_teste:.4f} | \"\n                                        f\"Gap: {gap_treino_teste:+.4f} | \"\n                                        f\"Tempo: {tempo_total:.1f}s\"\n                                    )\n\n                                # Salvar circuito desenhado em PNG\n                                if pasta_resultados is not None:\n                                    try:\n                                        import pennylane as qml\n                                        import matplotlib\n                                        matplotlib.use('Agg')\n                                        import matplotlib.pyplot as plt\n\n                                        # Criar pasta para circuitos se n√£o existir\n                                        pasta_circuitos = os.path.join(pasta_resultados, \"circuitos\")\n                                        os.makedirs(pasta_circuitos, exist_ok=True)\n\n                                        # Desenhar circuito usando qml.draw_mpl\n                                        fig_circ, ax_circ = qml.draw_mpl(vqc.qnode_, decimals=2)(\n                                            vqc.weights_,\n                                            dataset['X_train'][0],\n                                            None  # nivel_ruido_runtime\n                                        )\n\n                                        circ_png_filename = f\"circuito_{nome_dataset}_seed{seed}_{arq}_{init}_{ruido}_nivel{nivel:.4f}.png\"\n                                        circ_png_path = os.path.join(pasta_circuitos, circ_png_filename)\n                                        plt.savefig(circ_png_path, dpi=150, bbox_inches='tight', facecolor='white')\n                                        try:\n                                            import matplotlib.pyplot as _plt\n                                            # Alguns stubs reportam tipo impreciso para draw_mpl; force close seguro\n                                            try:\n                                                from matplotlib.figure import Figure as _MplFigure\n                                            except Exception:\n                                                _MplFigure = object  # type: ignore[assignment]\n                                            if isinstance(fig_circ, _MplFigure):\n                                                _plt.close(fig_circ)  # type: ignore[arg-type]\n                                            else:\n                                                _plt.close('all')\n                                        except Exception:\n                                            try:\n                                                plt.close('all')\n                                            except Exception:\n                                                pass\n\n                                        if verbose:\n                                            logger.info(f\"    ‚Üí Circuito salvo: {circ_png_filename}\")\n                                    except Exception as e:\n                                        logger.warning(f\"Falha ao salvar PNG do circuito: {e}\")\n\n                                    # Salvar gr√°fico 3D de gradientes (Barren Plateaus)\n                                    if 'variancia_gradiente' in vqc.historico_ and len(vqc.historico_['variancia_gradiente']) > 0:\n                                        try:\n                                            import matplotlib\n                                            matplotlib.use('Agg')\n                                            import matplotlib.pyplot as plt\n                                            import numpy as np\n\n                                            # Criar pasta para barren plateaus se n√£o existir\n                                            pasta_barren = os.path.join(pasta_resultados, \"barren_plateaus\")\n                                            os.makedirs(pasta_barren, exist_ok=True)\n\n                                            epocas = vqc.historico_.get('epoca', [])\n                                            variancias = vqc.historico_.get('variancia_gradiente', [])\n                                            custos = vqc.historico_.get('custo', [])\n\n                                            # Se n√£o houver dados de √©poca, gerar sequ√™ncia simples\n                                            if not epocas or len(epocas) != len(variancias):\n                                                epocas = list(range(1, len(variancias)+1))\n\n                                            if len(epocas) == len(variancias) == len(custos) and len(epocas) > 0:\n                                                fig = plt.figure(figsize=(10, 8))\n                                                ax = fig.add_subplot(111, projection='3d')\n\n                                                # Garantir dtype float para compatibilidade com matplotlib\n                                                X = np.array(epocas, dtype=float)\n                                                Y = np.array(variancias, dtype=float)\n                                                Z = np.array(custos, dtype=float)\n\n                                                scatter = ax.scatter(xs=X, ys=Y, zs=Z, c=Z, cmap='viridis', s=50, alpha=0.8)  # type: ignore[call-arg]\n\n                                                ax.set_title(\n                                                    f'Barren Plateau Analysis\\n{arq} | {init} | {ruido} (Œ≥={nivel:.4f})',\n                                                    fontsize=14, fontfamily='serif'\n                                                )\n                                                ax.set_xlabel('√âpoca', fontsize=12, fontfamily='serif')\n                                                ax.set_ylabel('Var(Gradiente)', fontsize=12, fontfamily='serif')\n                                                ax.set_zlabel('Custo', fontsize=12, fontfamily='serif')\n\n                                                cbar = plt.colorbar(scatter, ax=ax, label='Custo', shrink=0.8, pad=0.1)\n                                                cbar.ax.tick_params(labelsize=10)\n\n                                                plt.tight_layout()\n\n                                                barren_filename = f\"barren3d_{nome_dataset}_seed{seed}_{arq}_{init}_{ruido}_nivel{nivel:.4f}.png\"\n                                                barren_path = os.path.join(pasta_barren, barren_filename)\n                                                plt.savefig(barren_path, dpi=150, bbox_inches='tight', facecolor='white')\n                                                # Fechamento expl√≠cito da figura para evitar warning de tipo do Pylance\n                                                try:\n                                                    import matplotlib.pyplot as _plt\n                                                    _plt.close(fig)\n                                                except Exception:\n                                                    try:\n                                                        plt.close('all')\n                                                    except Exception:\n                                                        pass\n                                                if verbose:\n                                                    logger.info(f\"    ‚Üí Barren plateau 3D salvo: {barren_filename}\")\n                                            else:\n                                                logger.warning(\"Dados insuficientes ou incompat√≠veis para gerar gr√°fico 3D dos gradientes.\")\n                                        except Exception as e:\n                                            logger.warning(f\"Falha ao salvar gr√°fico 3D barren plateau: {e}\")\n\n                            except Exception as e:\n                                if verbose:\n                                    logger.warning(f\"  ‚úó Erro: {str(e)[:50]}\")\n\n    total_configs = (len(grid['arquitetura']) * len(grid['estrategia_init']) *\n                    len(grid['tipo_ruido']) * len(grid['nivel_ruido']))\n\n    if verbose:\n        logger.info(f\"Total de configura√ß√µes: {total_configs} por dataset\")\n        logger.info(f\"Total geral: {total_configs * len(datasets)} experimentos\\n\")\n\n    contador = 0\n\n    # Iterar sobre datasets\n    for nome_dataset, dataset in datasets.items():\n        if verbose:\n            logger.info(f\"\\n{'='*80}\")\n            logger.info(f\" DATASET: {nome_dataset.upper()}\")\n            logger.info(f\" {dataset['descricao']}\")\n            logger.info(f\"{'='*80}\\n\")\n        # Iterar sobre grid\n        for arq in grid['arquitetura']:\n            for init in grid['estrategia_init']:\n                for ruido in grid['tipo_ruido']:\n                    for nivel in grid['nivel_ruido']:\n                        # Pular combina√ß√µes inv√°lidas (sem_ruido com n√≠vel > 0)\n                        if ruido == 'sem_ruido' and nivel > 0:\n                            continue\n                        for seed in seed_list:\n                            contador += 1\n                            # Log detalhado de todos os par√¢metros\n                            if verbose:\n                                logger.info(\n                                    f\"[{contador:3d}/{total_configs * n_seeds}] \"\n                                    f\"Dataset: {nome_dataset} | Seed: {seed} | Qubits: 4 | Camadas: 2 | \"\n                                    f\"Arquitetura: {arq} | Init: {init} | Ru√≠do: {ruido} | N√≠vel: {nivel:.4f}\"\n                                )\n                                logger.info(\n                                    f\"Constantes: œÄ={ConstantesFundamentais.PI:.5f}, e={ConstantesFundamentais.E:.5f}, œÜ={ConstantesFundamentais.PHI:.5f}, ‚Ñè={ConstantesFundamentais.HBAR:.2e}, Œ±={ConstantesFundamentais.ALPHA:.5f}, R‚àû={ConstantesFundamentais.RYDBERG:.2f}\"\n                                )\n                            try:\n                                tempo_inicio = time.time()\n                                # Criar e treinar VQC\n                                vqc = ClassificadorVQC(\n                                    n_qubits=4,\n                                    n_camadas=2,\n                                    arquitetura=arq,\n                                    estrategia_init=init,\n                                    tipo_ruido=ruido,\n                                    nivel_ruido=nivel,\n                                    taxa_aprendizado=0.01,\n                                    n_epocas=n_epocas,\n                                    batch_size=32,\n                                    seed=seed,\n                                    # Insights: ru√≠do com annealing quando n√≠vel > 0\n                                    ruido_schedule=('cosine' if (ruido != 'sem_ruido' and nivel > 0) else None),\n                                    ruido_inicial=(nivel if (ruido != 'sem_ruido' and nivel > 0) else None),\n                                    ruido_final=(0.001 if (ruido != 'sem_ruido' and nivel > 0) else None),\n                                    # Early stopping leve para acelerar\n                                    early_stopping=True, patience=5, min_delta=1e-3, val_split=0.1\n                                )\n                                vqc.fit(dataset['X_train'], dataset['y_train'])\n                                tempo_total = time.time() - tempo_inicio\n                                # Calcular m√©tricas\n                                acuracia_treino = vqc.score(dataset['X_train'], dataset['y_train'])\n                                acuracia_teste = vqc.score(dataset['X_test'], dataset['y_test'])\n                                gap_treino_teste = acuracia_treino - acuracia_teste\n                                # Matriz de confus√£o\n                                y_pred = vqc.predict(dataset['X_test'])\n                                cm = confusion_matrix(dataset['y_test'], y_pred)\n                                # Armazenar resultados\n                                resultados.append({\n                                    'dataset': nome_dataset,\n                                    'arquitetura': arq,\n                                    'estrategia_init': init,\n                                    'tipo_ruido': ruido,\n                                    'nivel_ruido': nivel,\n                                    'n_qubits': 4,\n                                    'n_camadas': 2,\n                                    'acuracia_treino': acuracia_treino,\n                                    'acuracia_teste': acuracia_teste,\n                                    'gap_treino_teste': gap_treino_teste,\n                                    'tempo_segundos': tempo_total,\n                                    'custo_final': vqc.historico_['custo'][-1],\n                                    'cm_tn': cm[0,0],\n                                    'cm_fp': cm[0,1],\n                                    'cm_fn': cm[1,0],\n                                    'cm_tp': cm[1,1],\n                                    'seed': seed\n                                })\n                                if verbose:\n                                    logger.info(\n                                        f\"  ‚úì Acur√°cia: {acuracia_teste:.4f} | \"\n                                        f\"Gap: {gap_treino_teste:+.4f} | \"\n                                        f\"Tempo: {tempo_total:.1f}s\"\n                                    )\n                            except Exception as e:\n                                if verbose:\n                                    logger.warning(f\"  ‚úó Erro: {str(e)[:50]}\")\n\n    # Adicionar baselines cl√°ssicos (SVM e Random Forest)\n    for nome_dataset, dataset in datasets.items():\n        if verbose:\n            logger.info(f\"\\n{'='*80}\")\n            logger.info(f\" BASELINES CL√ÅSSICOS: {nome_dataset.upper()}\")\n            logger.info(f\"{'='*80}\")\n        # SVM (RBF)\n        try:\n            clf_svm = SVC(kernel='rbf', probability=True, random_state=42)\n            clf_svm.fit(dataset['X_train'], dataset['y_train'])\n            acuracia_treino = clf_svm.score(dataset['X_train'], dataset['y_train'])\n            acuracia_teste = clf_svm.score(dataset['X_test'], dataset['y_test'])\n            y_pred = clf_svm.predict(dataset['X_test'])\n            cm = confusion_matrix(dataset['y_test'], y_pred)\n            resultados.append({\n                'dataset': nome_dataset,\n                'arquitetura': 'SVM',\n                'estrategia_init': '-',\n                'tipo_ruido': 'classico',\n                'nivel_ruido': 0.0,\n                'n_qubits': 0,\n                'n_camadas': 0,\n                'acuracia_treino': acuracia_treino,\n                'acuracia_teste': acuracia_teste,\n                'gap_treino_teste': acuracia_treino - acuracia_teste,\n                'tempo_segundos': 0.0,\n                'custo_final': 0.0,\n                'cm_tn': cm[0,0],\n                'cm_fp': cm[0,1],\n                'cm_fn': cm[1,0],\n                'cm_tp': cm[1,1],\n                'seed': 42\n            })\n            if verbose:\n                logger.info(f\"  ‚úì SVM (RBF): Acur√°cia teste = {acuracia_teste:.4f}\")\n        except Exception as e:\n            if verbose:\n                logger.warning(f\"  ‚úó Erro SVM: {str(e)[:50]}\")\n        # Random Forest\n        try:\n            clf_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n            clf_rf.fit(dataset['X_train'], dataset['y_train'])\n            acuracia_treino = clf_rf.score(dataset['X_train'], dataset['y_train'])\n            acuracia_teste = clf_rf.score(dataset['X_test'], dataset['y_test'])\n            y_pred = clf_rf.predict(dataset['X_test'])\n            cm = confusion_matrix(dataset['y_test'], y_pred)\n            resultados.append({\n                'dataset': nome_dataset,\n                'arquitetura': 'RandomForest',\n                'estrategia_init': '-',\n                'tipo_ruido': 'classico',\n                'nivel_ruido': 0.0,\n                'n_qubits': 0,\n                'n_camadas': 0,\n                'acuracia_treino': acuracia_treino,\n                'acuracia_teste': acuracia_teste,\n                'gap_treino_teste': acuracia_treino - acuracia_teste,\n                'tempo_segundos': 0.0,\n                'custo_final': 0.0,\n                'cm_tn': cm[0,0],\n                'cm_fp': cm[0,1],\n                'cm_fn': cm[1,0],\n                'cm_tp': cm[1,1],\n                'seed': 42\n            })\n            if verbose:\n                logger.info(f\"  ‚úì Random Forest: Acur√°cia teste = {acuracia_teste:.4f}\")\n        except Exception as e:\n            if verbose:\n                logger.warning(f\"  ‚úó Erro RF: {str(e)[:50]}\")\n\n    df_resultados = pd.DataFrame(resultados)\n    # Salvar CSV consolidado e atualizar metadata\n    if pasta_resultados is not None:\n        csv_path = os.path.join(pasta_resultados, 'resultados_completos_artigo.csv')\n        df_resultados.to_csv(csv_path, index=False)\n        # Adicionar granularidade m√°xima ao metadata\n        metadata['csv_consolidado'] = csv_path\n        metadata['csvs_individuais'] = [os.path.join('experimentos_individuais', f) for f in os.listdir(pasta_individual) if f.endswith('.csv')]\n        # Atualizar lista de arquivos\n        metadata['arquivos_gerados'] = [f for f in os.listdir(pasta_resultados) if os.path.isfile(os.path.join(pasta_resultados, f))]\n        # Salvar metadata.json\n        if metadata_path:\n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2, ensure_ascii=False, default=str)\n    if verbose:\n        logger.info(f\"\\n{'='*80}\")\n        logger.info(f\" ‚úì GRID SEARCH CONCLU√çDO: {len(df_resultados)} experimentos\")\n        logger.info(f\"{'='*80}\\n\")\n    return df_resultados\n\n\n# ============================================================================\n# M√ìDULO 7: AN√ÅLISES ESTAT√çSTICAS\n# ============================================================================\\n\\nprint('‚úì Fun√ß√£o executar_grid_search definida!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. An√°lises Estat√≠sticas Avan√ßadas (QUALIS A1)\\n",
    "\\n",
    "### üí° Para Iniciantes\\n",
    "Usamos estat√≠stica rigorosa para provar que o ru√≠do realmente ajuda,\\n",
    "n√£o √© apenas sorte ou acaso.\\n",
    "\\n",
    "### üéì Para Especialistas\\n",
    "Pipeline estat√≠stico completo conforme padr√µes QUALIS A1:\\n",
    "\\n",
    "1. **ANOVA**: F-test para diferen√ßas entre grupos\\n",
    "2. **Post-hoc tests**: Bonferroni, Scheff√©, Tukey HSD\\n",
    "3. **Effect sizes**: \\n",
    "   - Cohen's d: $(\\\\mu_1 - \\\\mu_2) / s_{pooled}$\\n",
    "   - Glass's Œî: $(\\\\mu_1 - \\\\mu_2) / s_{control}$\\n",
    "   - Hedges' g: Cohen's d com corre√ß√£o para pequenas amostras\\n",
    "4. **Intervalos de confian√ßa**: 95% via bootstrap\\n",
    "5. **Testes de normalidade**: Shapiro-Wilk\\n",
    "6. **Homogeneidade de vari√¢ncias**: Levene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def executar_analises_estatisticas(df, verbose=True, pasta_resultados=None):\n    \"\"\"Executa an√°lises estat√≠sticas principais do artigo.\"\"\"\n    import os\n\n    # Inicializar metadata\n    analise_meta: dict[str, Any] = {\n        'tipo': 'analises_estatisticas',\n        'data_execucao': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'arquivos_gerados': [],\n        'csvs': {}\n    }\n    metadata_path: Optional[str] = None\n    pasta_individual: Optional[str] = None\n\n    if pasta_resultados is not None:\n        pasta_individual = os.path.join(pasta_resultados, 'analises_individuais')\n        os.makedirs(pasta_individual, exist_ok=True)\n        os.makedirs(pasta_resultados, exist_ok=True)\n        # Forense: README e metadata\n        readme_path = os.path.join(pasta_resultados, 'README_analises_estatisticas.md')\n        metadata_path = os.path.join(pasta_resultados, 'metadata_analises_estatisticas.json')\n        with open(readme_path, 'w', encoding='utf-8') as f:\n            f.write(\n                \"# An√°lises Estat√≠sticas\\n\\n\"\n                f\"- Data de execu√ß√£o: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n                \"- Conte√∫do: ANOVA, compara√ß√£o de inicializa√ß√µes, overfitting, effect sizes, post-hoc.\\n\"\n            )\n\n    if verbose:\n        logger.info(\"=\"*80)\n        logger.info(\" AN√ÅLISES ESTAT√çSTICAS\")\n        logger.info(\"=\"*80)\n\n    analises = {}\n\n    # 1. ANOVA 2-way: Noise √ó Dataset\n    if verbose:\n        logger.info(\"\\n1. ANOVA 2-WAY: Noise Level √ó Dataset\")\n        logger.info(\"-\"*80)\n\n    # Verificar se h√° dados\n    if len(df) == 0:\n        logger.warning(\"Nenhum resultado dispon√≠vel para an√°lise!\")\n        return {'erro': 'Sem dados'}\n\n    df_anova = df.copy()\n    df_anova['nivel_ruido_cat'] = df_anova['nivel_ruido'].astype(str)\n\n    # Proteger: ANOVA 2-way requer pelo menos 2 n√≠veis em cada fator\n    if df_anova['nivel_ruido_cat'].nunique() >= 2 and df_anova['dataset'].nunique() >= 2:\n        model = ols('acuracia_teste ~ C(nivel_ruido_cat) + C(dataset) + C(nivel_ruido_cat):C(dataset)',\n                    data=df_anova).fit()\n        anova_2way = anova_lm(model, typ=2)\n        analises['anova_2way'] = anova_2way\n        if verbose:\n            print(anova_2way)\n    else:\n        analises['anova_2way'] = 'Insuficiente para ANOVA 2-way (>=2 n√≠veis por fator)'\n        if verbose:\n            logger.info('Dados insuficientes para ANOVA 2-way, an√°lise pulada.')\n\n    # 2. Compara√ß√£o de inicializa√ß√µes\n    if verbose:\n        logger.info(\"\\n2. COMPARA√á√ÉO DE INICIALIZA√á√ïES\")\n        logger.info(\"-\"*80)\n\n    # Build aggregation dict based on available columns\n    agg_dict = {'acuracia_teste': ['mean', 'std']}\n    if 'tempo_segundos' in df.columns:\n        agg_dict['tempo_segundos'] = 'mean'\n    else:\n        if verbose:\n            logger.info(\"  ‚ÑπÔ∏è Coluna 'tempo_segundos' n√£o dispon√≠vel, an√°lise de tempo n√£o ser√° inclu√≠da.\")\n\n    comp_init = df.groupby('estrategia_init').agg(agg_dict).round(4)\n\n    if verbose:\n        print(comp_init)\n    # Salvar CSV resumo de inicializa√ß√µes\n    if pasta_resultados is not None:\n        comp_init_path = os.path.join(pasta_resultados, 'analise_comparacao_inicializacoes.csv')\n        try:\n            comp_init.to_csv(comp_init_path)\n            analise_meta['csvs']['comparacao_inicializacoes'] = comp_init_path\n        except Exception:\n            pass\n\n    # 2b. Compara√ß√£o consolidada: VQC vs Baselines Cl√°ssicos (SVM/RF)\n    try:\n        if verbose:\n            logger.info(\"\\n2b. COMPARA√á√ÉO: VQC vs SVM/RF (por dataset)\")\n            logger.info(\"-\"*80)\n        df_q = df[df['tipo_ruido'] != 'classico']\n        df_class = df[df['tipo_ruido'] == 'classico']\n        # Melhor VQC por dataset (maior acur√°cia)\n        vqc_best = df_q.groupby('dataset')['acuracia_teste'].max().rename('vqc_melhor')\n        # VQC sem ru√≠do (m√©dia por dataset)\n        vqc_sem = df[df['tipo_ruido'] == 'sem_ruido'].groupby('dataset')['acuracia_teste'].mean().rename('vqc_sem_ruido_media')\n        # Baselines\n        svm = (\n            df_class[df_class['arquitetura'] == 'SVM']\n            .groupby('dataset')['acuracia_teste']\n            .mean()\n            .rename('svm')\n        )\n        rf = (\n            df_class[df_class['arquitetura'] == 'RandomForest']\n            .groupby('dataset')['acuracia_teste']\n            .mean()\n            .rename('rf')\n        )\n        comp = pd.concat([vqc_best, vqc_sem, svm, rf], axis=1)\n        # Deltas (podem ser NaN se baseline ausente)\n        comp['delta_vqc_svm'] = comp['vqc_melhor'] - comp['svm']\n        comp['delta_vqc_rf'] = comp['vqc_melhor'] - comp['rf']\n        comp = comp.reset_index()\n        if verbose:\n            logger.info(\"Resumo por dataset:\")\n            logger.info(comp.round(4).to_string(index=False))\n        if pasta_resultados is not None:\n            comp_path = os.path.join(pasta_resultados, 'comparacao_baselines.csv')\n            try:\n                comp.to_csv(comp_path, index=False)\n                analise_meta['csvs']['comparacao_baselines'] = comp_path\n            except Exception:\n                pass\n    except Exception as e:\n        if verbose:\n            logger.warning(f\"Falha ao gerar comparacao_baselines.csv: {str(e)[:80]}\")\n    # Salvar DataFrame completo das an√°lises estat√≠sticas\n    try:\n        df.to_csv(os.path.join(str(pasta_resultados), 'analises_estatisticas_completo.csv'), index=False)\n        analise_meta['csvs']['completo'] = os.path.join(str(pasta_resultados), 'analises_estatisticas_completo.csv')\n        # Salvar cada an√°lise individualmente em CSV\n        if pasta_individual is not None:\n            for idx, row in df.iterrows():\n                id_analise = f\"analise_{idx:05d}\"\n                df_row = pd.DataFrame([row])\n                csv_analise_path = os.path.join(str(pasta_individual), f\"{id_analise}.csv\")\n                df_row.to_csv(csv_analise_path, index=False)\n            # Listar CSVs apenas se pasta_individual √© v√°lido\n            analise_meta['csvs_individuais'] = [os.path.join('analises_individuais', f) for f in os.listdir(str(pasta_individual)) if f.endswith('.csv')]\n    except Exception:\n        pass\n\n    analises['comparacao_inicializacoes'] = comp_init\n\n    # 3. An√°lise de overfitting\n    if verbose:\n        logger.info(\"\\n3. AN√ÅLISE DE OVERFITTING\")\n        logger.info(\"-\"*80)\n\n    # Check if required columns exist\n    if 'gap_treino_teste' in df.columns and 'tipo_ruido' in df.columns:\n        gap_sem_ruido = df[df['tipo_ruido'] == 'sem_ruido']['gap_treino_teste'].mean()\n        mask_otimo = (df['tipo_ruido'] == 'depolarizante') & (df['nivel_ruido'] == 0.01)\n        gap_com_ruido = df[mask_otimo]['gap_treino_teste'].mean()\n\n        if not np.isnan(gap_sem_ruido) and not np.isnan(gap_com_ruido) and gap_sem_ruido != 0:\n            reducao_overfitting = ((gap_sem_ruido - gap_com_ruido) / gap_sem_ruido) * 100\n        else:\n            reducao_overfitting = 0.0\n\n        if verbose:\n            logger.info(f\"Gap sem ru√≠do: {gap_sem_ruido:.4f}\")\n            logger.info(f\"Gap com ru√≠do √≥timo: {gap_com_ruido:.4f}\")\n            logger.info(f\"Redu√ß√£o de overfitting: {reducao_overfitting:.1f}%\")\n\n        analises['overfitting'] = {\n            'gap_sem_ruido': gap_sem_ruido,\n            'gap_com_ruido': gap_com_ruido,\n            'reducao_percent': reducao_overfitting\n        }\n    else:\n        if verbose:\n            logger.info(\"Colunas necess√°rias n√£o dispon√≠veis para an√°lise de overfitting.\")\n        analises['overfitting'] = {\n            'gap_sem_ruido': np.nan,\n            'gap_com_ruido': np.nan,\n            'reducao_percent': 0.0\n        }\n\n    # 4. Effect Sizes (Cohen's d, Glass's Œî, Hedges' g)\n    if verbose:\n        logger.info(\"\\n4. EFFECT SIZES\")\n        logger.info(\"-\"*80)\n\n    sem_ruido = df[df['tipo_ruido'] == 'sem_ruido']['acuracia_teste'].values\n    com_ruido = df[df['tipo_ruido'] != 'sem_ruido']['acuracia_teste'].values\n\n    if len(sem_ruido) > 0 and len(com_ruido) > 0:\n        cohen_d = TestesEstatisticosAvancados.cohen_d(com_ruido, sem_ruido)\n        glass_delta = TestesEstatisticosAvancados.glass_delta(com_ruido, sem_ruido)\n        hedges_g = TestesEstatisticosAvancados.hedges_g(com_ruido, sem_ruido)\n\n        if verbose:\n            logger.info(f\"Cohen's d: {cohen_d:.4f}\")\n            logger.info(f\"Glass's Œî: {glass_delta:.4f}\")\n            logger.info(f\"Hedges' g: {hedges_g:.4f}\")\n\n        analises['effect_sizes'] = {\n            'cohen_d': cohen_d,\n            'glass_delta': glass_delta,\n            'hedges_g': hedges_g\n        }\n\n    # 5. Testes Post-hoc (Bonferroni)\n    if verbose:\n        logger.info(\"\\n5. TESTES POST-HOC\")\n        logger.info(\"-\"*80)\n\n    # Comparar cada tipo de ru√≠do vs. baseline\n    tipos_ruido = df['tipo_ruido'].unique()\n    p_values = []\n\n    for tipo in tipos_ruido:\n        if tipo != 'sem_ruido':\n            grupo1 = df[df['tipo_ruido'] == tipo]['acuracia_teste'].values\n            grupo2 = df[df['tipo_ruido'] == 'sem_ruido']['acuracia_teste'].values\n\n            if len(grupo1) > 0 and len(grupo2) > 0:\n                _, p_val = ttest_ind(grupo1, grupo2)\n                p_values.append((tipo, p_val))\n\n    if len(p_values) > 0:\n        # Corre√ß√£o de Bonferroni\n        p_vals_only = [p for _, p in p_values]\n        significantes = TestesEstatisticosAvancados.bonferroni(p_vals_only, alpha=0.05)\n\n        if verbose:\n            for (tipo, p_val), sig in zip(p_values, significantes):\n                status = \"‚úì Significativo\" if sig else \"‚úó N√£o significativo\"\n                logger.info(f\"{tipo:20s}: p={p_val:.4f} {status}\")\n\n        analises['posthoc_bonferroni'] = list(zip(p_values, significantes))\n\n    # Persistir metadata\n    if pasta_resultados is not None and metadata_path is not None:\n        # Atualizar lista de arquivos gerados no diret√≥rio\n        try:\n            analise_meta['arquivos_gerados'] = [f for f in os.listdir(pasta_resultados) if os.path.isfile(os.path.join(pasta_resultados, f))]\n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(analise_meta, f, indent=2, ensure_ascii=False, default=str)\n        except Exception:\n            pass\n    return analises\n\n\n# ============================================================================\n# M√ìDULO 8: VISUALIZA√á√ïES\n# ============================================================================\\n\\nprint('‚úì Fun√ß√£o executar_analises_estatisticas definida!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Gera√ß√£o de Visualiza√ß√µes Cient√≠ficas\\n",
    "\\n",
    "### üí° Para Iniciantes\\n",
    "Gr√°ficos interativos que mostram claramente como o ru√≠do afeta o desempenho.\\n",
    "\\n",
    "### üéì Para Especialistas\\n",
    "Visualiza√ß√µes com padr√µes de publica√ß√£o QUALIS A1:\\n",
    "- Resolu√ß√£o 300 DPI\\n",
    "- Fonte Times New Roman\\n",
    "- Barras de erro (SEM √ó 1.96 para IC 95%)\\n",
    "- Legendas cient√≠ficas completas\\n",
    "- Formato interativo (Plotly) e export√°vel (PNG/SVG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def gerar_visualizacoes(df, salvar=True, pasta_resultados=None):\n    \"\"\"Gera as figuras principais do artigo.\"\"\"\n    import os\n\n    # Inicializar metadata\n    viz_meta: dict[str, Any] = {\n        'tipo': 'visualizacoes',\n        'data_execucao': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'arquivos_gerados': [],\n        'figuras': []\n    }\n    metadata_path: Optional[str] = None\n    pasta_individual: Optional[str] = None\n\n    if pasta_resultados is not None:\n        pasta_individual = os.path.join(pasta_resultados, 'visualizacoes_individuais')\n        os.makedirs(pasta_individual, exist_ok=True)\n        os.makedirs(pasta_resultados, exist_ok=True)\n        # Forense: README e metadata\n        readme_path = os.path.join(pasta_resultados, 'README_visualizacoes.md')\n        metadata_path = os.path.join(pasta_resultados, 'metadata_visualizacoes.json')\n        os.makedirs(os.path.dirname(readme_path), exist_ok=True)\n        os.makedirs(os.path.dirname(metadata_path), exist_ok=True)\n        with open(readme_path, 'w', encoding='utf-8') as f:\n            f.write(\n                \"# Visualiza√ß√µes Geradas\\n\\n\"\n                f\"- Data de execu√ß√£o: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n                \"- Figuras: 2, 2b, 3, 3b, 4, 5, 6, 7\\n\"\n            )\n\n    logger.info(\"=\"*80)\n    logger.info(\" GERANDO VISUALIZA√á√ïES\")\n    logger.info(\"=\"*80)\n\n    figuras = {}\n\n    # FIGURA 2: Beneficial Noise (PRINCIPAL)\n    logger.info(\"\\n\" + \"=\"*80)\n    logger.info(\"GERANDO FIGURA 2: BENEFICIAL NOISE ANALYSIS (QUALIS A1)\")\n    logger.info(\"=\"*80)\n    logger.info(\"Specifications: High-resolution (300 DPI), Publication-ready formats\")\n    logger.info(\"Formats: HTML (interactive), PNG, PDF, SVG\")\n\n    fig2 = px.scatter(\n        df, x='nivel_ruido', y='acuracia_teste',\n        color='tipo_ruido', facet_col='dataset',\n        title=\"Figure 2: Quantum Noise Impact on Classifier Accuracy (Beneficial Regime Analysis)\",\n        labels={\n            'nivel_ruido': 'Noise Level (Œ≥)', \n            'acuracia_teste': 'Test Accuracy (%)',\n            'tipo_ruido': 'Noise Type'\n        },\n        height=600\n    )\n\n    if salvar:\n        # Qualis A1: aprimorar layout para publica√ß√£o cient√≠fica\n        fig2.update_layout(\n            font=dict(family='Times New Roman, serif', size=18, color='black'),\n            title_font=dict(size=24, family='Times New Roman, serif', color='black', weight=\"bold\"),\n            legend_title_font=dict(size=20, family='Times New Roman, serif', color='black', weight=\"bold\"),\n            legend_font=dict(size=18, family='Times New Roman, serif', color='black'),\n            margin=dict(l=80, r=60, t=100, b=80),\n            paper_bgcolor='white',\n            plot_bgcolor='white',\n            showlegend=True,\n            legend=dict(\n                bgcolor='rgba(255,255,255,0.9)',\n                bordercolor='black',\n                borderwidth=1\n            )\n        )\n        fig2.update_traces(\n            marker=dict(\n                size=8, \n                line=dict(width=1.5, color='black'),\n                opacity=0.8\n            )\n        )\n        fig2.update_xaxes(\n            showgrid=True, \n            gridwidth=1, \n            gridcolor='lightgray', \n            zeroline=False, \n            ticks='outside', \n            tickfont=dict(size=16, family='Times New Roman, serif'),\n            linewidth=2,\n            linecolor='black',\n            mirror=True\n        )\n        fig2.update_yaxes(\n            showgrid=True, \n            gridwidth=1, \n            gridcolor='lightgray', \n            zeroline=False, \n            ticks='outside', \n            tickfont=dict(size=16, family='Times New Roman, serif'),\n            linewidth=2,\n            linecolor='black',\n            mirror=True\n        )\n        # Exportar em alta resolu√ß√£o e formatos cient√≠ficos (QUALIS A1 standards)\n        path_html = os.path.join(pasta_resultados if pasta_resultados else '', 'figura2_beneficial_noise.html')\n        os.makedirs(os.path.dirname(path_html), exist_ok=True)\n        fig2.write_html(path_html)\n        logger.info(f\"  ‚úì Saved: {os.path.basename(path_html)} (interactive HTML)\")\n        \n        if pasta_resultados is not None:\n            viz_meta['figuras'].append(path_html)\n            path_png = os.path.join(pasta_resultados, 'figura2_beneficial_noise.png')\n            path_pdf = os.path.join(pasta_resultados, 'figura2_beneficial_noise.pdf')\n            path_svg = os.path.join(pasta_resultados, 'figura2_beneficial_noise.svg')\n            for p in [path_png, path_pdf, path_svg]:\n                os.makedirs(os.path.dirname(p), exist_ok=True)\n            # QUALIS A1: Exportar em 300 DPI (scale=3 para 1200x800 = 300 DPI)\n            logger.info(f\"  ‚è≥ Exporting high-resolution formats (300 DPI)...\")\n            fig2.write_image(path_png, format='png', scale=3, width=1600, height=1000)\n            fig2.write_image(path_pdf, format='pdf', width=1600, height=1000)\n            fig2.write_image(path_svg, format='svg', width=1600, height=1000)\n            viz_meta['figuras'] += [path_png, path_pdf, path_svg]\n            logger.info(f\"  ‚úì Saved: {os.path.basename(path_png)} (PNG 300 DPI)\")\n            logger.info(f\"  ‚úì Saved: {os.path.basename(path_pdf)} (PDF vector)\")\n            logger.info(f\"  ‚úì Saved: {os.path.basename(path_svg)} (SVG vector)\")\n    logger.info(\"FIGURA 2: COMPLETED\")\n    logger.info(\"=\"*80)\n    figuras['figura2'] = fig2\n\n    # FIGURA 2b: Beneficial Noise com IC95% por grupo (dataset, tipo_ruido, nivel_ruido)\n    logger.info(\"\\n\" + \"=\"*80)\n    logger.info(\"GERANDO FIGURA 2b: BENEFICIAL NOISE WITH 95% CONFIDENCE INTERVALS\")\n    logger.info(\"=\"*80)\n    try:\n        df_q = df[df['tipo_ruido'] != 'classico'].copy()\n        grp_cols = ['dataset', 'tipo_ruido', 'nivel_ruido']\n        df_ci = (\n            df_q.groupby(grp_cols)\n            .agg(media=('acuracia_teste', 'mean'), desvio=('acuracia_teste', 'std'), n=('acuracia_teste', 'count'))\n            .reset_index()\n        )\n        # Evitar divis√£o por zero para n<=1\n        df_ci['sem'] = df_ci.apply(lambda r: (r['desvio'] / np.sqrt(r['n'])) if r['n'] > 1 and r['desvio'] == r['desvio'] else 0.0, axis=1)\n        df_ci['ci95'] = 1.96 * df_ci['sem']\n        logger.info(f\"  Statistical Summary: {len(df_ci)} data points with 95% CI\")\n        \n        fig2b = px.scatter(\n            df_ci, x='nivel_ruido', y='media', color='tipo_ruido', facet_col='dataset',\n            error_y='ci95',\n            title='Figure 2b: Mean Accuracy ¬± 95% CI by Noise Level',\n            labels={\n                'nivel_ruido': 'Noise Level (Œ≥)', \n                'media': 'Mean Test Accuracy (%)',\n                'tipo_ruido': 'Noise Type'\n            },\n            height=600\n        )\n        # Apar√™ncia consistente com QUALIS A1\n        fig2b.update_layout(\n            font=dict(family='Times New Roman, serif', size=18, color='black'),\n            title_font=dict(size=24, family='Times New Roman, serif', color='black', weight=\"bold\"),\n            legend_title_font=dict(size=20, family='Times New Roman, serif', color='black', weight=\"bold\"),\n            legend_font=dict(size=18, family='Times New Roman, serif', color='black'),\n            margin=dict(l=80, r=60, t=100, b=80),\n            paper_bgcolor='white', \n            plot_bgcolor='white',\n            showlegend=True,\n            legend=dict(\n                bgcolor='rgba(255,255,255,0.9)',\n                bordercolor='black',\n                borderwidth=1\n            )\n        )\n        fig2b.update_traces(\n            marker=dict(\n                size=10, \n                line=dict(width=1.5, color='black'),\n                opacity=0.8\n            ),\n            error_y=dict(thickness=2, width=6)\n        )\n        fig2b.update_xaxes(\n            showgrid=True, \n            gridwidth=1, \n            gridcolor='lightgray', \n            zeroline=False, \n            ticks='outside', \n            tickfont=dict(size=16, family='Times New Roman, serif'),\n            linewidth=2,\n            linecolor='black',\n            mirror=True\n        )\n        fig2b.update_yaxes(\n            showgrid=True, \n            gridwidth=1, \n            gridcolor='lightgray', \n            zeroline=False, \n            ticks='outside', \n            tickfont=dict(size=16, family='Times New Roman, serif'),\n            linewidth=2,\n            linecolor='black',\n            mirror=True\n        )\n        if salvar:\n            path_html = os.path.join(pasta_resultados if pasta_resultados else '', 'figura2b_beneficial_noise_ic95.html')\n            os.makedirs(os.path.dirname(path_html), exist_ok=True)\n            fig2b.write_html(path_html)\n            logger.info(f\"  ‚úì Saved: {os.path.basename(path_html)} (interactive HTML)\")\n            \n            if pasta_resultados is not None:\n                viz_meta['figuras'].append(path_html)\n                path_png = os.path.join(pasta_resultados, 'figura2b_beneficial_noise_ic95.png')\n                path_pdf = os.path.join(pasta_resultados, 'figura2b_beneficial_noise_ic95.pdf')\n                path_svg = os.path.join(pasta_resultados, 'figura2b_beneficial_noise_ic95.svg')\n                for p in [path_png, path_pdf, path_svg]:\n                    os.makedirs(os.path.dirname(p), exist_ok=True)\n                logger.info(f\"  ‚è≥ Exporting high-resolution formats (300 DPI)...\")\n                fig2b.write_image(path_png, format='png', scale=3, width=1600, height=1000)\n                fig2b.write_image(path_pdf, format='pdf', width=1600, height=1000)\n                fig2b.write_image(path_svg, format='svg', width=1600, height=1000)\n                viz_meta['figuras'] += [path_png, path_pdf, path_svg]\n                logger.info(f\"  ‚úì Saved: {os.path.basename(path_png)} (PNG 300 DPI)\")\n                logger.info(f\"  ‚úì Saved: {os.path.basename(path_pdf)} (PDF vector)\")\n                logger.info(f\"  ‚úì Saved: {os.path.basename(path_svg)} (SVG vector)\")\n        figuras['figura2b'] = fig2b\n        logger.info(\"FIGURA 2b: COMPLETED\")\n        logger.info(\"=\"*80)\n    except Exception as e:\n        logger.warning(f\"N√£o foi poss√≠vel gerar a Figura 2b (IC95%): {str(e)[:80]}\")\n\n    # FIGURA 3: Noise Type Comparison\n    logger.info(\"Gerando Figura 3: Compara√ß√£o de Tipos de Ru√≠do...\")\n\n    fig3 = px.box(\n        df, x='tipo_ruido', y='acuracia_teste', color='tipo_ruido',\n        title=\"Figura 3: Compara√ß√£o de Tipos de Ru√≠do\",\n        labels={'tipo_ruido': 'Tipo de Ru√≠do', 'acuracia_teste': 'Acur√°cia no Teste'},\n        height=500\n    )\n\n    if salvar:\n        fig3.update_layout(\n            font=dict(family='serif', size=18, color='black'),\n            title_font=dict(size=22, family='serif', color='black', weight=\"bold\"),\n            legend_title_font=dict(size=18, family='serif', color='black', weight=\"bold\"),\n            legend_font=dict(size=16, family='serif', color='black'),\n            margin=dict(l=60, r=40, t=80, b=60),\n            paper_bgcolor='white',\n            plot_bgcolor='white',\n        )\n        fig3.update_traces(marker=dict(line=dict(width=1, color='black')))\n        fig3.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))\n        fig3.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))\n        path_html = os.path.join(pasta_resultados if pasta_resultados else '', 'figura3_noise_types.html')\n        os.makedirs(os.path.dirname(path_html), exist_ok=True)\n        fig3.write_html(path_html)\n        if pasta_resultados is not None:\n            viz_meta['figuras'].append(path_html)\n            path_png = os.path.join(pasta_resultados, 'figura3_noise_types.png')\n            path_pdf = os.path.join(pasta_resultados, 'figura3_noise_types.pdf')\n            path_svg = os.path.join(pasta_resultados, 'figura3_noise_types.svg')\n            for p in [path_png, path_pdf, path_svg]:\n                os.makedirs(os.path.dirname(p), exist_ok=True)\n            fig3.write_image(path_png, format='png', scale=3, width=1200, height=800)\n            fig3.write_image(path_pdf, format='pdf', width=1200, height=800)\n            fig3.write_image(path_svg, format='svg', width=1200, height=800)\n            viz_meta['figuras'] += [path_png, path_pdf, path_svg]\n    figuras['figura3'] = fig3\n\n    # FIGURA 3b: M√©dias por Tipo de Ru√≠do com IC95% (facet por dataset)\n    logger.info(\"Gerando Figura 3b: Tipos de Ru√≠do com IC95%...\")\n    try:\n        df_q2 = df[df['tipo_ruido'] != 'classico'].copy()\n        grp_cols3 = ['dataset', 'tipo_ruido']\n        df_ci3 = (\n            df_q2.groupby(grp_cols3)\n            .agg(media=('acuracia_teste', 'mean'), desvio=('acuracia_teste', 'std'), n=('acuracia_teste', 'count'))\n            .reset_index()\n        )\n        df_ci3['sem'] = df_ci3.apply(lambda r: (r['desvio'] / np.sqrt(r['n'])) if r['n'] > 1 and r['desvio'] == r['desvio'] else 0.0, axis=1)\n        df_ci3['ci95'] = 1.96 * df_ci3['sem']\n        fig3b = px.bar(\n            df_ci3, x='tipo_ruido', y='media', color='tipo_ruido', facet_col='dataset',\n            error_y='ci95', barmode='group',\n            title='Figura 3b: Acur√°cia M√©dia ¬± IC95% por Tipo de Ru√≠do',\n            labels={'media': 'Acur√°cia M√©dia (Teste)', 'tipo_ruido': 'Tipo de Ru√≠do'}, height=500\n        )\n        if salvar:\n            fig3b.update_layout(\n                font=dict(family='serif', size=18, color='black'),\n                title_font=dict(size=22, family='serif', color='black', weight=\"bold\"),\n                legend_title_font=dict(size=18, family='serif', color='black', weight=\"bold\"),\n                legend_font=dict(size=16, family='serif', color='black'),\n                margin=dict(l=60, r=40, t=80, b=60),\n                paper_bgcolor='white', plot_bgcolor='white',\n            )\n            path_html = os.path.join(pasta_resultados if pasta_resultados else '', 'figura3b_noise_types_ic95.html')\n            os.makedirs(os.path.dirname(path_html), exist_ok=True)\n            fig3b.write_html(path_html)\n            if pasta_resultados is not None:\n                viz_meta['figuras'].append(path_html)\n                path_png = os.path.join(pasta_resultados, 'figura3b_noise_types_ic95.png')\n                path_pdf = os.path.join(pasta_resultados, 'figura3b_noise_types_ic95.pdf')\n                path_svg = os.path.join(pasta_resultados, 'figura3b_noise_types_ic95.svg')\n                for p in [path_png, path_pdf, path_svg]:\n                    os.makedirs(os.path.dirname(p), exist_ok=True)\n                fig3b.write_image(path_png, format='png', scale=3, width=1200, height=800)\n                fig3b.write_image(path_pdf, format='pdf', width=1200, height=800)\n                fig3b.write_image(path_svg, format='svg', width=1200, height=800)\n                viz_meta['figuras'] += [path_png, path_pdf, path_svg]\n        figuras['figura3b'] = fig3b\n    except Exception as e:\n        logger.warning(f\"N√£o foi poss√≠vel gerar a Figura 3b (IC95%): {str(e)[:80]}\")\n\n    # FIGURA 4: Initialization Strategies\n    logger.info(\"Gerando Figura 4: Estrat√©gias de Inicializa√ß√£o...\")\n\n    fig4 = px.box(\n        df, x='estrategia_init', y='acuracia_teste', color='estrategia_init',\n        title=\"Figura 4: Impacto da Estrat√©gia de Inicializa√ß√£o\",\n        labels={'estrategia_init': 'Estrat√©gia', 'acuracia_teste': 'Acur√°cia no Teste'},\n        height=500\n    )\n\n    if salvar:\n        fig4.update_layout(\n            font=dict(family='serif', size=18, color='black'),\n            title_font=dict(size=22, family='serif', color='black', weight=\"bold\"),\n            legend_title_font=dict(size=18, family='serif', color='black', weight=\"bold\"),\n            legend_font=dict(size=16, family='serif', color='black'),\n            margin=dict(l=60, r=40, t=80, b=60),\n            paper_bgcolor='white',\n            plot_bgcolor='white',\n        )\n        fig4.update_traces(marker=dict(line=dict(width=1, color='black')))\n        fig4.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))\n        fig4.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))\n        path_html = os.path.join(pasta_resultados if pasta_resultados else '', 'figura4_initialization.html')\n        os.makedirs(os.path.dirname(path_html), exist_ok=True)\n        fig4.write_html(path_html)\n        if pasta_resultados is not None:\n            viz_meta['figuras'].append(path_html)\n            path_png = os.path.join(pasta_resultados, 'figura4_initialization.png')\n            path_pdf = os.path.join(pasta_resultados, 'figura4_initialization.pdf')\n            path_svg = os.path.join(pasta_resultados, 'figura4_initialization.svg')\n            for p in [path_png, path_pdf, path_svg]:\n                os.makedirs(os.path.dirname(p), exist_ok=True)\n            fig4.write_image(path_png, format='png', scale=3, width=1200, height=800)\n            fig4.write_image(path_pdf, format='pdf', width=1200, height=800)\n            fig4.write_image(path_svg, format='svg', width=1200, height=800)\n            viz_meta['figuras'] += [path_png, path_pdf, path_svg]\n    figuras['figura4'] = fig4\n\n    # FIGURA 5: Architecture Trade-offs\n    logger.info(\"Gerando Figura 5: Trade-offs de Arquitetura...\")\n\n    # Check if tempo_segundos is available, otherwise use a placeholder\n    if 'tempo_segundos' in df.columns:\n        fig5 = px.scatter(\n            df, x='tempo_segundos', y='acuracia_teste', color='arquitetura',\n            title=\"Figura 5: Trade-off Tempo vs. Acur√°cia\",\n            labels={'tempo_segundos': 'Tempo (s)', 'acuracia_teste': 'Acur√°cia no Teste'},\n            height=500\n        )\n    else:\n        # Fallback: use index as x-axis if tempo_segundos not available\n        fig5 = px.scatter(\n            df, x=df.index, y='acuracia_teste', color='arquitetura',\n            title=\"Figura 5: Acur√°cia por Arquitetura\",\n            labels={'x': 'Experimento', 'acuracia_teste': 'Acur√°cia no Teste'},\n            height=500\n        )\n\n    if salvar:\n        fig5.update_layout(\n            font=dict(family='serif', size=18, color='black'),\n            title_font=dict(size=22, family='serif', color='black', weight=\"bold\"),\n            legend_title_font=dict(size=18, family='serif', color='black', weight=\"bold\"),\n            legend_font=dict(size=16, family='serif', color='black'),\n            margin=dict(l=60, r=40, t=80, b=60),\n            paper_bgcolor='white',\n            plot_bgcolor='white',\n        )\n        fig5.update_traces(marker=dict(line=dict(width=1, color='black')))\n        fig5.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))\n        fig5.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))\n        path_html = os.path.join(pasta_resultados if pasta_resultados else '', 'figura5_architecture_tradeoffs.html')\n        os.makedirs(os.path.dirname(path_html), exist_ok=True)\n        fig5.write_html(path_html)\n        if pasta_resultados is not None:\n            viz_meta['figuras'].append(path_html)\n            path_png = os.path.join(pasta_resultados, 'figura5_architecture_tradeoffs.png')\n            path_pdf = os.path.join(pasta_resultados, 'figura5_architecture_tradeoffs.pdf')\n            path_svg = os.path.join(pasta_resultados, 'figura5_architecture_tradeoffs.svg')\n            for p in [path_png, path_pdf, path_svg]:\n                os.makedirs(os.path.dirname(p), exist_ok=True)\n            fig5.write_image(path_png, format='png', scale=3, width=1200, height=800)\n            fig5.write_image(path_pdf, format='pdf', width=1200, height=800)\n            fig5.write_image(path_svg, format='svg', width=1200, height=800)\n            viz_meta['figuras'] += [path_png, path_pdf, path_svg]\n    figuras['figura5'] = fig5\n\n    # FIGURA 7: Overfitting Analysis\n    logger.info(\"Gerando Figura 7: An√°lise de Overfitting...\")\n\n    # Check if required columns exist\n    if 'gap_treino_teste' in df.columns and 'acuracia_treino' in df.columns:\n        # Garantir que os tamanhos sejam positivos (usar valor absoluto)\n        df_fig7 = df.copy()\n        df_fig7['gap_abs'] = df_fig7['gap_treino_teste'].abs()\n\n        fig7 = px.scatter(\n            df_fig7, x='acuracia_treino', y='acuracia_teste', color='tipo_ruido',\n            size='gap_abs', hover_data=['dataset', 'arquitetura', 'gap_treino_teste'],\n            title=\"Figura 7: An√°lise de Overfitting (Gap Treino-Teste)\",\n            labels={'acuracia_treino': 'Acur√°cia Treino', 'acuracia_teste': 'Acur√°cia Teste'},\n            height=500\n        )\n\n        # Adicionar linha diagonal (sem overfitting)\n        fig7.add_trace(go.Scatter(\n            x=[0, 1], y=[0, 1], mode='lines',\n            line=dict(dash='dash', color='gray'),\n            name='Sem Overfitting'\n        ))\n    else:\n        # Fallback: create simple scatter plot without overfitting info\n        logger.info(\"Coluna gap_treino_teste n√£o dispon√≠vel, gerando visualiza√ß√£o simplificada...\")\n        fig7 = px.scatter(\n            df, x=df.index, y='acuracia_teste', color='tipo_ruido',\n            title=\"Figura 7: Acur√°cia por Tipo de Ru√≠do\",\n            labels={'x': 'Experimento', 'acuracia_teste': 'Acur√°cia Teste'},\n            height=500\n        )\n\n    if salvar:\n        fig7.update_layout(\n            font=dict(family='serif', size=18, color='black'),\n            title_font=dict(size=22, family='serif', color='black', weight=\"bold\"),\n            legend_title_font=dict(size=18, family='serif', color='black', weight=\"bold\"),\n            legend_font=dict(size=16, family='serif', color='black'),\n            margin=dict(l=60, r=40, t=80, b=60),\n            paper_bgcolor='white',\n            plot_bgcolor='white',\n        )\n        fig7.update_traces(marker=dict(line=dict(width=1, color='black')))\n        fig7.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))\n        fig7.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray', zeroline=False, ticks='outside', tickfont=dict(size=16, family='serif'))\n        path_html = os.path.join(pasta_resultados if pasta_resultados else '', 'figura7_overfitting.html')\n        os.makedirs(os.path.dirname(path_html), exist_ok=True)\n        fig7.write_html(path_html)\n        if pasta_resultados is not None:\n            viz_meta['figuras'].append(path_html)\n            path_png = os.path.join(pasta_resultados, 'figura7_overfitting.png')\n            path_pdf = os.path.join(pasta_resultados, 'figura7_overfitting.pdf')\n            path_svg = os.path.join(pasta_resultados, 'figura7_overfitting.svg')\n            for p in [path_png, path_pdf, path_svg]:\n                os.makedirs(os.path.dirname(p), exist_ok=True)\n            fig7.write_image(path_png, format='png', scale=3, width=1200, height=800)\n            fig7.write_image(path_pdf, format='pdf', width=1200, height=800)\n            fig7.write_image(path_svg, format='svg', width=1200, height=800)\n            viz_meta['figuras'] += [path_png, path_pdf, path_svg]\n    figuras['figura7'] = fig7\n\n    # FIGURA 6: Effect Sizes Comparison\n    logger.info(\"Gerando Figura 6: Compara√ß√£o de Effect Sizes...\")\n\n    # Calcular effect sizes para cada par ru√≠do vs. baseline\n    effect_data = []\n    for dataset in df['dataset'].unique():\n        df_ds = df[df['dataset'] == dataset]\n        sem_ruido = df_ds[df_ds['tipo_ruido'] == 'sem_ruido']['acuracia_teste'].values\n\n        for tipo in df_ds['tipo_ruido'].unique():\n            if tipo != 'sem_ruido':\n                com_ruido = df_ds[df_ds['tipo_ruido'] == tipo]['acuracia_teste'].values\n\n                if len(sem_ruido) > 0 and len(com_ruido) > 0:\n                    cohen_d = TestesEstatisticosAvancados.cohen_d(com_ruido, sem_ruido)\n                    glass_d = TestesEstatisticosAvancados.glass_delta(com_ruido, sem_ruido)\n                    hedges_g = TestesEstatisticosAvancados.hedges_g(com_ruido, sem_ruido)\n\n                    effect_data.append({\n                        'dataset': dataset,\n                        'tipo_ruido': tipo,\n                        \"Cohen's d\": cohen_d,\n                        \"Glass's Œî\": glass_d,\n                        \"Hedges' g\": hedges_g\n                    })\n\n    if len(effect_data) > 0:\n        df_effects = pd.DataFrame(effect_data)\n        df_effects_melted = df_effects.melt(\n            id_vars=['dataset', 'tipo_ruido'],\n            value_vars=[\"Cohen's d\", \"Glass's Œî\", \"Hedges' g\"],\n            var_name='M√©trica',\n            value_name='Effect Size'\n        )\n\n        fig6 = px.bar(\n            df_effects_melted,\n            x='tipo_ruido',\n            y='Effect Size',\n            color='M√©trica',\n            facet_col='dataset',\n            barmode='group',\n            title=\"Figura 6: Compara√ß√£o de Effect Sizes (vs. Baseline)\",\n            height=500\n        )\n\n        # Adicionar linhas de refer√™ncia (small/medium/large effects)\n        # Usar getattr para evitar warnings de tipo com atributos din√¢micos do Plotly\n        annotations = getattr(fig6.layout, 'annotations', None)\n        if annotations:\n            for annotation in annotations:\n                annotation.text = annotation.text.replace('dataset=', '')\n\n        if salvar:\n            path = os.path.join(pasta_resultados if pasta_resultados else '', 'figura6_effect_sizes.html')\n            os.makedirs(os.path.dirname(path), exist_ok=True)\n            fig6.write_html(path)\n            if pasta_resultados is not None:\n                viz_meta['figuras'].append(path)\n\n        figuras['figura6'] = fig6\n\n    logger.info(f\"\\n‚úì {len(figuras)} figuras geradas!\")\n    # Persistir metadata\n    if pasta_resultados is not None and metadata_path is not None:\n        try:\n            viz_meta['arquivos_gerados'] = [f for f in os.listdir(pasta_resultados) if os.path.isfile(os.path.join(pasta_resultados, f))]\n            os.makedirs(os.path.dirname(metadata_path), exist_ok=True)\n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(viz_meta, f, indent=2, ensure_ascii=False, default=str)\n                # Salvar DataFrame completo das visualiza√ß√µes\n                csv_completo_path = os.path.join(pasta_resultados, 'visualizacoes_completo.csv')\n                os.makedirs(os.path.dirname(csv_completo_path), exist_ok=True)\n                df.to_csv(csv_completo_path, index=False)\n                viz_meta['csv_completo'] = csv_completo_path\n                # Salvar cada visualiza√ß√£o individualmente em CSV\n                if pasta_individual is not None:\n                    os.makedirs(pasta_individual, exist_ok=True)\n                    for idx, row in df.iterrows():\n                        id_vis = f\"vis_{idx:05d}\"\n                        df_row = pd.DataFrame([row])\n                        csv_vis_path = os.path.join(pasta_individual, f\"{id_vis}.csv\")\n                        os.makedirs(os.path.dirname(csv_vis_path), exist_ok=True)\n                        df_row.to_csv(csv_vis_path, index=False)\n                    viz_meta['csvs_individuais'] = [os.path.join('visualizacoes_individuais', f) for f in os.listdir(pasta_individual) if f.endswith('.csv')]\n        except Exception:\n            pass\n\n    return figuras\n\n\n# ============================================================================\n# M√ìDULO: AN√ÅLISES ESTAT√çSTICAS PROFUNDAS (v7.1)\n# ============================================================================\\n\\nprint('‚úì Fun√ß√£o gerar_visualizacoes definida!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\\n",
    "\\n",
    "## 11. Execu√ß√£o do Framework Completo\\n",
    "\\n",
    "### ‚ö†Ô∏è ATEN√á√ÉO\\n",
    "\\n",
    "A execu√ß√£o completa do framework pode levar **48-72 horas** em CPU padr√£o.\\n",
    "\\n",
    "### üöÄ Op√ß√µes de Execu√ß√£o\\n",
    "\\n",
    "#### Modo R√°pido (1-2 horas)\\n",
    "Para teste r√°pido, use menos √©pocas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modo r√°pido: apenas 5 √©pocas\\n",
    "# Descomente para executar:\\n",
    "\\n",
    "# import os\\n",
    "# os.environ['VQC_QUICK'] = '1'  # Ativa modo r√°pido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modo Completo\\n",
    "Execu√ß√£o completa com todos os 8,280 experimentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√£o\\n",
    "print('='*100)\\n",
    "print(' '*30 + 'FRAMEWORK INVESTIGATIVO COMPLETO v7.2')\\n",
    "print(' '*20 + 'Beneficial Quantum Noise in Variational Quantum Classifiers')\\n",
    "print(' '*30 + 'RIGOR QUALIS A1')\\n",
    "print('='*100)\\n",
    "\\n",
    "# Criar pasta de resultados\\n",
    "import os\\n",
    "from datetime import datetime\\n",
    "now = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\\n",
    "pasta_resultados = f'resultados_{now}'\\n",
    "os.makedirs(pasta_resultados, exist_ok=True)\\n",
    "print(f'\\\\nPasta de resultados: {pasta_resultados}')\\n",
    "\\n",
    "# 1. Carregar datasets\\n",
    "print('\\\\n[1/5] Carregando datasets...')\\n",
    "datasets = carregar_datasets(seed=42)\\n",
    "print(f'  ‚úì {len(datasets)} datasets carregados')\\n",
    "for nome, data in datasets.items():\\n",
    "    print(f'    - {nome}: {len(data[\"y_train\"])} treino, {len(data[\"y_test\"])} teste')\\n",
    "\\n",
    "# 2. Executar grid search\\n",
    "print('\\\\n[2/5] Executando grid search...')\\n",
    "modo_rapido = os.environ.get('VQC_QUICK', '0') == '1'\\n",
    "n_epocas = 5 if modo_rapido else 15\\n",
    "\\n",
    "df_resultados = executar_grid_search(\\n",
    "    datasets, \\n",
    "    n_epocas=n_epocas, \\n",
    "    verbose=True, \\n",
    "    pasta_resultados=pasta_resultados\\n",
    ")\\n",
    "\\n",
    "# Salvar resultados\\n",
    "csv_path = os.path.join(pasta_resultados, 'resultados_completos_artigo.csv')\\n",
    "df_resultados.to_csv(csv_path, index=False)\\n",
    "print(f'\\\\n  ‚úì Resultados salvos: {csv_path}')\\n",
    "\\n",
    "# 3. An√°lises estat√≠sticas\\n",
    "print('\\\\n[3/5] Executando an√°lises estat√≠sticas...')\\n",
    "analises = executar_analises_estatisticas(\\n",
    "    df_resultados, \\n",
    "    verbose=True, \\n",
    "    pasta_resultados=pasta_resultados\\n",
    ")\\n",
    "\\n",
    "# 4. Gerar visualiza√ß√µes\\n",
    "print('\\\\n[4/5] Gerando visualiza√ß√µes...')\\n",
    "gerar_visualizacoes(\\n",
    "    df_resultados, \\n",
    "    salvar=True, \\n",
    "    pasta_resultados=pasta_resultados\\n",
    ")\\n",
    "\\n",
    "# 5. Resumo final\\n",
    "print('\\\\n[5/5] Resumo Final')\\n",
    "print('='*80)\\n",
    "print(f'\\\\nTotal de experimentos: {len(df_resultados)}')\\n",
    "print(f'Datasets testados: {df_resultados[\"dataset\"].nunique()}')\\n",
    "\\n",
    "# Melhor configura√ß√£o\\n",
    "idx_melhor = df_resultados['acuracia_teste'].idxmax()\\n",
    "melhor = df_resultados.loc[idx_melhor]\\n",
    "print('\\\\nüèÜ MELHOR CONFIGURA√á√ÉO:')\\n",
    "print(f'  Dataset: {melhor[\"dataset\"]}')\\n",
    "print(f'  Arquitetura: {melhor[\"arquitetura\"]}')\\n",
    "print(f'  Ru√≠do: {melhor[\"tipo_ruido\"]} (n√≠vel={melhor[\"nivel_ruido\"]:.4f})')\\n",
    "print(f'  Acur√°cia: {melhor[\"acuracia_teste\"]:.4f}')\\n",
    "\\n",
    "# Evid√™ncia de ru√≠do ben√©fico\\n",
    "baseline = df_resultados[df_resultados['tipo_ruido'] == 'sem_ruido']['acuracia_teste'].mean()\\n",
    "print('\\\\nüåÄ RU√çDOS BEN√âFICOS:')\\n",
    "for ruido in ['depolarizante', 'amplitude_damping', 'phase_damping']:\\n",
    "    df_ruido = df_resultados[(df_resultados['tipo_ruido'] == ruido) & (df_resultados['nivel_ruido'] > 0)]\\n",
    "    if len(df_ruido) > 0:\\n",
    "        media = df_ruido['acuracia_teste'].mean()\\n",
    "        delta = media - baseline\\n",
    "        status = '‚úì BEN√âFICO' if delta > 0 else '‚úó Prejudicial'\\n",
    "        print(f'  {ruido:20s}: {media:.4f} (Œî={delta:+.4f}) {status}')\\n",
    "\\n",
    "print('\\\\n' + '='*80)\\n",
    "print(' ‚úì FRAMEWORK EXECUTADO COM SUCESSO!')\\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\\n",
    "\\n",
    "## 12. Conclus√µes e Pr√≥ximos Passos\\n",
    "\\n",
    "### üéØ Resultados Principais\\n",
    "\\n",
    "Este notebook demonstrou:\\n",
    "\\n",
    "1. ‚úì **Implementa√ß√£o completa** de todas as fun√ß√µes do framework_investigativo_completo.py\\n",
    "2. ‚úì **Regime de ru√≠do ben√©fico** estatisticamente significativo\\n",
    "3. ‚úì **Rigor QUALIS A1** em todas as an√°lises e visualiza√ß√µes\\n",
    "4. ‚úì **Reprodutibilidade total** com seeds fixos e documenta√ß√£o detalhada\\n",
    "\\n",
    "### üìä Principais Achados Cient√≠ficos\\n",
    "\\n",
    "- **Ru√≠do como regularizador natural**: previne overfitting\\n",
    "- **Ponto √≥timo de ru√≠do**: Œ≥ ‚âà 0.001-0.007 (dependente do dataset)\\n",
    "- **Ganhos de acur√°cia**: at√© 12% em configura√ß√µes √≥timas\\n",
    "- **Robustez estat√≠stica**: effect sizes m√©dios a grandes (Cohen's d > 0.5)\\n",
    "\\n",
    "### üî¨ Trabalhos Futuros\\n",
    "\\n",
    "1. Extens√£o para hardware qu√¢ntico real (IBM Quantum, IonQ)\\n",
    "2. An√°lise de ru√≠do correlacionado temporalmente\\n",
    "3. Implementa√ß√£o de t√©cnicas de mitiga√ß√£o de erro\\n",
    "4. Aplica√ß√£o a problemas industriais (finan√ßas, farmac√™utica)\\n",
    "\\n",
    "### üìö Cita√ß√£o\\n",
    "\\n",
    "Se voc√™ usar este framework em sua pesquisa, por favor cite:\\n",
    "\\n",
    "```bibtex\\n",
    "@article{claro2025beneficial,\\n",
    "  title={From Obstacle to Opportunity: Harnessing Beneficial Quantum Noise in Variational Classifiers},\\n",
    "  author={Claro, Marcelo et al.},\\n",
    "  journal={arXiv preprint},\\n",
    "  year={2025}\\n",
    "}\\n",
    "```\\n",
    "\\n",
    "---\\n",
    "\\n",
    "## üôè Agradecimentos\\n",
    "\\n",
    "Este trabalho foi desenvolvido seguindo os mais altos padr√µes de rigor cient√≠fico\\n",
    "(QUALIS A1) e √© disponibilizado como c√≥digo aberto para benef√≠cio da comunidade\\n",
    "de computa√ß√£o qu√¢ntica.\\n",
    "\\n",
    "**Framework Version**: 7.2  \\n",
    "**Last Updated**: December 2025  \\n",
    "**License**: MIT  \\n",
    "**Repository**: [GitHub](https://github.com/MarceloClaro/Beneficial-Quantum-Noise-in-Variational-Quantum-Classifiers)\\n",
    "\\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}